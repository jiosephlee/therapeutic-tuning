\title{Direct Preference Optimization: Your Language Model is Secretly a Reward Model}

\begin{abstract}
Although large-scale unsupervised language models (LMs) acquire extensive knowledge and some reasoning capabilities, precisely guiding their outputs is challenging because of their fully unsupervised training process.
Current strategies for improving controllability involve collecting human assessments comparing model outputs and then fine-tuning the unsupervised LM to reflect these choices, typically by employing reinforcement learning from human feedback (RLHF).
Nonetheless, RLHF introduces significant complexity and instability, as it first fits a reward model to represent human evaluations, and then adjusts the LM with reinforcement learning to optimize for this reward while constraining divergence from the original model.
In this work, we propose a novel reward model parameterization for RLHF that enables direct derivation of the optimal policy in closed form, thereby reframing the standard RLHF challenge as a straightforward classification problem.
We term the resulting approach \textit{Direct Preference Optimization} (DPO), which is robust, efficient, and computationally inexpensive, removing the need for language model sampling during fine-tuning and extensive hyperparameter searches.
Empirical results demonstrate that DPO can tune LMs to match human preferences as effectively as, or better than, prior approaches. Importantly, DPO outperforms PPO-based RLHF in steering sentiment of generated text and is competitive or superior in tasks such as summarization and single-turn conversation, while being much simpler to implement and train.
\end{abstract}

\section{Introduction}
Large-scale unsupervised language models (LMs) trained on massive corpora have demonstrated remarkable and unexpected abilities~\citep{chowdhery2022palm, brown2020language, touvron2023llama, bubeck2023sparks}. Despite this, these models are exposed to human-generated content originating from individuals with diverse intentions, values, and levels of expertise. Not all of these are desirable for imitation; for instance, although we expect an AI coding assistant to \textit{recognize} frequent programming errors so it can fix them, when producing code, we want the model to favor the (sometimes uncommon) high-quality examples found in its training dataset. In a similar vein, we may require our language model to \textit{know about} a prevalent misconception held by half of the population, but certainly do not wish for it to repeat this error as fact in half of its responses! Thus, filtering out the model’s \emph{preferred outputs and conduct} from its broad \textit{knowledge base and skill set} is essential to ensure AI systems remain safe, effective, and manageable \citep{ouyang2022training}. Current strategies typically employ reinforcement learning (RL) to guide LMs toward human-aligned responses, but we will demonstrate that the RL-based objectives employed in these approaches can be fulfilled precisely by utilizing a simple binary cross-entropy objective, thereby streamlining the preference optimization process.

\begin{figure}
    \centering
    \includegraphics[width=0.999\textwidth]{figures/diagrams/teaser.png}
    \caption{\textbf{DPO aligns with human preferences without reinforcement learning.} Traditional techniques for tuning language models using human feedback typically involve training a reward model on a collection of prompts and human-annotated preference pairs, followed by employing RL to derive a policy that maximizes this estimated reward. By contrast, DPO streamlines the process by directly optimizing the policy to satisfy preference data through a straightforward classification loss, effectively learning an \textit{implicit} reward model from which the optimal policy can be analytically obtained.}
    \vspace{-2mm}
    \label{fig:teaser}
\end{figure}

In general, current approaches impart desired traits to language models by using carefully assembled collections of human preferences that capture what behaviors people consider safe and beneficial. This preference learning step follows an initial phase where the model undergoes extensive unsupervised pre-training on a massive corpus of text. Although the simplest strategy for learning preferences involves supervised fine-tuning on high-quality human demonstrations, the most effective approaches are those based on reinforcement learning from human or AI feedback (RLHF/RLAIF; \citep{christiano2017deep,bai2022constitutional}). RLHF approaches train a reward model using human preference data, then employ reinforcement learning to optimize the language model's policy so it generates responses with high reward, all while ensuring it does not diverge too much from the pre-trained model. Despite the fact that RLHF leads to models with strong conversational and coding performance, the RLHF workflow is much more involved than straightforward supervised learning. It requires training multiple language models and conducting policy sampling within the training loop, which results in substantial computational overhead.

In this work, we present a method for directly tuning a language model to match human preferences, bypassing the need for explicit reward modeling or reinforcement learning techniques. We introduce Direct Preference Optimization (DPO), a method that inherently targets the same goal as typical RLHF approaches—maximizing reward subject to a KL-divergence penalty—but is both easy to implement and to train. At a high level, DPO adjusts the model by increasing the relative log-likelihood of responses favored by humans compared to those less favored, while applying a dynamic, instance-specific importance weighting to avoid model collapse, an issue observed with straightforward probability ratio objectives. DPO, much like previous algorithms, utilizes a theoretical preference framework (such as the Bradley-Terry model; \cite{bradley1952rankanalysis}) to assess the alignment between a reward function and observed preference data. In contrast to traditional methods, which employ the preference model to generate a loss for training a reward model before optimizing the policy to fit this reward, DPO leverages a variable transformation so that the preference loss depends directly on the policy itself. As a result, given a collection of human preference data over model outputs, DPO can optimize the policy using a straightforward binary cross-entropy loss, yielding the optimal policy for an implicit reward function inferred from the preference data.

The primary contribution of this work is Direct Preference Optimization (DPO), a straightforward algorithm that enables preference-based language model training without the need for reinforcement learning. Experimental results demonstrate that DPO matches or surpasses the performance of established approaches, such as PPO-based RLHF, in preference learning tasks like sentiment control, summarization, and dialogue, utilizing language models containing up to 6B parameters.

\section{Related Work}

Self-supervised language models, as their size increases, gain the capability to perform certain tasks in a zero-shot manner \citep{radford2019language} or with only a few example prompts \citep{gpt3,megatron,chowdhery2022palm}. Yet, their effectiveness on downstream applications and their ability to follow user instructions are often notably enhanced when these models are further fine-tuned on datasets containing instructions and human-generated responses \citep{mishra-etal-2022-cross,sanh2022multitask,chung2022scaling,thoppilan2022lamda}. This process, known as `instruction-tuning,' allows LLMs to better handle novel instructions not present in the original instruction-tuning data and generally makes them more user-friendly \citep{chung2022scaling}. Although instruction tuning has proven highly effective, collecting \textit{relative} human assessments of response quality is typically less demanding than obtaining expert-labeled examples. Consequently, later approaches have adapted LLMs by leveraging datasets of human preference judgments, yielding improvements in tasks such as translation \citep{kreutzer-etal-2018-reliability}, summarization \citep{stiennon2022learning,ziegler2020finetuning}, narrative generation \citep{ziegler2020finetuning}, and instruction adherence \citep{ouyang2022training,ramamurthy2023is}. These techniques generally involve first training a neural reward model to reflect these human preferences, often using a preference aggregation scheme such as the Bradley-Terry model \citep{bradley1952rankanalysis}, and then further adapting the language model using reinforcement learning—typically via algorithms like REINFORCE \citep{williams1992reinforce}, proximal policy optimization (PPO; \cite{schulman2017proximal}), or related methods \citep{ramamurthy2023is}—to maximize the learned reward. Another closely-related research direction involves LLMs that have been instruction-tuned with human input, which are then used to create additional artificial preference data focused on aspects like safety or harmlessness \citep{bai2022constitutional}, relying only on high-level guidance from humans via text-based rubrics during model annotation. These lines of research bring together advances from two areas: reinforcement learning-based training of language models for various tasks~\citep{Ranzato2015SequenceLT,paulus2018a,wu2018learning} and general approaches for preference-based learning from humans \citep{christiano2017deep,kupcsik2018learning}. Despite the attractive nature of using pairwise human preference data, applying reinforcement learning for fine-tuning large language models is still a significant practical obstacle; this work introduces a theoretically-founded method for optimizing based on relative preferences without relying on reinforcement learning.

Beyond language-related tasks, policy learning from preferences has been explored in both bandit and reinforcement learning frameworks, resulting in various methodologies. In particular, contextual bandit learning that leverages preferences or rankings over actions—rather than direct reward signals—is referred to as the contextual dueling bandit (CDB; \cite{yue2012karmed,dudik2015contextual}). Without access to absolute rewards, the theory for CDBs replaces the idea of an optimal policy with the concept of a \textit{von Neumann winner}, defined as a policy whose expected probability of outperforming \textit{any} other policy is at least 50\% \citep{dudik2015contextual}. Notably, in the CDB framework, preference feedback is acquired online, whereas in the context of human preference learning, training typically occurs on a static dataset containing offline, preference-labeled action pairs \citep{yan2022human}. In a similar vein, \textit{preference-based RL} (PbRL) learns from binary preference signals produced by an \textit{unknown} underlying `scoring' function, instead of relying on reward values \citep{BusaFekete2014,ruiz2023dueling}. A range of PbRL algorithms have been developed—including approaches that exploit off-policy preference data—but these methods usually involve explicitly estimating the latent scoring function (i.e., reward model) before optimizing the policy with respect to it \citep{jain2013learning,BusaFekete2014,christiano2017deep,sadigh2017active,kupcsik2018learning}. In contrast, we introduce a unified, single-stage method that directly learns a policy to align with given preferences.

\section{Preliminaries}\label{section:prelims}

We summarize the RLHF process outlined by \citeauthor{ziegler2020finetuning}, and subsequently expanded upon in \citep{stiennon2022learning, bai2022training, ouyang2022training}. This framework generally consists of three main steps: (1) supervised fine-tuning (SFT), (2) collecting preferences and learning a reward model, and (3) reinforcement learning optimization.

\textbf{SFT}: RLHF usually starts by applying supervised fine-tuning to a pre-trained language model using high-quality datasets tailored to the target task (such as dialogue or summarization), resulting in a model denoted as $\pisft$.

\textbf{Reward Modelling Phase}: During the second stage, prompts $x$ are given to the SFT model, which then generates answer pairs $(y_1, y_2)\sim \pisft(y \mid x)$. These answer pairs are evaluated by human annotators, who indicate their preference by selecting one response over the other, represented as $y_w\succ y_l \mid x$, where $y_w$ and $y_l$ stand for the chosen and non-chosen answers within $(y_1, y_2)$. It is presumed that these preferences stem from an underlying, unknown reward function $r^*(y, x)$. There are several methods to capture such preferences, with the Bradley-Terry (BT) model \cite{bradley1952rankanalysis} being widely used (though more comprehensive ranking frameworks like the Plackett-Luce models \citep{plackett1975analysis, luce2012individual} also fit within this setup if multiple rankings per prompt are available). The BT model defines the probability of human preference $p^*$ as follows:
\begin{equation}\label{eq:bradley-terry}
    p^*(y_1\succ y_2 \mid x)=\frac{\exp\left(r^*(x, y_1)\right)}{\exp\left(r^*(x, y_1)\right) + \exp\left(r^*(x, y_2)\right)}.
\end{equation}
Supposing that we have a fixed set of comparisons $\mathcal{D}=\bigl\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\bigr\}_{i=1}^N$ drawn from $p^*$, we can introduce a parameterized reward function $r_{\phi}(x, y)$ and estimate its parameters through maximum likelihood. Treating this as a binary classification, the negative log-likelihood objective becomes:
\begin{equation}\label{eq:reward_model}
    \mathcal{L}_R(r_{\phi}, \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\bigl[\log \sigma(r_{\phi}(x, y_w)- r_{\phi}(x, y_l))\bigr]
\end{equation}
where $\sigma$ is the sigmoid (logistic) function. For language models, $r_{\phi}(x, y)$ is typically initialized using the SFT model $\pisft(y \mid x)$, appending a linear layer after the transformer's final layer to output a single scalar reward \cite{ziegler2020finetuning}. To promote stability and reduce reward variance, prior work applies normalization so that $\mathbb{E}_{x,y\sim \mathcal{D}}\left[r_\phi(x, y)\right] = 0$ for every $x$.

\textbf{RL Fine-Tuning Phase}: In the reinforcement learning (RL) stage, the language model receives feedback through the previously trained reward function. Consistent with earlier studies~\citep{jaques2017sequence, jaques2020human}, the training objective is expressed as
\begin{equation}\label{eq:RL}
\max_{\pi_{\theta}}  \mathbb{E}_{x\sim \mathcal{D}, y\sim \pi_{\theta}(y \mid x)}\left[r_{\phi}(x, y)\right] - \beta\mathbb{D}_{\textrm{KL}}\left[\pi_{\theta}(y\mid x)\;\|\; \piref(y\mid x)\right],
\end{equation}
where $\beta$ is a hyperparameter that regulates how much $\pi_\theta$ can diverge from the reference policy $\piref$, which refers to the initial SFT model $\pisft$. Generally, $\pi_\theta$ is initialized with $\pisft$ as well. This KL divergence constraint is essential to prevent the model from straying too far from the data distribution on which the reward model is reliable, helps preserve diversity in the generated outputs, and reduces the risk of mode-collapse to a few high-reward responses. Since language generation produces discrete outputs, this loss is non-differentiable, and thus is commonly addressed using reinforcement learning algorithms. The conventional method \citep{ziegler2020finetuning, stiennon2022learning, bai2022training, ouyang2022training} involves defining the reward as ${r(x, y) = r_{\phi}(x, y) -\beta (\log \pi_{\theta}(y\mid x) - \log \piref(y\mid x))}$ and applying PPO~\cite{schulman2017proximal} for optimization.

\section{Direct Preference Optimization}\label{sec:DPO}

Driven by the difficulties in scaling reinforcement learning techniques to tasks like language model fine-tuning, our objective is to develop a straightforward policy optimization method that works directly from preference data. Unlike traditional RLHF strategies that first learn a reward function and then optimize it with RL, our method uses a specific reward model parameterization that allows for the direct computation of the optimal policy in closed form, thereby eliminating the need for iterative RL training. 

As we will elaborate on, our main contribution is to exploit an analytical relationship between reward functions and their corresponding optimal policies, allowing us to reframe a loss defined on reward functions as a loss directly over policies. This change-of-variables technique removes the need to explicitly train a separate reward model, yet still aligns with established human preference frameworks like the Bradley-Terry model. In this setup, the policy network simultaneously captures the behavior of both the language model and the implicit reward signal.

\textbf{Deducing the DPO objective.} We begin with the standard reinforcement learning objective as outlined in previous studies, Eq.~\ref{eq:RL}, defined with a generic reward function $r$. Drawing on earlier work~\citep{peters2007reinforcement, peng2019advantage, korbak2022reinforcement, go2023aligning}, it can be readily established that the optimal policy for the KL-regularized reward maximization objective in Eq.~\ref{eq:RL} is given by:
\begin{equation}\label{eq:op_policy}
    \pi_r(y\mid x) = \frac{1}{Z(x)}\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right),
\end{equation}
where $Z(x) =\sum_{y}\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)$ serves as the normalization constant. For the full derivation, refer to Appendix \ref{app:derivation1}. Even when replacing the ground-truth reward function $r^*$ with its MLE approximation $r_{\phi}$, calculating the partition function $Z(x)$ remains computationally intensive~\citep{korbak2022reinforcement, go2023aligning}, which complicates practical application. Nevertheless, Eq.~\ref{eq:op_policy} can be manipulated to write the reward as a function of its optimal policy $\pi_r$, the reference policy $\piref$, and the unknown normalization $Z(\cdot)$. Specifically, by taking the logarithm of both sides in Eq.~\ref{eq:op_policy} and rearranging, we arrive at:
\begin{equation}\label{eq:main_eq}
    r(x,y) =\beta \log \frac{\pi_r(y\mid x)}{\piref(y\mid x)} + \beta \log Z(x).
\end{equation}
Applying this formulation to the true reward $r^*$ and the corresponding optimal policy $\pi^*$, we note that the Bradley-Terry model relies solely on reward differences between two outputs, i.e., ${p^*(y_1 \succ y_2 \mid x) = \sigma(r^*(x, y_1) - r^*(x, y_2))}$. Inserting the reparameterization from Eq.~\ref{eq:main_eq} into the preference model Eq.~\ref{eq:bradley-terry}, the normalization terms cancel out, allowing the human preference likelihood to be rewritten solely in terms of the optimal policy $\pi^*$ and the reference $\piref$. As a result, the optimal RLHF policy $\pi^*$ under the Bradley-Terry framework adheres to:
\begin{equation}\label{eq:objective}
    p^*(y_1\succ y_2 \mid x)=\frac{1}{1 + \exp\left(\beta \log \frac{\pi^*(y_2\mid x)}{\piref(y_2\mid x)} - \beta \log \frac{\pi^*(y_1\mid x)}{\piref(y_1\mid x)}\right)}
\end{equation}
A complete proof is provided in Appendix~\ref{app:derivation2}. Although Eq.~\ref{eq:objective} leverages the Bradley-Terry framework, similar derivations extend to the more general Plackett-Luce models~\citep{plackett1975analysis, luce2012individual}, as detailed in Appendix~\ref{app:plackett_luce_models}.

With the human preference probabilities now expressed in terms of the optimal policy instead of the reward function, we are able to define a maximum likelihood objective for a parameterized policy $\pi_\theta$. In a manner similar to the reward modeling framework (see Eq.~\ref{eq:reward_model}), our objective for the policy takes the following form:
\begin{equation}\label{eq:optimum_model}
    \mathcal{L}_\text{DPO}(\pi_{\theta}; \piref) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\left[\log \sigma \left(\beta \log \frac{\pi_{\theta}(y_w\mid x)}{\piref(y_w\mid x)} - \beta \log \frac{\pi_{\theta}(y_l\mid x)}{\piref(y_l\mid x)}\right)\right].
\end{equation}
Through this formulation, we leverage an implicit reward function via an alternative parameterization, where the optimal policy corresponds directly to $\pi_\theta$. Additionally, because this approach is tantamount to fitting a reparameterized Bradley-Terry model, it inherits certain theoretical guarantees—such as consistency under reasonable assumptions about the preference data distribution \cite{bong2022generalized}. We elaborate further on the theoretical aspects of DPO and its relation to prior work in Section~\ref{sec:theory}.

\textbf{What effect does the DPO update have?} To gain a mechanistic perspective on DPO, it is helpful to examine the gradient of the loss $\mathcal{L}_\text{DPO}$. The gradient with respect to the parameters $\theta$ can be expressed as:
\begin{multline*}\label{eq:gradient}
    \nabla_\theta \mathcal{L}_\text{DPO}(\pi_\theta;\piref) = \\ -\beta\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \bigg[\underbrace{\sigma(\hat{r}_\theta(x, y_l) - \hat{r}_\theta (x, y_w))}_\text{larger magnitude when reward estimates are inaccurate}\bigg[\underbrace{\nabla_\theta\log \pi(y_w \mid x)}_\text{push up probability of $y_w$} - \underbrace{\nabla_\theta\log\pi(y_l \mid x)}_\text{push down probability of $y_l$}\bigg]\bigg],
\end{multline*}
where $\hat{r}_\theta(x, y) = \beta \log \frac{\pi_\theta(y \mid x)}{\piref(y \mid x)}$ denotes the reward implicitly specified by the model $\pi_\theta$ relative to the reference $\piref$ (see Section~\ref{sec:theory} for further details). Conceptually, the gradient of $\mathcal{L}_\text{DPO}$ amplifies the likelihood of favored completions $y_w$ and reduces that of less preferred ones $y_l$. Crucially, each example is weighted according to how much the implicit reward model $\hat{r}_\theta$ overvalues the less preferred completion, modulated by $\beta$—reflecting the degree to which the reward model misranks completions and the influence of the KL regularization. Our empirical findings highlight the necessity of this weighting: omitting the weighting factor often leads to degenerate language model behavior (see Appendix Table~\ref{tab:unlikelihood_generations}).

\textbf{Overview of DPO.}  
The standard DPO process proceeds as follows: 1) For each prompt $x$, draw completions $y_1, y_2 \sim \piref(\cdot \mid x)$, annotate them with human preference signals to build an offline dataset of preferences $\mathcal{D} = \{x^{(i)}, y_w^{(i)}, y_l^{(i)}\}_{i=1}^N$; 2) train the language model $\pi_\theta$ by minimizing the objective $\mathcal{L}_\text{DPO}$, using the chosen $\piref$, constructed $\mathcal{D}$, and specified $\beta$.  
In practice, rather than generating new samples and collecting fresh human preferences, it is preferable to leverage publicly released preference datasets. As these datasets are produced using $\pisft$, we set $\piref = \pisft$ when possible. If $\pisft$ is unavailable, we instead initialize $\piref$ via maximum likelihood estimation over preferred completions ${(x, y_w)}$, i.e., ${\piref = \argmax_{\pi}\mathbb{E}_{x, y_w \sim \mathcal{D}}\left[\log \pi(y_w \mid x)\right]}$. This approach serves to reduce the distribution mismatch between the ideal reference distribution (which is not accessible) and the $\piref$ actually utilized in DPO. Additional implementation and hyperparameter details are provided in Appendix~\ref{app:implementation}.

\section{Theoretical Examination of DPO}
Here, we offer a deeper understanding of the DPO approach, present theoretical justification, and discuss how DPO's benefits address shortcomings observed in actor-critic methods like PPO~\cite{schulman2017proximal} frequently employed in RLHF.

\label{sec:theory}

\subsection{Your Language Model Is Secretly a Reward Model} DPO manages to avoid both explicitly modeling a reward function and conducting RL by learning the policy directly through a single maximum likelihood objective. Observe that the objective in Eq. \ref{eq:main_eq} is identical to a Bradley-Terry model, where the reward is parameterized as $r^*(x, y) = \beta \log\frac{\pi^*_\theta(y \mid x)}{\piref(y \mid x)}$. We optimize our parametric model $\pi_{\theta}$, which is equivalent to optimizing the reward model as in Eq. \ref{eq:reward_model}, after applying a variable change. In this section, we will construct the theoretical foundation for this reparameterization, demonstrate that it does not restrict the space of learned reward models, and show that it enables precise recovery of the optimal policy. We start by introducing an equivalence relation between reward functions.

\begin{definition}
Two reward functions $r(x, y)$ and $r'(x, y)$ are called equivalent if and only if there exists a function $f$ such that $r(x, y)-r'(x, y) = f(x)$.
\end{definition}
It is straightforward to verify that this forms an equivalence relation, dividing the space of reward functions into equivalence classes. The following two lemmas can be stated:

\begin{lemma}\label{lemma:same_prefrence} Within the Plackett-Luce preference model—including the Bradley-Terry model—any two reward functions belonging to the same class yield identical preference distributions.
\end{lemma}

\begin{lemma}\label{lemma:same_policy}
    Any two reward functions belonging to the same equivalence class yield identical optimal policies when solving the constrained RL problem.
\end{lemma}

The proofs for these statements are immediate, so we present them in Appendix \ref{app:lemma1}. The first lemma highlights a well-known ambiguity present in the Plackett-Luce family of models \cite{plackett1975analysis}. Because of this ambiguity, it is typically necessary to enforce extra identifiability constraints in order to derive meaningful guarantees on the MLE solutions to Eq. \ref{eq:reward_model} \cite{bong2022generalized}. The second lemma asserts that all reward functions within a single equivalence class lead to the same optimal policy. Consequently, for the purposes of our objective, it suffices to recover any reward function from the class that yields the optimal policy. We formally prove the following theorem in Appendix~\ref{app:thm1}:

\begin{theorem}\label{thm:main}
    Assuming mild conditions, every reward class compatible with Plackett-Luce (and, in particular, Bradley-Terry) models can be captured via the reparameterization ${r(x, y) = \beta \log \frac{\pi(y\mid x)}{\piref(y\mid x)}}$ for an appropriate model $\pi(y\mid x)$ and fixed reference model $\piref(y \mid x)$.
\end{theorem}

\begin{sproof}
    Take any reward function $r(x, y)$, which corresponds to some optimal policy $\pi_r(y \mid x)$ as given by Eq. \ref{eq:op_policy}. We show that there is a reward function in the equivalence class of $r$ that admits the claimed reparameterized form. Define the mapping $f$ as  
\begin{equation}
    f(r; \piref, \beta)(x, y) = r(x, y) - \beta\log\sum_{y}\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)
\end{equation}
Here, $f$ normalizes the reward by subtracting the log partition function of $\pi_r$. Since this normalization only depends on $x$, $f(r; \piref, \beta)(x, y)$ remains in the equivalence class of $r(x, y)$. Substituting $r$ with the RHS of Eq.~\ref{eq:main_eq} (valid for any reward), we find $f(r; \piref, \beta)(x, y) = \beta \log \frac{\pi_r(y\mid x)}{\piref(y\mid x)}$. Thus, the map $f$ yields a representative of the equivalence class of $r$ in the desired reparameterized form, ensuring no generality is lost with our chosen reward model structure.
\end{sproof}

Another perspective on Theorem~\ref{thm:main} is that it specifies the exact representative within each equivalence class that the DPO reparameterization recovers—namely, the reward function satisfying:
\begin{equation}\label{eq:lag_p}
     \sum_{y}\underbrace{\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)}_{=\pi(y\mid x)\text{, by Thm.~\ref{thm:main} reparam.}} = 1,
\end{equation}
which ensures that $\pi(y\mid x)$ defines a valid probability distribution (all probabilities are positive and sum to one).

By examining Eq.~\ref{eq:op_policy}, we observe that Eq.~\ref{eq:lag_p} corresponds to the partition function associated with the optimal policy determined by $r(x, y)$.

The main idea behind the DPO method is to impose specific constraints on the under-specified Plackett-Luce (and, specifically, Bradley-Terry) models, so as to maintain the expressiveness of the reward models, while ensuring that the optimal policy from Eq. \ref{eq:op_policy} has an analytically tractable partition function for every context $x$.

\subsection{Instability of Actor-Critic Algorithms}
Our framework can also help analyze sources of instability in commonly used actor-critic methods for RLHF, like PPO. Specifically, we examine the RL fine-tuning stage described in Section \ref{section:prelims}, following the RLHF process. We connect this to the control as inference perspective \cite{levine2018reinforcement} for the constrained RL formulation given in \ref{eq:RL}. We consider a model $\pi_{\theta}(y\mid x)$ with parameters, and seek to minimize $\mathbb{D}_{\text{KL}}[\pi_{\theta}(y|x) \mid \mid \pi^*(y\mid x)]$, where $\pi^*$, defined in Eq. \ref{eq:optimum_model}, is the reward-optimal policy determined by $r_{\phi}(y, x)$. Through some manipulations, this results in the following objective:
\begin{equation}\label{eq:AC}
    \max_{\pi_{\theta}}\mathbb{E}_{\pi_{\theta}(y\mid x)}\bigg[\underbrace{r_{\phi}(x, y) -\beta\log\sum_{y}\piref(y\mid x)\exp\left(\frac{1}{\beta}r_{\phi}(x, y)\right)}_{f(r_{\phi}, \piref, \beta)} - \underbrace{\beta\log\frac{\pi_{\theta}(y\mid x)}{\piref(y\mid x)}}_{\text{KL}}\bigg]
\end{equation}
This objective is identical to those maximized in earlier studies \citep{ziegler2020finetuning, stiennon2022learning, bai2022training, ouyang2022training} that utilize the DPO-like reward for the $r_{\phi}$ reward class. Here, the normalization inside $f(r_{\phi}, \piref, \beta)$ can be interpreted as the soft value function for the reference policy $\piref$. Although this normalization does not modify the optimal policy, omitting it can cause the policy gradient's variance to grow large, resulting in unstable learning. One way to address the normalization is to approximate it with a learned value function, but this approach can present optimization challenges. As an alternative, previous work has normalized rewards using the human completion as a baseline, effectively a single Monte-Carlo estimate of the normalization. The DPO reparameterization, in contrast, produces a reward function that eliminates the need for any baselines.

\section{Experiments}
In this section, we experimentally assess how well DPO can train policies using only preference data. First, in a controlled text generation scenario, we investigate how effectively DPO balances reward maximization against keeping the KL-divergence from the reference policy low, and compare its performance to established preference-based methods such as PPO. Then, we test DPO on larger-scale models and more complex RLHF challenges, such as dialogue and summarization tasks. Our results indicate that, with minimal hyperparameter adjustment, DPO typically matches or surpasses robust baselines like RLHF with PPO, as well as outperforming the strategy of selecting the best among $N$ sampled outputs according to a learned reward model. Prior to discussing our findings, we outline the experimental protocol; more information can be found in Appendix~\ref{app:exp_details}.

\textbf{Tasks.} Our study investigates three distinct open-ended text generation tasks. In each experiment, learning algorithms derive a policy from a dataset of preferences $\mathcal{D}=\bigl\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\bigr\}_{i=1}^N$. For \textbf{controlled sentiment generation}, $x$ represents an initial segment of a movie review from the IMDb dataset \cite{maas-EtAl:2011:ACL-HLT2011}, and the policy's objective is to generate a continuation $y$ with positive sentiment. To facilitate controlled analysis, we automatically create preference pairs for generations using a pre-trained sentiment classifier, ensuring that $p(\text{positive}\mid x,y_w)>p(\text{positive}\mid x,y_l)$. For SFT, GPT-2-large is fine-tuned until convergence using training set reviews from the IMDB dataset (see App~\ref{app:sentiment_details} for more information). For the \textbf{summarization} task, $x$ consists of a Reddit forum post, and the policy is tasked with generating a summary $y$ of its core content. Consistent with previous studies, we utilize the Reddit TL;DR summarization dataset \citep{volske-etal-2017-tl}, as well as human preference data compiled by \citeauthor{stiennon2022learning}. The SFT model employed is fine-tuned on summaries written by humans for forum posts\footnote{\url{https://huggingface.co/CarperAI/openai_summarize_tldr_sft}} and trained using the TRLX \citep{leandro_von_werra_2023_7790115} RLHF library. Human preference annotations were sourced from \citeauthor{stiennon2022learning} and involve samples generated by a comparably-trained, yet different, SFT model. Lastly, for the \textbf{single-turn dialogue} task, $x$ is a human prompt—ranging from scientific questions to personal advice requests. The policy must return an informative and engaging answer $y$; for this, we leverage the Anthropic Helpful and Harmless dialogue dataset \citep{bai2022training}, which comprises 170k conversations between people and an automated system. Every conversation concludes with two responses produced by a large (but unspecified) language model, along with a label indicating which response was preferred by a human annotator. Since no pre-trained SFT model is available for this domain, we construct the SFT model by fine-tuning a general-purpose language model using only the responses labeled as preferred.

\begin{figure}
    \centering
    \includegraphics[width=0.50\textwidth]{figures/results/frontier.pdf}
    \includegraphics[width=0.49\textwidth]{figures/results/tldr_winrate_vs_temp.pdf}
    \caption{\textbf{Left.} The trade-off curve of expected reward versus KL divergence from the reference policy. DPO attains the greatest expected reward across all KL constraints, highlighting its optimization effectiveness. \textbf{Right.} TL;DR summarization win rates compared to human-authored summaries, evaluated with GPT-4. DPO outperforms PPO at its optimal performance level in summarization, and also maintains better stability across varying sampling temperatures.}
    \vspace{-2mm}
    \label{fig:frontier-tldr-main}
\end{figure}

\textbf{Evaluation.} We employ two distinct evaluation strategies in our experiments. To assess how well each algorithm optimizes the constrained reward maximization objective, we perform controlled sentiment generation experiments, measuring the trade-off between achieved reward and KL-divergence from the reference policy. This reward/KL frontier is measurable because we have access to the true reward function (provided by a sentiment classifier). In contrast, since the ground-truth reward function is unavailable in practical scenarios, we compare algorithms based on their \textit{win rate} relative to a baseline policy, utilizing GPT-4 as a stand-in for human judgment to assess summary quality and helpfulness of responses in summarization and single-turn dialogue tasks, respectively. For summarization, the test set reference summaries serve as the baseline; for dialogue, we use the preferred response in the dataset as the baseline. Although recent research indicates that LMs may outperform traditional metrics as automated evaluators \citep{Chen2023ExploringTU}, we also carry out a human study to validate our use of GPT-4 for evaluation, as detailed in Sec.~\ref{sec:human-judgments}. Our results indicate that GPT-4’s evaluations are highly aligned with those of humans, with human-GPT-4 agreement generally matching or surpassing agreement levels among human annotators.

\textbf{Methods.} Alongside DPO, we assess a range of established strategies for training language models in alignment with human preferences. As a baseline, we test zero-shot prompting using \textbf{GPT-J} \citep{gpt-j} for summarization, and 2-shot prompting with \textbf{Pythia-2.8B} \citep{biderman2023pythia} for the dialogue task. We also evaluate the \textbf{SFT} model and introduce \textbf{Preferred-FT}, which is produced by fine-tuning with supervised learning on the preferred completion $y_w$—sourced from either the SFT model (for controlled sentiment and summarization) or a standard language model (in the single-turn dialogue scenario). Another pseudo-supervised approach is \textbf{Unlikelihood}~\citep{welleck2019neural}, which adjusts the model to increase the likelihood of $y_w$ and simultaneously decrease the likelihood of $y_l$; an optional scaling factor $\alpha\in[0,1]$ can be applied to the unlikelihood penalty. We further include \textbf{PPO} \citep{schulman2017proximal}, where rewards are derived from preference-labeled data, as well as \textbf{PPO-GT}, an oracle model that utilizes ground-truth reward signals available in the sentiment-controlled environment. For our sentiment studies, two PPO-GT implementations are used: an off-the-shelf version \cite{leandro_von_werra_2023_7790115} and an enhanced version that incorporates reward normalization and hyperparameter tuning to boost results—these adjustments are also applied to the standard PPO model with learned rewards. Lastly, we investigate the \textbf{Best of $N$} baseline, which entails generating $N$ outputs from the SFT model (or Preferred-FT in the dialogue task) and selecting the top-scoring response according to a reward model trained on preference data. While this method can produce strong results by separating reward model performance from PPO training dynamics, it is computationally costly for even moderate values of $N$, since it requires producing $N$ completions per test prompt.

\subsection{How well can DPO optimize the RLHF objective?}

\begin{figure}
    \centering
    \includegraphics[width=0.50\textwidth]{figures/results/dialogue_winrate_vs_temp.pdf}
    \includegraphics[width=0.49\textwidth]{figures/results/dialogue_winrate_vs_steps.pdf}
    \caption{\textbf{Left.} Win rates evaluated by GPT-4 for Anthropic-HH single-turn dialogues; DPO stands out as the sole approach that outperforms chosen summaries from the Anthropic-HH evaluation set. \textbf{Right.} Win rates across various sampling temperatures throughout the training process. DPO consistently outperforms the dataset labels at different points during training, regardless of sampling temperature.}
    \vspace{-2mm}
    \label{fig:dialogue-main}
\end{figure}

The KL-regularized reward maximization objective commonly utilized in standard RLHF approaches seeks to maximize reward while simultaneously limiting how much the policy diverges from the reference policy. As a result, when assessing different algorithms, it is important to consider not only the reward obtained but also the associated KL divergence; obtaining marginally higher reward at the cost of a significantly larger KL is not always preferable. Figure~\ref{fig:frontier-tldr-main} displays the reward-KL tradeoff curves for several methods in the sentiment analysis context. For each algorithm, we conduct multiple training runs, varying the hyperparameter that controls policy conservativeness in each experiment (using target KL $\in\{3,6,9,12\}$ for PPO, $\beta \in \{0.05,0.1,1,5\}$, $\alpha\in\{0.05,0.1,0.5,1\}$ for unlikelihood, and different random seeds for preferred-FT), resulting in a total of 22 runs. At intervals of 100 training steps up to convergence, we evaluate each policy over a suite of test prompts, recording both the mean reward under the actual reward function and the mean sequence-level KL\footnote{Meaning, the sum of per-step KL divergences.} with respect to the reference policy, $\text{KL}\left(\pi\mid \mid \piref\right)$. Our results show that DPO establishes a markedly more efficient frontier than other methods, obtaining the highest reward without incurring a large KL. This finding stands out for a couple of reasons. First, although DPO and PPO seek to optimize the same objective, DPO achieves greater efficiency and its reward/KL balance strictly surpasses PPO. Second, DPO attains a better frontier than PPO, \emph{even when PPO is allowed access to ground truth rewards} (PPO-GT).

\subsection{Is DPO scalable to real-world preference datasets?}
\label{sec:dpo-real-datasets}
We proceed by analyzing the fine-tuning effectiveness of DPO on tasks like summarization and single-turn dialogue. For summarization, automatic metrics such as ROUGE often show weak alignment with human preferences~\citep{stiennon2022learning}, and earlier studies have shown that fine-tuning language models using PPO with human feedback produces more compelling summaries. To assess different approaches, we generate completions on the TL;DR summarization dataset's test split, then measure the average win rate versus reference outputs in the test set. We sample completions for every method using temperatures between 0.0 and 1.0, and display the win rates in Figure~\ref{fig:frontier-tldr-main} (right). DPO, PPO, and Preferred-FT are all trained further from the identical GPT-J SFT checkpoint\footnote{\url{https://huggingface.co/CarperAI/openai_summarize_tldr_sft}}. DPO attains an approximate 61\% win rate at a temperature of 0.0, surpassing PPO's win rate of around 57\% at its optimal sampling temperature (also 0.0). Additionally, DPO achieves a superior peak win rate relative to the best-of-$N$ baseline. It is important to highlight that DPO’s $\beta$ hyperparameter was not thoroughly tuned, so these findings might not fully capture DPO’s best possible performance. Furthermore, DPO demonstrates much greater stability across different sampling temperatures in contrast to PPO, whose win rate can drop to the base GPT-J model’s level at elevated temperatures. Preferred-FT, meanwhile, offers little improvement over the SFT baseline. In Section~\ref{sec:human-judgments}, we also directly compare DPO and PPO in human evaluations, showing that DPO samples at temperature 0.25 are favored 58\% of the time over PPO samples at temperature 0.

For single-turn dialogues, we assess various approaches using the subset of the Anthropic HH test split \citep{bai2022training} that contains a single round of human-assistant exchange. In the GPT-4 evaluations, we use the preferred completions in the test set as references to calculate win rates across the different techniques. Since a standard SFT model is not available for this task, we initialize with a pre-trained Pythia-2.8B, apply Preferred-FT to train a reference model on selected completions to ensure they remain in-distribution, and subsequently apply DPO training. Additionally, we benchmark against the strongest result from 128 Preferred-FT completions (noting that the Best of $N$ baseline plateaus by $N=128$ for this dataset; see Appendix Figure~\ref{fig:best-of-n}) and a 2-shot prompt variant of the Pythia-2.8B base model, observing that DPO achieves comparable or superior performance at optimal sampling temperatures for each method. We also test an RLHF model fine-tuned via PPO on the Anthropic HH dataset\footnote{\url{https://huggingface.co/reciprocate/ppo_hh_pythia-6B}} from a reputable source\footnote{\url{https://github.com/CarperAI/trlx/tree/main/examples/hh}}, but we are unable to identify any prompting or temperature setting that outperforms the Pythia-2.8B base model. Drawing from our TL;DR experiment findings and the fact that both PPO and Best of 128 are aligned on optimizing the same reward objective, we use Best of 128 as a rough surrogate for PPO-level effectiveness. In summary, DPO stands out as the only computationally efficient strategy that consistently surpasses the preferred completions in the Anthropic HH dataset, delivering performance comparable to or exceeding the much more resource-intensive Best of 128 baseline. Lastly, Figure~\ref{fig:dialogue-main} illustrates that DPO attains its peak performance in relatively few training steps.

\subsection{Generalization to a new input distribution}

\begin{wraptable}{r}{0.375\textwidth}
    \small
    \vspace{-10mm}
    \begin{tabular}{ccc}
        \toprule
        & \multicolumn{2}{c}{\textbf{Win percentage against ground truth}} \\
        \cmidrule(lr){2-3}
        \textbf{Alg.} & Temp $0$ & Temp $0.25$ \\
        \midrule
        DPO & 0.36 & 0.31 \\
        PPO & 0.26 & 0.23 \\
        \bottomrule
    \end{tabular}
    \caption{GPT-4 win percentages compared to reference summaries for out-of-distribution CNN/DailyMail articles.}
    \vspace{-3mm}
    \label{tab:ood}
\end{wraptable}

To further assess how PPO and DPO perform under distribution shifts, we test the policies trained in our Reddit TL;DR summarization experiment on an out-of-domain dataset: specifically, we evaluate them on news articles from the test portion of the CNN/DailyMail corpus \citep{nallapati-etal-2016-abstractive}, employing the optimal sampling temperatures derived from TL;DR (0 and 0.25). The outcomes are detailed in Table~\ref{tab:ood}. To measure performance, we calculate the GPT-4 win rate versus ground-truth summaries, utilizing the same GPT-4 (C) prompt as in Reddit TL;DR, except substituting ``forum post'' with ``news article''. On this new distribution, DPO still significantly surpasses the PPO policy. These findings offer preliminary evidence that DPO policies exhibit comparable generalization to PPO policies, despite DPO not leveraging the additional unlabeled Reddit TL;DR prompts available to PPO.

\subsection{Corroborating GPT-4 assessments with human evaluation}
\label{sec:human-judgments}
We implement a human evaluation to test the dependability of GPT-4’s assessments, leveraging outcomes from the TL;DR summarization experiment alongside two different GPT-4 prompt types. The \textbf{GPT-4 (S)} (simple) prompt asks which summary captures the most important information from the post. The \textbf{GPT-4 (C)} (concise) prompt additionally inquires which summary is more concise; we include this prompt since GPT-4, when using the \textbf{GPT-4 (S)} prompt, tends to favor longer, more redundant summaries compared to human preferences. Complete prompt wordings are available in Appendix~\ref{app:prompts}. Our evaluation involves three pairwise comparisons: one using the best-performing method (DPO, temp. 0.25), one using the worst (PPO, temp. 1.0), and one using a 
\begin{wraptable}{r}{0.47\textwidth}
    \centering
    \small
    \vspace{-1.5mm}
    \begin{tabular}{lccc}
    \toprule
        & \textbf{DPO} & \textbf{SFT} & \textbf{PPO-1} \\
        \cmidrule(lr){2-4}
        N respondents & 272 & 122 & 199 \\
        \midrule
        GPT-4 (S) win \% & 47 & 27 & 13 \\
        GPT-4 (C) win \% & 54 & 32 & 12 \\
        Human win \% & 58 & 43 & 17 \\
        \midrule
        GPT-4 (S)-H agree & 70 & 77 & 86 \\
        GPT-4 (C)-H agree & 67 & 79 & 85 \\
        H-H agree & 65 & - & 87 \\
        \bottomrule
    \end{tabular}
    \vspace{-1mm}
    \caption{Comparison of win rates and per-judgment agreement between human and GPT-4 evaluations on TL;DR summarization data. \textbf{Levels of agreement between humans and GPT-4 are comparable to inter-human agreement.} Each trial pairs a summary from the listed method with a summary from PPO at temperature 0.}
    \vspace{-5mm}
    \label{tab:human_results}
\end{wraptable}
method with intermediate performance (SFT, temp. 0.25), chosen to capture a range of summary qualities. Each approach is compared against PPO with greedy sampling (optimal temperature). Our findings reveal that, under both prompts, GPT-4 aligns with human choices nearly as frequently as humans agree with each other, indicating that GPT-4 serves as a viable stand-in for human raters (for DPO and PPO-1, we gathered several human ratings due to the limited number of available annotators). On the whole, the \textbf{GPT-4 (C)} prompt yields win rates that more closely mirror human preferences; as a result, we employ this prompt for primary results in Section~\ref{sec:dpo-real-datasets}. Further details on the human evaluation procedure, including the rater interface and volunteer list, can be found in Appendix~\ref{app:human-study}.

\section{Discussion}
Preference-based learning offers an effective and scalable approach for training language models that are both capable and aligned with human values. In this work, we present DPO, a straightforward training method for learning from preferences that does not require reinforcement learning. Instead of forcing the preference learning task into a conventional RL framework to leverage standard RL tools, DPO establishes a correspondence between language model policies and reward functions, allowing direct optimization for human preferences using a simple cross-entropy loss. This approach avoids the need for reinforcement learning and does not sacrifice generality. Remarkably, DPO achieves results on par with or superior to popular RLHF techniques, such as those utilizing PPO, and requires almost no hyperparameter tuning. Consequently, DPO significantly lowers the complexity and effort involved in training language models guided by human feedback.

\textbf{Limitations \& Future Directions.} Our findings prompt several significant questions for further research. For instance, how does the DPO policy perform on out-of-distribution data in comparison to approaches based on explicit reward functions? Preliminary evidence indicates that DPO policies may generalize on par with models trained via PPO, though more thorough investigation is warranted. Another question is whether leveraging self-labeling with the DPO policy can make productive use of prompts without labels. Furthermore, the manifestation of reward over-optimization within the direct preference optimization context remains unclear, as does whether the modest drop in performance seen in Figure~\ref{fig:dialogue-main}-right is an example of this phenomenon. Although our experiments consider models up to 6B parameters, extending DPO to much larger, cutting-edge models presents a promising avenue for future exploration. In terms of assessment, we observe that win rates determined by GPT-4 can vary depending on the evaluation prompt; subsequent work could focus on optimizing prompt design to ensure high-quality automated evaluations. Finally, DPO’s potential extends beyond human preference-based language model training, offering opportunities for its use in training generative models across various modalities.

\end{document}