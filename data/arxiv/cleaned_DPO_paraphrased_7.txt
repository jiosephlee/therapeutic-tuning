\title{Direct Preference Optimization: Your Language Model is Secretly a Reward Model}

\begin{abstract}
Although large-scale unsupervised language models (LMs) acquire extensive world knowledge and some capacity for reasoning, it remains challenging to precisely direct their outputs due to the fully unsupervised nature of their training process. 
Current strategies for enhancing the controllability of LMs involve gathering human assessments comparing different model outputs, then fine-tuning the unsupervised LM to better align with these preferences, most commonly using reinforcement learning from human feedback (RLHF). 
However, RLHF is complicated and prone to instability, as it first trains a reward model to capture human evaluations, and then applies reinforcement learning to adjust the large LM in a way that maximizes this estimated reward while avoiding excessive deviation from the original model.
In this work, we propose a novel way of parameterizing the reward model in RLHF, which allows the optimal policy to be computed analytically in closed form, reducing the standard RLHF objective to a straightforward classification loss.
We call this approach \textit{Direct Preference Optimization} (DPO). DPO is robust, efficient, and lightweight in terms of computation, removing the need to sample from the LM during fine-tuning or to carry out extensive hyperparameter searches.
Through experiments, we demonstrate that DPO can fine-tune LMs to conform to human preferences as effectively as, or better than, prevailing techniques. In particular, DPO surpasses RLHF based on PPO in controlling generation sentiment and achieves comparable or improved quality in summarization and single-turn dialogue, while being much simpler to implement and train.
\end{abstract}

\section{Introduction}
Large-scale unsupervised language models (LMs) trained on massive corpora have demonstrated remarkable abilities~\citep{chowdhery2022palm, brown2020language, touvron2023llama, bubeck2023sparks}. Despite this, these models are exposed to data produced by humans with diverse intentions, values, and expertise. Some of these attributes are not always beneficial to emulate; for instance, although we may want an AI coding assistant to \textit{recognize} frequent programming errors so it can rectify them, we nonetheless prefer that code generation is influenced by the (sometimes uncommon) exemplary coding talent found in its training set. Likewise, we may wish for the model to be \textit{informed} about a widespread misunderstanding accepted by half the population, but we do not intend for it to assert this misconception as accurate in 50\% of relevant queries. Thus, being able to \emph{select} the model's preferred outputs and actions from its broad \textit{knowledge and capabilities} is essential for developing AI systems that are reliable, high-performing, and governable~\citep{ouyang2022training}. Although current techniques generally align LMs with human preferences through reinforcement learning (RL), we demonstrate that the RL-based objective employed by these approaches can be solved precisely using a straightforward binary cross-entropy loss, streamlining the entire preference learning process.

\begin{figure}
    \centering
    \includegraphics[width=0.999\textwidth]{figures/diagrams/teaser.png}
    \caption{\textbf{DPO trains on human preferences without using reinforcement learning.} Traditional approaches to fine-tuning language models with human feedback begin by training a reward model on prompt-response pairs annotated with human preference labels, followed by RL to optimize a policy that maximizes this reward signal. In contrast, DPO skips RL by directly optimizing for a policy that aligns with human preferences through a straightforward classification loss, learning an \textit{implicit} reward model whose optimal policy can be derived analytically.}
    \vspace{-2mm}
    \label{fig:teaser}
\end{figure}

In general, current approaches shape language models toward desired behaviors by leveraging carefully constructed datasets of human preferences that reflect what people consider safe and useful. This process of learning from preferences takes place after the model undergoes extensive unsupervised pre-training on large-scale textual data. The simplest method for preference learning involves supervised fine-tuning on human-provided demonstrations of desirable outputs. However, the most effective approaches to date rely on reinforcement learning from human (or AI) feedback (RLHF/RLAIF; \citep{christiano2017deep,bai2022constitutional}). In RLHF, a reward model is trained on human preference data and is subsequently used in an RL loop to optimize the language model so that it generates outputs with high predicted reward, all while constraining the model to remain relatively close to its pre-trained state. Although RLHF models demonstrate strong performance in conversation and code generation tasks, the RLHF procedure is substantially more intricate than standard supervised learning, requiring the training of several language models and repeated sampling from the model during the RL loop, leading to much higher computational demands.

In this work, we present a method for directly optimizing a language model to align with human preferences, bypassing the need for explicit reward modeling or reinforcement learning frameworks. Our approach, named Direct Preference Optimization (DPO), is an algorithm that implicitly targets the same objective as conventional RLHF techniques—maximizing reward while enforcing a KL-divergence regularization—but with a design that is both easy to implement and efficient to train. In essence, DPO updates the model by boosting the relative log likelihood of preferred outputs over less preferred ones, and utilizes a per-sample, adaptive importance weight that avoids the collapse seen with a basic probability ratio loss. Like established algorithms, DPO makes use of a theoretical preference framework (e.g., the Bradley-Terry model; \cite{bradley1952rankanalysis}) to assess the alignment between a reward function and observed preference judgments. However, unlike existing approaches that first train a reward model using the preference loss and then optimize a policy against this learned reward, DPO instead reformulates the preference loss so that it is a direct function of the policy. Leveraging datasets of human preference comparisons, DPO can thus optimize the policy using a straightforward binary cross entropy objective, yielding an optimal policy for an implicit reward function tailored to the preference data.

The primary contribution of our work is Direct Preference Optimization (DPO), an intuitive algorithm for training language models on preference data without relying on reinforcement learning. Experimental results demonstrate that DPO matches or surpasses the performance of current approaches—including PPO-based RLHF—when learning from preferences in areas like sentiment control, summarization, and dialogue, even with language models containing up to 6B parameters.

\section{Related Work}

Self-supervised language models, as they scale up, acquire the ability to solve certain tasks in a zero-shot manner \citep{radford2019language} or by leveraging few-shot prompts \citep{gpt3,megatron,chowdhery2022palm}. Nevertheless, their effectiveness on downstream applications and alignment with user intentions is greatly enhanced through fine-tuning on collections of instructions paired with human-generated completions \citep{mishra-etal-2022-cross,sanh2022multitask,chung2022scaling,thoppilan2022lamda}. This process, often referred to as `instruction-tuning,’ allows LLMs to generalize beyond the specific instructions found in the tuning data, thereby broadly increasing their practical utility \citep{chung2022scaling}. 

Although instruction tuning has proven highly effective, collecting \textit{relative} assessments of response quality from humans is often simpler than obtaining expert-curated demonstrations. Consequently, subsequent research has focused on further fine-tuning LLMs using datasets based on human preferences, leading to improved abilities in translation \citep{kreutzer-etal-2018-reliability}, summarization \citep{stiennon2022learning,ziegler2020finetuning}, narrative generation \citep{ziegler2020finetuning}, and instruction adherence \citep{ouyang2022training,ramamurthy2023is}. In these approaches, a neural reward model is first trained to reflect the preference data—using preference models such as the Bradley-Terry model \citep{bradley1952rankanalysis}—and then the language model itself is fine-tuned to maximize these rewards using reinforcement learning methods, most frequently REINFORCE \citep{williams1992reinforce}, proximal policy optimization (PPO; \cite{schulman2017proximal}), or related variants \citep{ramamurthy2023is}. 

A related research direction uses LLMs already fine-tuned with human feedback to generate synthetic preference data tailored to particular attributes like safety or harmlessness \citep{bai2022constitutional}, relying on minimal human supervision such as textual guidelines for the model’s annotations. This convergence brings together two main areas: one concerning the use of reinforcement learning to train language models for a variety of goals~\citep{Ranzato2015SequenceLT,paulus2018a,wu2018learning}, and another concerning broad approaches to learning from human preference data \citep{christiano2017deep,kupcsik2018learning}. 

Despite the appeal of relying on relative human preferences, applying reinforcement learning to fine-tune large language models continues to be a significant practical hurdle; the present work introduces a theoretically sound method for optimizing relative preferences that does not require reinforcement learning.

Beyond language-based scenarios, policy learning from preferences has been investigated in both bandit and reinforcement learning frameworks, with multiple strategies introduced. When using preferences or action rankings instead of traditional rewards in contextual bandit learning, this problem is known as the contextual dueling bandit (CDB; \cite{yue2012karmed,dudik2015contextual}). Without access to absolute reward signals, the theoretical study of CDBs replaces the concept of an optimal policy with a \textit{von Neumann winner}—a policy that achieves an expected win rate of at least 50\% when compared against \textit{any} alternative policy \citep{dudik2015contextual}. A key distinction is that in the CDB framework, preference information is revealed online, whereas learning from human preferences typically relies on a static, offline collection of preference-labeled action pairs \citep{yan2022human}. In a similar vein, \textit{preference-based RL} (PbRL) utilizes binary preferences from an \textit{unknown} scoring mechanism rather than explicit rewards \citep{BusaFekete2014,ruiz2023dueling}. There are several PbRL techniques, some of which can leverage off-policy preference data; these approaches often require first inferring the underlying scoring or reward function, followed by optimizing policies with respect to it \citep{jain2013learning,BusaFekete2014,christiano2017deep,sadigh2017active,kupcsik2018learning}. In contrast, our method introduces a single-stage policy learning technique that directly updates a policy to align with preference data.

\section{Preliminaries}\label{section:prelims}

We summarize the RLHF process described in \citeauthor{ziegler2020finetuning} (and subsequently in \citep{stiennon2022learning, bai2022training, ouyang2022training}). This pipeline generally consists of three main stages: (1) supervised fine-tuning (SFT), (2) collecting preference data and training a reward model, and (3) reinforcement learning-based optimization.

\textbf{SFT}: RLHF usually starts by applying supervised learning to a pre-trained LM, refining it on high-quality datasets relevant to the target tasks (such as dialogue or summarization), resulting in a model denoted as $\pisft$.

\textbf{Reward Modeling Stage}: During this stage, the SFT model receives prompts $x$ and generates pairs of responses $(y_1, y_2)\sim \pisft(y \mid x)$. These answer pairs are evaluated by human annotators, who indicate their preferred response, represented as $y_w\succ y_l \mid x$, with $y_w$ and $y_l$ signifying the chosen and unchosen completions out of $(y_1, y_2)$, respectively. It is assumed that such preferences are determined by an underlying, unknown reward function $r^*(y, x)$. Various techniques exist to model these preferences, with the Bradley-Terry (BT) model \cite{bradley1952rankanalysis} being a widely adopted method; however, broader models like Plackett-Luce \citep{plackett1975analysis, luce2012individual} can also fit this scheme if multiple ranked outputs are available. Under the BT model, the probability distribution describing human preferences $p^*$ can be expressed as:
\begin{equation}\label{eq:bradley-terry}
    p^*(y_1\succ y_2 \mid x)=\frac{\exp\left(r^*(x, y_1)\right)}{\exp\left(r^*(x, y_1)\right) + \exp\left(r^*(x, y_2)\right)}.
\end{equation}
Given a static dataset of comparisons $\mathcal{D}=\bigl\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\bigr\}_{i=1}^N$ sampled according to $p^*$, a reward function $r_{\phi}(x, y)$ is parameterized and its parameters are learned via maximum likelihood. Casting this as a binary classification task, the negative log-likelihood objective is:
\begin{equation}\label{eq:reward_model}
    \mathcal{L}_R(r_{\phi}, \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\bigl[\log \sigma(r_{\phi}(x, y_w)- r_{\phi}(x, y_l))\bigr]
\end{equation}
where $\sigma$ denotes the logistic sigmoid function. For language models, the reward network $r_{\phi}(x, y)$ is typically initialized from the SFT model $\pisft(y \mid x)$, with a linear head appended to the last transformer block to yield a single scalar reward output \cite{ziegler2020finetuning}. To promote a reward model with lower variability, previous research often standardizes reward values so that $\mathbb{E}_{x,y\sim \mathcal{D}}\left[r_\phi(x, y)\right] = 0$ for each $x$.

\textbf{RL Fine-Tuning Stage}: In the reinforcement learning phase, the previously learned reward function serves as feedback for the language model. Consistent with earlier studies~\citep{jaques2017sequence, jaques2020human}, the optimization objective is defined as
\begin{equation}\label{eq:RL}
\max_{\pi_{\theta}}  \mathbb{E}_{x\sim \mathcal{D}, y\sim \pi_{\theta}(y \mid x)}\left[r_{\phi}(x, y)\right] - \beta\mathbb{D}_{\textrm{KL}}\left[\pi_{\theta}(y\mid x)\; ||\; \piref(y\mid x)\right],
\end{equation}
where $\beta$ acts as a regularization parameter that limits divergence from the base reference policy $\piref$, specifically the starting SFT model $\pisft$. 
Typically, the policy $\pi_\theta$ of the language model is also initialized from $\pisft$. This regularization is crucial to prevent the model from straying significantly from the data distribution on which the reward model is reliable, as well as to ensure diversity in the generated outputs and avoid collapsing to a few high-reward responses. Because language generation involves discrete outputs, the objective is not directly differentiable, requiring the use of reinforcement learning methods for optimization. The prevailing technique \citep{ziegler2020finetuning, stiennon2022learning, bai2022training, ouyang2022training} constructs the reward as ${r(x, y) = r_{\phi}(x, y) -\beta (\log \pi_{\theta}(y\mid x) - \log \piref(y\mid x))}$ and maximizes it using PPO \cite{schulman2017proximal}.

\section{Direct Preference Optimization}\label{sec:DPO}

Driven by the difficulties inherent in deploying reinforcement learning techniques to large-scale tasks like language model fine-tuning, we aim to develop a straightforward method for policy optimization that directly utilizes preference data. In contrast to conventional RLHF strategies that first train a reward model and subsequently optimize it with RL, our method employs a specific parameterization of the reward function, allowing us to analytically determine the optimal policy—eliminating the need for an RL training process. 

As we will elaborate on shortly, the central idea is to use an explicit, analytical correspondence between reward functions and their respective optimal policies, thereby converting a loss over rewards into a loss over policies. This variable substitution approach bypasses the need to train a separate reward model, while still optimizing with respect to established human preference models like the Bradley-Terry framework. In this setup, the policy network serves the dual role of both the language model and the underlying (implicit) reward function.

\textbf{Derivation of the DPO objective.} We begin with the standard RL objective as in previous work, Eq.~\ref{eq:RL}, utilizing a general reward function $r$. As demonstrated by earlier studies~\citep{peters2007reinforcement, peng2019advantage, korbak2022reinforcement, go2023aligning}, it follows directly that the optimal policy for the KL-regularized reward maximization problem in Eq.~\ref{eq:RL} has the structure:
\begin{equation}\label{eq:op_policy}
    \pi_r(y\mid x) = \frac{1}{Z(x)}\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right),
\end{equation}%
where $Z(x) =\sum_{y}\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)$ denotes the normalizing constant. For detailed derivation, refer to Appendix~\ref{app:derivation1}. Even if we have an MLE-based estimate $r_{\phi}$ for the actual reward function $r^*$, directly computing the partition function $Z(x)$ remains computationally challenging~\citep{korbak2022reinforcement, go2023aligning}, which limits the practical applicability of this policy form. However, we can algebraically manipulate Eq.~\ref{eq:op_policy} to rewrite the reward in terms of the optimal policy $\pi_r$, the reference policy $\piref$, and the unknown $Z(\cdot)$. Specifically, by taking logarithms on both sides of Eq.~\ref{eq:op_policy} and rearranging terms, we get:
\begin{equation}\label{eq:main_eq}
    r(x,y) =\beta \log \frac{\pi_r(y\mid x)}{\piref(y\mid x)} + \beta \log Z(x).
\end{equation}
This reparametrization can also be applied to the true reward $r^*$ and its associated optimal policy $\pi^*$. Importantly, the Bradley-Terry model relies exclusively on differences between rewards for two outputs, i.e., ${p^*(y_1 \succ y_2 \mid x) = \sigma(r^*(x, y_1) - r^*(x, y_2))}$. By plugging in the expression from Eq.~\ref{eq:main_eq} for $r^*(x, y)$ into the preference model Eq.~\ref{eq:bradley-terry}, we see that the partition function terms eliminate each other, allowing the human preference probability to be written entirely in terms of the optimal policy $\pi^*$ and the reference policy $\piref$. Hence, the optimal RLHF policy $\pi^*$ as per the Bradley-Terry setup fulfills:
\begin{equation}\label{eq:objective}
    p^*(y_1\succ y_2 \mid x)=\frac{1}{1 + \exp\left(\beta \log \frac{\pi^*(y_2\mid x)}{\piref(y_2\mid x)} - \beta \log \frac{\pi^*(y_1\mid x)}{\piref(y_1\mid x)}\right)}
\end{equation}
The complete derivation can be found in Appendix~\ref{app:derivation2}. While Eq.~\ref{eq:objective} is grounded in the Bradley-Terry framework, analogous results can be obtained for broader preference models such as the Plackett-Luce family~\citep{plackett1975analysis, luce2012individual}, with details provided in Appendix~\ref{app:plackett_luce_models}.

Having expressed the human preference probabilities in terms of the optimal policy rather than the reward function, we are now able to define a maximum likelihood objective for a parameterized policy $\pi_\theta$. In parallel with the reward model approach (cf. Eq.~\ref{eq:reward_model}), the objective for our policy can be written as:
\begin{equation}\label{eq:optimum_model}
    \mathcal{L}_\text{DPO}(\pi_{\theta}; \piref) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\left[\log \sigma \left(\beta \log \frac{\pi_{\theta}(y_w\mid x)}{\piref(y_w\mid x)} - \beta \log \frac{\pi_{\theta}(y_l\mid x)}{\piref(y_l\mid x)}\right)\right].
\end{equation}
Through this formulation, we implicitly learn a reward with a different parameterization, such that the policy $\pi_\theta$ is optimal with respect to it. Additionally, since our framework is equivalent to fitting a reparameterized Bradley-Terry model, it inherits useful theoretical guarantees, including consistency under appropriate conditions on the distribution of preference data \cite{bong2022generalized}. Further theoretical aspects of DPO and its comparison with related approaches are explored in Section~\ref{sec:theory}.

\textbf{What is the effect of the DPO update?} To gain a mechanistic insight into DPO, it's helpful to examine the gradient of the loss function $\mathcal{L}_\text{DPO}$. The derivative with respect to the parameters $\theta$ can be expressed as:
\begin{multline*}\label{eq:gradient}
    \nabla_\theta \mathcal{L}_\text{DPO}(\pi_\theta;\piref) = \\ -\beta\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \bigg[\underbrace{\sigma(\hat{r}_\theta(x, y_l) - \hat{r}_\theta(x, y_w))}_\text{greater weight when reward ranking is incorrect}\bigg[\underbrace{\nabla_\theta\log \pi(y_w \mid x)}_\text{promote $y_w$} - \underbrace{\nabla_\theta\log\pi(y_l \mid x)}_\text{demote $y_l$}\bigg]\bigg],
\end{multline*}
with $\hat{r}_\theta(x, y) = \beta \log \frac{\pi_\theta(y \mid x)}{\piref(y \mid x)}$ representing the implicit reward assigned by the model $\pi_\theta$ relative to the reference $\piref$ (see Section~\ref{sec:theory} for details). In essence, the gradient of $\mathcal{L}_\text{DPO}$ nudges the model to increase the probability of preferred responses $y_w$ and decrease the probability of less preferred responses $y_l$. Crucially, each training instance is weighted according to how much more favorably the implicit reward model $\hat{r}_\theta$ evaluates the dispreferred option, scaled by $\beta$—in other words, the degree to which the reward model misorders the samples, factoring in the KL regularization strength. Our results highlight that this weighting is crucial; omitting the weighting factor leads to pathological model behavior (see Appendix Table~\ref{tab:unlikelihood_generations}).

\textbf{DPO summary.}  
The standard DPO process consists of: 1) Generating completions $y_1, y_2 \sim \piref(\cdot \mid x)$ for each prompt $x$, and annotating them according to human preferences to form an offline dataset of preferences $\mathcal{D} = \{x^{(i)}, y_w^{(i)}, y_l^{(i)}\}_{i=1}^N$, and 2) training the language model $\pi_\theta$ to minimize the objective $\mathcal{L}_\text{DPO}$ for the provided $\piref$, dataset $\mathcal{D}$, and chosen $\beta$.  
In real-world scenarios, it is preferable to use existing public preference datasets instead of creating new samples and collecting fresh human annotations. As these preference datasets are typically constructed with $\pisft$, we set $\piref = \pisft$ whenever possible. If $\pisft$ is unavailable, we instead initialize $\piref$ by maximizing the likelihood of the preferred completions $(x, y_w)$, i.e., ${\piref = \argmax_{\pi}\mathbb{E}_{x, y_w \sim \mathcal{D}}\left[\log \pi(y_w \mid x)\right]}$. This approach helps to reduce distributional mismatch between the intractable true reference distribution and the practical $\piref$ employed in DPO. For more information about implementation choices and hyperparameter settings, see Appendix~\ref{app:implementation}.

\section{Theoretical Examination of DPO}
Here, we offer a deeper analysis of the DPO approach, present theoretical justification, and connect the benefits of DPO to the limitations encountered by actor-critic methods in RLHF (like PPO~\cite{schulman2017proximal}).

\label{sec:theory}

\subsection{Your Language Model Is Secretly a Reward Model} DPO enables both reward modeling and policy learning without the need for explicitly training a reward function or running RL, instead relying solely on a single maximum likelihood objective. Importantly, the optimization target in Eq. \ref{eq:main_eq} corresponds to a Bradley-Terry model, where the reward is parameterized as $r^*(x, y) = \beta \log\frac{\pi^*_\theta(y \mid x)}{\piref(y \mid x)}$. We train our parameterized model $\pi_{\theta}$, which is mathematically equivalent to optimizing the reward model as shown in Eq. \ref{eq:reward_model} after applying a variable transformation. This section develops the theoretical foundation for this reparameterization, demonstrates that it does not restrict the family of reward models that can be learned, and proves it permits exact retrieval of the optimal policy. We start by introducing an equivalence relation among reward functions.

\begin{definition}
Two reward functions $r(x, y)$ and $r'(x, y)$ are considered equivalent if and only if there exists a function $f$ such that $r(x, y) - r'(x, y) = f(x)$.
\end{definition}
Clearly, this forms an equivalence relation, dividing the collection of reward functions into equivalence classes. This leads to the following two lemmas:

\begin{lemma}\label{lemma:same_prefrence} Within the Plackett-Luce model, and specifically under the Bradley-Terry preference structure, any two reward functions belonging to the same class generate identical preference distributions.
\end{lemma}

\begin{lemma}\label{lemma:same_policy}
    Any two reward functions belonging to the same equivalence class yield the identical optimal policy in the constrained RL setting.
\end{lemma}
The arguments are direct and are provided in Appendix \ref{app:lemma1}. The first lemma highlights a well-known identifiability problem inherent to the Plackett-Luce family of models \cite{plackett1975analysis}. Because of this ambiguity, it is customary to add further identifiability conditions to ensure that MLE solutions to Eq. \ref{eq:reward_model} are meaningful \cite{bong2022generalized}. The second lemma indicates that all reward functions within a given class correspond to the same optimal policy; therefore, for our ultimate goal, it suffices to recover any single reward function from the optimal equivalence class. We formally establish the following result in Appendix~\ref{app:thm1}:
\begin{theorem}\label{thm:main}
    Assuming mild conditions, every reward equivalence class compatible with Plackett-Luce models (and Bradley-Terry in particular) can be expressed in the form ${r(x, y) = \beta \log \frac{\pi(y\mid x)}{\piref(y\mid x)}}$, where $\pi(y\mid x)$ is some model and $\piref(y \mid x)$ is a fixed reference distribution.
\end{theorem}
\begin{sproof}
    Let $r(x, y)$ be any reward function, which defines an associated optimal policy $\pi_r(y \mid x)$ as described in Eq. \ref{eq:op_policy}. We aim to demonstrate that another reward function in the equivalence class of $r$ admits the above reparameterization. Define the transformation $f$ by  
\begin{equation}
    f(r; \piref, \beta)(x, y) = r(x, y) - \beta\log\sum_{y}\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)
\end{equation}
Here, $f$ normalizes the reward by subtracting the logarithm of the partition function of $\pi_r$. Since this normalization depends solely on $x$, $f(r; \piref, \beta)(x, y)$ remains in the equivalence class of $r(x, y)$. Now, by substituting $r$ on the right-hand side of Eq.~\ref{eq:main_eq} (valid for any reward function), we get $f(r; \piref, \beta)(x, y) = \beta \log \frac{\pi_r(y\mid x)}{\piref(y\mid x)}$. In other words, the transformation $f$ yields a reward function of the intended structure within the equivalence class of $r$, showing that our reparameterization covers all possible reward models without loss of generality.
\end{sproof}
Alternatively, Theorem~\ref{thm:main} can be interpreted as uniquely specifying which reward function from each equivalence class the DPO reparameterization selects—specifically, the function that satisfies:
\begin{equation}\label{eq:lag_p}
     \sum_{y}\underbrace{\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)}_{=\pi(y\mid x)\text{, as per Theorem~\ref{thm:main}'s reparameterization}} = 1,
\end{equation}
meaning that $\pi(y\mid x)$ forms a proper probability distribution (i.e., it is non-negative and sums to one).
Moreover, as seen from Eq.~\ref{eq:op_policy}, Eq.~\ref{eq:lag_p} represents the normalization constant, or partition function, of the optimal policy induced by the reward $r(x, y)$. The central idea behind the DPO algorithm is that by placing suitable constraints on the otherwise ambiguous Plackett-Luce (or Bradley-Terry) family of preference models, we can maintain the same family of possible reward functions, yet explicitly enforce a closed-form expression for the optimal policy in Eq. \ref{eq:op_policy} for every prompt $x$.

\subsection{Actor-Critic Algorithm Instabilities}
Our framework also allows us to analyze the instabilities present in standard actor-critic methods utilized in RLHF, such as PPO. Specifically, we mirror the RLHF pipeline and concentrate on the RL fine-tuning phase described in Section \ref{section:prelims}. Connections can be made to the control as inference paradigm \cite{levine2018reinforcement} when considering the constrained RL problem from \ref{eq:RL}. With a parameterized policy $\pi_{\theta}(y\mid x)$, the goal is to minimize $\mathbb{D}_{\text{KL}}[\pi_{\theta}(y|x) \| \pi^*(y\mid x)]$, where $\pi^*$ represents the optimal policy defined in Eq. \ref{eq:optimum_model} and shaped by the reward function $r_{\phi}(y, x)$. Through some manipulation, this results in the following optimization objective:
\begin{equation}\label{eq:AC}
    \max_{\pi_{\theta}}\mathbb{E}_{\pi_{\theta}(y\mid x)}\left[\underbrace{r_{\phi}(x, y) -\beta\log\sum_{y}\piref(y\mid x)\exp\left(\frac{1}{\beta}r_{\phi}(x, y)\right)}_{f(r_{\phi}, \piref, \beta)} - \underbrace{\beta\log\frac{\pi_{\theta}(y\mid x)}{\piref(y\mid x)}}_{\text{KL}}\right]
\end{equation}
This objective matches those used in previous literature \citep{ziegler2020finetuning, stiennon2022learning, bai2022training, ouyang2022training}, leveraging the DPO-equivalent reward for the specific reward class $r_{\phi}$. Within this formulation, the normalization inside $f(r_{\phi}, \piref, \beta)$ can be interpreted as the soft value function corresponding to the reference policy $\piref$. Although this normalization component does not influence the optimal policy, omitting it can result in high-variance policy gradients, thereby destabilizing training. One can introduce a learned value function to estimate the normalization, but this adds its own optimization challenges. As an alternative, earlier approaches have normalized rewards using a human completion baseline, effectively providing a single-sample Monte Carlo approximation of the normalization factor. By contrast, the DPO reparameterization produces a reward function that is baseline-free.

\section{Experiments}
Here, we empirically assess how effectively DPO can learn policies directly from preference data. Initially, in a controlled text generation environment, we investigate the efficiency with which DPO balances reward maximization against minimizing the KL-divergence to the reference policy, and compare it to widely used preference optimization methods like PPO. We then test DPO on larger model architectures and more challenging RLHF tasks, such as dialogue and summarization. Our findings indicate that, even with minimal hyperparameter tuning, DPO consistently matches or surpasses strong baselines like RLHF with PPO, as well as surpassing the strategy of selecting the best out of $N$ samples based on a learned reward model. Prior to detailing our findings, we outline the experimental protocols used; further specifics are provided in Appendix~\ref{app:exp_details}.

\textbf{Tasks.} Our study investigates three open-domain text generation tasks. For every task, methods are trained to optimize a policy based on a preference dataset $\mathcal{D}=\bigl\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\bigr\}_{i=1}^N$. In the \textbf{controlled sentiment generation} task, $x$ is a sentence prefix sampled from the IMDb movie review dataset \cite{maas-EtAl:2011:ACL-HLT2011}, and the aim is for the policy to produce a continuation $y$ expressing positive sentiment. For an automated evaluation, we \textit{generate} preference pairs using outputs scored by a pre-trained sentiment classifier, such that $p(\text{positive}\mid x,y_w)>p(\text{positive}\mid x,y_l)$. For SFT, GPT-2-large is fine-tuned until convergence using reviews in the IMDb training set (additional details in App~\ref{app:sentiment_details}). In the \textbf{summarization} task, $x$ corresponds to a Reddit forum post, and the policy is required to output a summary $y$ capturing the core content. Following prior work, we utilize the Reddit TL;DR summarization dataset \citep{volske-etal-2017-tl} and a set of human preferences collected by \citeauthor{stiennon2022learning}. The SFT model is obtained by fine-tuning on human-authored post summaries\footnote{\url{https://huggingface.co/CarperAI/openai_summarize_tldr_sft}} using the TRLX \citep{leandro_von_werra_2023_7790115} RLHF library. The set of human preferences was collected on outputs from a distinct but related SFT model by \citeauthor{stiennon2022learning}. Lastly, for the \textbf{single-turn dialogue} task, $x$ is a user prompt, which may include questions on any topic such as astrophysics or personal advice. Here, the policy generates an informative and engaging answer $y$; we employ the Anthropic Helpful and Harmless dialogue dataset \citep{bai2022training}, which consists of 170k exchanges between people and a virtual assistant. Each conversation ends with two responses generated by a large (though unspecified) language model, along with a label marking which answer the human preferred. Since there is no pre-built SFT model for this task, we construct one by fine-tuning a publicly available language model only on the preferred completions.

\begin{figure}
    \centering
    \includegraphics[width=0.50\textwidth]{figures/results/frontier.pdf}
    \includegraphics[width=0.49\textwidth]{figures/results/tldr_winrate_vs_temp.pdf}
    \caption{\textbf{Left.} The trade-off curve of expected reward against KL divergence from the reference policy. DPO consistently attains the highest expected reward for every KL level, highlighting its optimization effectiveness. \textbf{Right.} TL;DR summarization win rates compared to human-authored summaries, as judged by GPT-4. DPO outperforms the best results achieved by PPO on summarization tasks and remains more stable across varying sampling temperatures.}
    \vspace{-2mm}
    \label{fig:frontier-tldr-main}
\end{figure}

\textbf{Evaluation.} We employ two distinct evaluation strategies in our experiments. To assess how well each algorithm optimizes the constrained reward maximization objective, we examine their performance in controlled sentiment generation by plotting the trade-off frontier between obtained reward and KL-divergence from the reference policy; this is feasible because the ground-truth reward function (a sentiment classifier) is available. In contrast, since the true reward function is generally inaccessible in real-world scenarios, we instead measure each algorithm's \textit{win rate} relative to a baseline policy, utilizing GPT-4 as a stand-in for human judgment to evaluate summary quality and helpfulness of responses in summarization and single-turn dialogue tasks, respectively. For summarization, the baseline consists of reference summaries from the test set; for dialogue, we use the preferred response from the test data as the baseline. Although recent research indicates that LMs may outperform traditional automatic metrics as evaluators \citep{Chen2023ExploringTU}, we also carry out a human study to validate the use of GPT-4 for evaluation in Sec.~\ref{sec:human-judgments}. Our findings show that GPT-4's evaluations are highly consistent with those of humans, with human-GPT-4 agreement equaling or surpassing typical inter-annotator agreement.

\textbf{Methods.} Beyond DPO, we assess a variety of established techniques for aligning language models with human preferences. For the summarization task, we employ zero-shot prompting using \textbf{GPT-J} \citep{gpt-j}, while for the dialogue task we utilize 2-shot prompting with \textbf{Pythia-2.8B} \citep{biderman2023pythia}. We also include the \textbf{SFT} model, as well as \textbf{Preferred-FT}, which involves supervised fine-tuning on the preferred output $y_w$—sourced from the SFT model for controlled sentiment and summarization, or from a general language model in single-turn dialogue. Another approach we examine is \textbf{Unlikelihood}~\citep{welleck2019neural}, which trains the policy to increase the likelihood of $y_w$ while actively decreasing the probability of $y_l$, incorporating an optional `unlikelihood' weighting coefficient $\alpha\in[0,1]$. Our analysis further covers \textbf{PPO} \citep{schulman2017proximal}, using a reward model derived from preference data, and \textbf{PPO-GT}, an oracle model that learns directly from the true reward function in the controlled sentiment scenario. For sentiment experiments, we utilize two PPO-GT variants: an off-the-shelf implementation \cite{leandro_von_werra_2023_7790115}, and an adapted version with normalized rewards and optimized hyperparameters (we also implement these modifications for standard PPO with learned rewards). Lastly, we include the \textbf{Best of $N$} baseline, in which $N$ completions are generated by the SFT model (or Preferred-FT for dialogue), and the response with the highest score—assigned by a reward model trained on preference data—is selected. Despite its effectiveness, this approach decouples reward model quality from PPO training and becomes computationally infeasible for even moderate $N$, as it necessitates sampling $N$ completions per query during inference.

\subsection{How well can DPO optimize the RLHF objective?}

\begin{figure}
    \centering
    \includegraphics[width=0.50\textwidth]{figures/results/dialogue_winrate_vs_temp.pdf}
    \includegraphics[width=0.49\textwidth]{figures/results/dialogue_winrate_vs_steps.pdf}
    \caption{\textbf{Left.} Win rates as assessed by GPT-4 on Anthropic-HH single-turn dialogue; among the compared methods, only DPO surpasses the selected summaries from the Anthropic-HH evaluation set. \textbf{Right.} Win rates across various sampling temperatures during the training process. DPO consistently outperforms the dataset labels throughout training, regardless of the sampling temperature.}
    \vspace{-2mm}
    \label{fig:dialogue-main}
\end{figure}

The KL-constrained reward maximization objective commonly used in RLHF methods trades off reward optimization against maintaining closeness to the reference policy. Consequently, when evaluating algorithms, it is important to consider not only the obtained reward but also the KL divergence; achieving marginally higher rewards with a substantially larger KL is often undesirable. Figure~\ref{fig:frontier-tldr-main} depicts the reward versus KL tradeoff for several algorithms in the sentiment analysis task. For each algorithm, we run multiple experiments, each with a different value of the policy conservativeness hyperparameter (target KL $\in\{3,6,9,12\}$ for PPO, $\beta \in \{0.05,0.1,1,5\}$, $\alpha\in\{0.05,0.1,0.5,1\}$ for unlikelihood, and varied random seeds for preferred-FT), resulting in a total of 22 training runs. At intervals of every 100 training steps until convergence, we assess each trained policy using a fixed test prompt set, measuring both the average reward according to the actual reward function and the mean sequence-level KL divergence\footnote{Meaning the sum over the timestep-wise KL-divergences.} to the reference policy $\text{KL}\left(\pi\mid \mid \piref\right)$. Our findings reveal that DPO yields the most effective reward-KL tradeoff, simultaneously attaining the highest reward and keeping KL low. This observation is particularly striking for two reasons. First, both DPO and PPO target the same optimization objective, yet DPO achieves substantially greater efficiency, with its reward/KL curve strictly surpassing that of PPO. Second, DPO delivers a superior reward-KL frontier compared to PPO, \emph{even when PPO has access to the true reward signal} (PPO-GT).

\subsection{Can DPO be applied to large-scale real preference datasets?}
\label{sec:dpo-real-datasets}
We proceed to assess DPO’s fine-tuning effectiveness on tasks involving summarization and single-turn dialogue. In the case of summarization, automatic metrics like ROUGE are often weakly aligned with human judgment~\citep{stiennon2022learning}. Previous research has demonstrated that fine-tuning language models with PPO on human feedback can yield more useful summaries. To compare various approaches, we generate outputs using the test set from the TL;DR summarization dataset and calculate their average win rate when pitted against the reference summaries. For each method, generations are sampled across temperatures ranging from 0.0 to 1.0, with win rates reported in Figure~\ref{fig:frontier-tldr-main} (right). DPO, PPO, and Preferred-FT all utilize the same GPT-J SFT base model\footnote{\url{https://huggingface.co/CarperAI/openai_summarize_tldr_sft}} for fine-tuning. Our analysis shows that at temperature 0.0, DPO attains an approximate win rate of 61\%, outperforming PPO, which achieves about 57\% at its optimal setting (temperature 0.0). Furthermore, DPO secures a superior maximum win rate compared to the strongest $N$ baseline. It is important to mention that we did not extensively optimize the $\beta$ hyperparameter for DPO, indicating that actual performance could be even higher. Additionally, DPO demonstrates greater resilience to changes in sampling temperature relative to PPO, whose results can drop to the level of the original GPT-J model as the temperature increases. Preferred-FT, on the other hand, offers limited improvement over SFT. In Section~\ref{sec:human-judgments}, we further contrast DPO and PPO using human preference evaluations, where DPO samples at temperature 0.25 are selected over PPO outputs at temperature 0 in 58\% of comparisons.

For single-turn dialogue tasks, we assess various approaches on the portion of the Anthropic HH dataset \citep{bai2022training} test split that involves a single human-assistant exchange. For GPT-4 evaluations, we use the preferred completions from the test set as references to calculate the win rate across methods. Since a standard SFT model does not exist for this benchmark, we initiate with the pre-trained Pythia-2.8B, apply Preferred-FT to fine-tune a reference model on the selected completions to ensure in-distribution outputs, and subsequently train using DPO. We also benchmark against the top completion out of 128 generated by Preferred-FT (with the Best of $N$ baseline saturating at $N=128$ for this dataset; see Appendix Figure~\ref{fig:best-of-n}), as well as a 2-shot prompted variant of the Pythia-2.8B base model. Our findings indicate that DPO matches or exceeds the best results for optimal temperature settings of each method. Additionally, we evaluate an RLHF model trained using PPO on the Anthropic HH dataset \footnote{\url{https://huggingface.co/reciprocate/ppo_hh_pythia-6B}} provided by a reputable source \footnote{\url{https://github.com/CarperAI/trlx/tree/main/examples/hh}}, but do not identify any prompt or sampling temperature configuration that surpasses the base Pythia-2.8B model's performance. Based on our observations with TL;DR and considering both methods optimize the same reward objective, we treat Best of 128 as an approximate indicator of PPO-level effectiveness. In summary, DPO stands out as the only computationally efficient technique that outperforms the preferred completions on the Anthropic HH dataset and delivers results on par with or better than the resource-intensive Best of 128 baseline. Lastly, as seen in Figure~\ref{fig:dialogue-main}, DPO achieves its peak performance in relatively few training steps.

\subsection{Generalization to a new input distribution}

\begin{wraptable}{r}{0.375\textwidth}
    \small
    \vspace{-10mm}
    \begin{tabular}{ccc}
        \toprule
        & \multicolumn{2}{c}{\textbf{Victory Rate Compared to Ground Truth}} \\
        \cmidrule(lr){2-3}
        \textbf{Algorithm} & Temp $0$ & Temp $0.25$ \\
        \midrule
        DPO & 0.36 & 0.31 \\
        PPO & 0.26 & 0.23 \\
        \bottomrule
    \end{tabular}
    \caption{GPT-4 victory rates versus reference summaries on out-of-distribution CNN/DailyMail articles.}
    \vspace{-3mm}
    \label{tab:ood}
\end{wraptable}

To further assess how PPO and DPO perform under distributional changes, we tested both the PPO and DPO policies trained in our Reddit TL;DR summarization setup on an alternate dataset: the news articles found in the test set of the CNN/DailyMail corpus \citep{nallapati-etal-2016-abstractive}, employing the top sampling temperatures from TL;DR (0 and 0.25). Table~\ref{tab:ood} displays these findings. We measured the GPT-4 win rate versus the gold-standard summaries from the datasets, utilizing the same GPT-4 (C) prompt as in Reddit TL;DR, but swapping ``forum post'' for ``news article''. On this shifted distribution, DPO maintains a notable advantage over the PPO policy. These findings offer preliminary evidence that DPO policies are capable of generalizing at least as well as PPO policies, despite DPO lacking access to the extra unlabeled Reddit TL;DR prompts available to PPO.

\subsection{Corroborating GPT-4 Assessments with Human Evaluations}
\label{sec:human-judgments}
We carry out a user study to assess the trustworthiness of GPT-4's evaluations by leveraging results from the TL;DR summarization task and two distinct prompting approaches for GPT-4. The \textbf{GPT-4 (S)} (simple) prompt directly asks which summary better encapsulates the essential content of the post. In contrast, the \textbf{GPT-4 (C)} (concise) prompt requests which summary is both more informative and concise; we include this prompt since GPT-4, when using the \textbf{GPT-4 (S)} prompt, tends to select longer and more redundant summaries than human raters do. Full prompt wording is provided in Appendix~\ref{app:prompts}. We set up three separate comparisons, selecting the top-performing model (DPO, temp. 0.25), the lowest performer (PPO, temp. 1.0), and a model with intermediate results (SFT, temp. 0.25), aiming to capture a spectrum of summary qualities; all are evaluated against the outputs of PPO with greedy sampling (the best-performing temperature for PPO).

\begin{wraptable}{r}{0.47\textwidth}
    \centering
    \small
    \vspace{-1.5mm}
    \begin{tabular}{lccc}
    \toprule
        & \textbf{DPO} & \textbf{SFT} & \textbf{PPO-1} \\
        \cmidrule(lr){2-4}
        N respondents & 272 & 122 & 199 \\
        \midrule
        GPT-4 (S) win \% & 47 & 27 & 13 \\
        GPT-4 (C) win \% & 54 & 32 & 12 \\
        Human win \% & 58 & 43 & 17 \\
        \midrule
        GPT-4 (S)-H agree & 70 & 77 & 86 \\
        GPT-4 (C)-H agree & 67 & 79 & 85 \\
        H-H agree & 65 & - & 87 \\
        \bottomrule
    \end{tabular}
    \vspace{-1mm}
    \caption{A comparison of win percentages and agreement rates between humans and GPT-4 on TL;DR summary pairs. \textbf{Human raters show similar agreement with GPT-4 as they do with each other.} In each experiment, a summary from the specified model is compared to a summary from PPO at temperature 0.}
    \vspace{-5mm}
    \label{tab:human_results}
\end{wraptable}

Our analysis indicates that, for both prompts, GPT-4's selections align with human judgments at rates comparable to the agreement levels among different human raters themselves, implying that GPT-4 serves as a suitable stand-in for human evaluation (multiple human ratings were only gathered for the DPO and PPO-1 experiments, given resource constraints). Notably, the \textbf{GPT-4 (C)} prompt produces win rates that more closely mirror human preferences, and thus this prompt is used for the principal results in Section~\ref{sec:dpo-real-datasets}. Further methodological details—including the rater interface and volunteer participant information—can be found in Appendix~\ref{app:human-study}.

\section{Discussion}
Preference-based learning offers a robust and scalable approach to developing language models that are both capable and aligned. In this work, we have presented DPO, a straightforward method for training language models using preference data without relying on reinforcement learning. Instead of forcing the preference learning task into the mold of traditional RL to take advantage of standard RL methods, DPO establishes a direct correspondence between language model policies and reward functions. This allows models to be trained directly on human preferences using a simple cross-entropy loss, completely bypassing the need for reinforcement learning and without sacrificing generality. Remarkably, DPO achieves performance on par with, or superior to, current RLHF approaches—such as those built on PPO—while requiring almost no hyperparameter tuning. As a result, DPO significantly lowers the hurdles for leveraging human preferences in training language models.

\textbf{Limitations \& Future Directions.} Our findings prompt several key questions for subsequent research. How well does the DPO policy handle out-of-distribution scenarios compared to approaches that utilize an explicit reward signal? Preliminary evidence indicates that DPO policies achieve generalization on par with PPO-based approaches, though a more thorough investigation is warranted. For instance, could employing self-labeling with the DPO policy similarly leverage unlabeled prompts effectively? Additionally, how does over-optimization of the reward function appear within the direct preference optimization framework, and could the modest drop in performance seen in Figure~\ref{fig:dialogue-main}-right be an example of this phenomenon? Furthermore, while our analysis covers models up to 6B parameters, expanding DPO to much larger, state-of-the-art models presents an intriguing avenue for exploration. Concerning evaluation, we observe that GPT-4's calculated win rates depend on the specific prompt used; future research might explore optimal methods for eliciting reliable judgments from automated evaluators. Lastly, DPO could be applied not only to training language models via human feedback but also extended to generative modeling tasks in other domains.

\end{document}