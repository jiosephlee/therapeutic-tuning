\title{Direct Preference Optimization: Your Language Model is Secretly a Reward Model}

\begin{abstract}
Although large-scale unsupervised language models (LMs) acquire extensive world knowledge and exhibit some reasoning capabilities, exerting precise control over their outputs remains a challenge due to the fully unsupervised training process.
To achieve better steerability, current techniques gather human judgments on the comparative quality of model outputs and subsequently fine-tune the unsupervised LM to better reflect these preferences, typically using reinforcement learning from human feedback (RLHF).
However, RLHF involves a complex and often unstable pipeline, where a reward model is first trained to approximate human preferences, followed by reinforcement learning that adjusts the LM to maximize this reward signal while keeping it close to the initial model.
In this work, we propose a novel way of parameterizing the reward model within RLHF, which makes it possible to derive the associated optimal policy analytically, thereby reducing the RLHF process to optimizing a straightforward classification loss.
This approach, which we term \textit{Direct Preference Optimization} (DPO), is robust, effective, and requires much less computation, removing the need for LM sampling during fine-tuning or extensive hyperparameter searches.
Empirically, we demonstrate that DPO is capable of fine-tuning LMs to match or surpass the alignment with human preferences achieved by prior approaches. In particular, DPO outperforms PPO-based RLHF in controlling output sentiment and achieves equal or higher quality for tasks such as summarization and single-turn dialogue, all while being much easier to implement and train.
\end{abstract}

\section{Introduction}
Large-scale unsupervised language models (LMs) trained on massive corpora have demonstrated unexpected abilities~\citep{chowdhery2022palm, brown2020language, touvron2023llama,bubeck2023sparks}. Despite this, these models learn from human-created data encompassing a broad spectrum of intentions, priorities, and expertise. Not all of these human tendencies are desirable to replicate; for instance, while we want an AI coding assistant to \textit{recognize} common programming errors to be able to fix them, we still prefer it to \textit{generate} code that reflects the infrequent but high-level coding proficiency found in its dataset. In the same vein, a language model should be \textit{informed} about widespread misconceptions—say, one that half of people believe—yet we do not want it to assert this misconception as true in half of its responses! Thus, the ability to \emph{directly shape the model's output and conduct}, drawing from its extensive \textit{knowledge and skills}, is essential for creating AI that is safe, reliable, and manageable \citep{ouyang2022training}. Although current approaches generally guide LMs toward human preferences using reinforcement learning (RL), we will demonstrate that the RL-based objective utilized in these methods can in fact be exactly minimized using a straightforward binary cross-entropy loss, thus streamlining the entire preference optimization process.

\begin{figure}
    \centering
    \includegraphics[width=0.999\textwidth]{figures/diagrams/teaser.png}
    \caption{\textbf{DPO aligns with human preferences without the need for reinforcement learning.} Traditional strategies for fine-tuning language models using human feedback typically involve first training a reward model on a collection of prompts and human preference judgments over response pairs, followed by using RL to identify a policy that maximizes this learned reward. In contrast, DPO directly tunes the policy to adhere to human preferences using a straightforward classification objective, effectively learning an \textit{implicit} reward model from which the optimal policy can be analytically derived.}
    \vspace{-2mm}
    \label{fig:teaser}
\end{figure}

Broadly speaking, current approaches impart desired behaviors to language models by leveraging carefully curated human preference datasets that reflect behaviors humans deem safe and useful. This process of preference learning typically follows an initial phase of large-scale, unsupervised pre-training on extensive text corpora. The most direct method for preference learning involves supervised fine-tuning using high-quality human-generated responses. However, the most effective family of techniques is reinforcement learning from human (or AI) feedback (RLHF/RLAIF; \citep{christiano2017deep,bai2022constitutional}). In RLHF, a reward model is trained on human preference data, and reinforcement learning is then employed to steer the language model’s outputs towards high-reward responses while ensuring the model does not stray too far from its original state. Although RLHF leads to models with outstanding conversational and coding capabilities, the RLHF process is substantially more complicated than straightforward supervised learning. It requires training multiple language models, repeatedly sampling from the model during training, and as a result, imposes heavy computational demands.

In this work, we demonstrate how a language model can be directly tuned to align with human preferences, bypassing explicit reward modeling and reinforcement learning. We introduce Direct Preference Optimization (DPO), a method that implicitly targets the same objective as conventional RLHF approaches—maximizing reward while maintaining a KL-divergence constraint—yet is easy to implement and train. Conceptually, DPO updates the model by boosting the log probability ratio of preferred responses over less preferred ones, incorporating a dynamic, per-example weighting to avoid model collapse observed with a basic probability ratio approach. Like prior methods, DPO assumes a theoretical preference framework (such as the Bradley-Terry model; \cite{bradley1952rankanalysis}) to assess the alignment of a reward function with actual preference data. But, rather than using the preference model to establish a loss for training a reward model followed by policy optimization, DPO leverages a variable transformation to express the preference loss directly in terms of the policy. With access to a dataset capturing human choices among model outputs, DPO can optimize a policy using a simple binary cross-entropy loss, yielding the optimal policy under a latent reward function inferred from the preferences.

The core contribution of our work is Direct Preference Optimization (DPO), a straightforward algorithm that trains language models from preference data without relying on reinforcement learning. Through our experiments, we demonstrate that DPO matches or surpasses the effectiveness of current techniques—including PPO-based RLHF—for preference-based learning across tasks like sentiment control, summarization, and dialogue, utilizing language models with as many as 6B parameters.

\section{Related Work}

Self-supervised language models of increasing size have demonstrated the capability to solve certain tasks in a zero-shot manner \citep{radford2019language} or with minimal examples through few-shot prompting \citep{gpt3,megatron,chowdhery2022palm}. Nonetheless, their effectiveness on downstream applications and their alignment with user intentions are greatly enhanced by fine-tuning on collections of instructions and responses written by humans \citep{mishra-etal-2022-cross,sanh2022multitask,chung2022scaling,thoppilan2022lamda}. This process, known as `instruction-tuning,' allows LLMs to better generalize to new instructions beyond those seen during tuning and broadly boosts their practicality \citep{chung2022scaling}. While instruction tuning has proved successful, collecting \textit{relative} judgments of response quality from humans is generally easier than gathering expert demonstrations. As a result, more recent studies have fine-tuned LLMs using datasets reflecting human preferences, which has led to improvements in translation \citep{kreutzer-etal-2018-reliability}, summarization \citep{stiennon2022learning,ziegler2020finetuning}, storytelling \citep{ziegler2020finetuning}, and following instructions \citep{ouyang2022training,ramamurthy2023is}. These approaches involve first training a neural reward model to match the set of human preferences under a preference framework such as the Bradley-Terry model \citep{bradley1952rankanalysis}, and then further tuning the language model to optimize this reward signal through reinforcement learning, typically with algorithms like REINFORCE \citep{williams1992reinforce}, proximal policy optimization (PPO; \cite{schulman2017proximal}), or similar variants \citep{ramamurthy2023is}. A related area of research uses LLMs, already fine-tuned for following instructions with human feedback, to create additional artificial preference datasets for qualities like safety or harmlessness \citep{bai2022constitutional}, relying only on light human oversight in the form of textual rubrics for guiding LLM annotations. Collectively, these strategies bring together two previously distinct areas: training language models using reinforcement learning to achieve various aims~\citep{Ranzato2015SequenceLT,paulus2018a,wu2018learning}, and general techniques for learning from human preferences \citep{christiano2017deep,kupcsik2018learning}. Although using relative human preference data is attractive, the process of fine-tuning large language models with reinforcement learning remains a significant practical hurdle; this paper introduces a theoretically grounded method for optimizing relative preferences that avoids reliance on RL.

Beyond language-focused applications, learning policies from preferences has been explored in both bandit and reinforcement learning frameworks, with a variety of methods introduced. In the case of contextual bandits, using preference or ranking feedback over actions instead of explicit rewards leads to the formulation known as contextual dueling bandits (CDB; \cite{yue2012karmed,dudik2015contextual}). When absolute reward information is unavailable, theoretical work on CDBs replaces the conventional optimal policy concept with the idea of a \textit{von Neumann winner}—a policy that is expected to outperform \textit{any} alternative policy at least 50\% of the time \citep{dudik2015contextual}. Notably, in the CDB paradigm, feedback in the form of preferences is provided interactively, whereas preference-based policy learning from human feedback typically relies on a static, offline dataset of action pairs annotated with preferences \citep{yan2022human}. In a related vein, \textit{preference-based RL} (PbRL) employs binary preferences derived from an \textit{unknown} underlying 'scoring' function in place of explicit rewards \citep{BusaFekete2014,ruiz2023dueling}. There are several algorithms for PbRL, some of which can utilize off-policy preference data, but they generally proceed by first inferring the hidden scoring (reward) function and then optimizing with respect to it \citep{jain2013learning,BusaFekete2014,christiano2017deep,sadigh2017active,kupcsik2018learning}. In contrast, our approach introduces a unified policy learning procedure that directly adjusts the policy to align with observed preferences.

\section{Preliminaries}\label{section:prelims}

We summarize the RLHF workflow described by \citeauthor{ziegler2020finetuning}, and subsequently in \citep{stiennon2022learning, bai2022training, ouyang2022training}. This process generally consists of three stages: (1) supervised fine-tuning (SFT), (2) collecting preferences and training a reward model, and (3) reinforcement learning optimization.

\textbf{SFT}: RLHF usually starts by applying supervised fine-tuning to a pre-trained LM using curated data relevant to the target task(s) (such as dialogue or summarization), resulting in a model denoted as $\pisft$.

\textbf{Reward Modelling Stage}: During this stage, the SFT model receives prompts $x$ and generates answer pairs $(y_1, y_2) \sim \pisft(y \mid x)$. These response pairs are evaluated by human annotators, who indicate a preference for one of the responses. This is denoted as $y_w \succ y_l \mid x$, where $y_w$ stands for the preferred answer and $y_l$ for the less preferred one, chosen from $(y_1, y_2)$. It is assumed that these preferences arise from an underlying but unknown reward function $r^*(y, x)$. Several preference modeling techniques exist; a widely used approach is the Bradley-Terry (BT) model \cite{bradley1952rankanalysis}. However, other general ranking models like Plackett-Luce \citep{plackett1975analysis, luce2012individual} can be used if multiple ranked responses are available. According to the BT model, the probability of a human preferring $y_1$ over $y_2$ given $x$ is:

\begin{equation}\label{eq:bradley-terry}
    p^*(y_1 \succ y_2 \mid x) = \frac{\exp\left(r^*(x, y_1)\right)}{\exp\left(r^*(x, y_1)\right) + \exp\left(r^*(x, y_2)\right)}.
\end{equation}

Given a fixed dataset of human comparisons $\mathcal{D} = \left\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\right\}_{i=1}^N$ sampled from $p^*$, we define a parameterized reward model $r_{\phi}(x, y)$ and optimize its parameters via maximum likelihood estimation. When treated as a binary classification task, the loss function is the negative log-likelihood:

\begin{equation}\label{eq:reward_model}
    \mathcal{L}_R(r_{\phi}, \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[\log \sigma(r_{\phi}(x, y_w) - r_{\phi}(x, y_l))\right]
\end{equation}

where $\sigma$ denotes the logistic sigmoid function. In large language models, the reward network $r_{\phi}(x, y)$ is commonly initialized using the SFT model $\pisft(y \mid x)$, appending a linear head to the final transformer layer to output a scalar reward estimate \cite{ziegler2020finetuning}. To stabilize the reward function, it is customary to normalize the output so that $\mathbb{E}_{x, y \sim \mathcal{D}}\left[r_\phi(x, y)\right] = 0$ for all prompts $x$.

\textbf{RL Fine-Tuning Phase}: In the reinforcement learning phase, the reward function that has been learned is leveraged to guide the language model’s outputs. Building on prior work~\citep{jaques2017sequence, jaques2020human}, the problem is cast as the following optimization:
\begin{equation}\label{eq:RL}
\max_{\pi_{\theta}}  \mathbb{E}_{x\sim \mathcal{D}, y\sim \pi_{\theta}(y \mid x)}\left[r_{\phi}(x, y)\right] - \beta\mathbb{D}_{\textrm{KL}}\left[\pi_{\theta}(y\mid x)\mid \mid \piref(y\mid x)\right],
\end{equation}
where $\beta$ regulates how much the model policy can diverge from the reference policy $\piref$, which refers to the original SFT model $\pisft$. 
Typically, $\pi_\theta$ is initialized with $\pisft$. The inclusion of the KL-divergence term is crucial to restrict the policy from straying too far from data regions where the reward model is trustworthy, and to preserve diversity in the generated text, thus avoiding collapse to just a few high-reward responses. Since language generation operates over discrete outputs, this objective cannot be differentiated directly and is customarily solved via reinforcement learning. The prevailing methodology \citep{ziegler2020finetuning, stiennon2022learning, bai2022training, ouyang2022training} defines the reward as ${r(x, y) = r_{\phi}(x, y) -\beta (\log \pi_{\theta}(y\mid x) - \log \piref(y\mid x))}$ and applies PPO \cite{schulman2017proximal} for optimization.

\section{Direct Preference Optimization}\label{sec:DPO}

Driven by the difficulties in using reinforcement learning methods on large-scale tasks like language model fine-tuning, our objective is to develop a straightforward method for optimizing policies directly from preference data. In contrast to standard RLHF techniques that first learn a reward function and then use RL to optimize the policy, our method employs a specific parameterization of the reward model that allows us to obtain the optimal policy in closed form, eliminating the need for an RL loop. 

As we will detail below, our main idea is to utilize an analytical mapping between reward functions and their corresponding optimal policies, which lets us convert a loss defined over rewards into a loss defined over policies. This change-of-variables technique eliminates the necessity of training a separate, explicit reward model, yet still optimizes with respect to established models of human preference, like the Bradley-Terry model. Effectively, the policy network jointly serves as both the language model and the (implicit) reward function.

\textbf{Formulating the DPO objective.} We begin with the general RL objective as used in previous works, Eq.~\ref{eq:RL}, for an arbitrary reward function $r$. Consistent with the literature~\citep{peters2007reinforcement, peng2019advantage, korbak2022reinforcement, go2023aligning}, one can readily verify that the KL-regularized reward maximization in Eq.~\ref{eq:RL} yields an optimal policy of the following form:
\begin{equation}\label{eq:op_policy}
    \pi_r(y\mid x) = \frac{1}{Z(x)}\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right),
\end{equation}
where the normalization constant $Z(x) =\sum_{y}\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)$ is known as the partition function. For a detailed derivation, see Appendix \ref{app:derivation1}. Even if the reward $r^*$ is replaced by its MLE approximation $r_{\phi}$, calculating the partition function $Z(x)$ remains computationally burdensome~\citep{korbak2022reinforcement, go2023aligning}, limiting the practicality of this form. Nevertheless, we can reformulate Eq.~\ref{eq:op_policy} to write the reward in terms of the optimal policy $\pi_r$, the reference policy $\piref$, and the partition function $Z(\cdot)$. Concretely, by taking the logarithm of Eq.~\ref{eq:op_policy} and rearranging, we have:
\begin{equation}\label{eq:main_eq}
    r(x,y) = \beta \log \frac{\pi_r(y\mid x)}{\piref(y\mid x)} + \beta \log Z(x).
\end{equation}
This transformation can also be applied for the true reward $r^*$ and its associated optimal policy $\pi^*$. Importantly, the Bradley-Terry model is determined only by the reward difference between two candidates: ${p^*(y_1 \succ y_2 \mid x) = \sigma(r^*(x, y_1) - r^*(x, y_2))}$. By substituting the reward expression from Eq.~\ref{eq:main_eq} into the preference model Eq.~\ref{eq:bradley-terry}, the partition function $Z(x)$ cancels out, enabling us to represent the probability of human preference exclusively through the optimal policy $\pi^*$ and reference $\piref$. Therefore, under the Bradley-Terry model, the optimal RLHF policy $\pi^*$ fulfills:
\begin{equation}\label{eq:objective}
    p^*(y_1\succ y_2 \mid x)=\frac{1}{1 + \exp\left(\beta \log \frac{\pi^*(y_2\mid x)}{\piref(y_2\mid x)} - \beta \log \frac{\pi^*(y_1\mid x)}{\piref(y_1\mid x)}\right)}
\end{equation}
A full derivation can be found in Appendix~\ref{app:derivation2}. Although Eq.~\ref{eq:objective} is based on the Bradley-Terry formulation, analogous expressions can be obtained for more general Plackett-Luce models~\citep{plackett1975analysis, luce2012individual}, which are elaborated in Appendix~\ref{app:plackett_luce_models}.

With the human preference probabilities now expressed via the optimal policy instead of the reward function, we can define a maximum likelihood objective for a parameterized policy $\pi_\theta$. Similar to the reward modeling formulation (see Eq.~\ref{eq:reward_model}), our objective for the policy becomes:
\begin{equation}\label{eq:optimum_model}
    \mathcal{L}_\text{DPO}(\pi_{\theta}; \piref) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\left[\log \sigma \left(\beta \log \frac{\pi_{\theta}(y_w\mid x)}{\piref(y_w\mid x)} - \beta \log \frac{\pi_{\theta}(y_l\mid x)}{\piref(y_l\mid x)}\right)\right].
\end{equation}
Through this approach, we implicitly learn a reward by optimizing a different parameterization, where the optimal solution is given by $\pi_\theta$. Furthermore, since this process is equivalent to fitting a reparameterized Bradley-Terry model, it inherits beneficial theoretical guarantees, such as consistency under appropriate conditions on the preference data distribution \cite{bong2022generalized}. We further elaborate on the theoretical aspects of DPO and its comparison to related methods in Section~\ref{sec:theory}.

\textbf{What effect does the DPO update have?} To gain a mechanistic perspective on DPO, it is instructive to examine the gradient of its loss function $\mathcal{L}_\text{DPO}$. The gradient with respect to the model parameters $\theta$ takes the following form:
\begin{multline*}\label{eq:gradient}
    \nabla_\theta \mathcal{L}_\text{DPO}(\pi_\theta;\piref) = \\ -\beta\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \bigg[\underbrace{\sigma(\hat{r}_\theta(x, y_l) - \hat{r}_\theta (x, y_w))}_\text{greater emphasis when reward ordering is incorrect}\bigg[\underbrace{\nabla_\theta\log \pi(y_w \mid x)}_\text{boost probability of $y_w$} - \underbrace{\nabla_\theta\log\pi(y_l \mid x)}_\text{reduce probability of $y_l$}\bigg]\bigg],
\end{multline*}
where the implicit reward signal is given by $\hat{r}_\theta(x, y) = \beta \log \frac{\pi_\theta(y \mid x)}{\piref(y \mid x)}$, derived from both the current model $\pi_\theta$ and the reference model $\piref$ (see Section~\ref{sec:theory} for more details). Conceptually, the gradient of $\mathcal{L}_\text{DPO}$ acts to encourage higher probabilities for the preferred responses $y_w$ and suppress the probabilities of less favored responses $y_l$. Crucially, the update for each sample is weighted according to how much the implicit reward function $\hat{r}_\theta$ erroneously scores the dispreferred output above the preferred one, modulated by $\beta$—that is, by the degree of misordering in the reward as well as the strength of the KL penalty. Our experimental findings underscore the necessity of this weighting: a simplistic approach omitting the weighting factor can lead the language model to collapse, as shown in Appendix Table~\ref{tab:unlikelihood_generations}.

\textbf{Overview of DPO.}  
The typical DPO workflow proceeds as follows: 1) For each prompt $x$, generate completions $y_1, y_2 \sim \piref(\cdot \mid x)$, then use human preference annotations to create the offline preference dataset $\mathcal{D} = \{x^{(i)}, y_w^{(i)}, y_l^{(i)}\}_{i=1}^N$; 2) train the language model $\pi_\theta$ by minimizing $\mathcal{L}_\text{DPO}$ with respect to the chosen $\piref$, the constructed $\mathcal{D}$, and the target $\beta$.  
In real-world scenarios, it is preferable to utilize existing public preference datasets rather than producing new samples and collecting fresh human labels. Given that these datasets are collected using $\pisft$, we set $\piref = \pisft$ when $\pisft$ is accessible. If not, we set $\piref$ to maximize the likelihood of preferred responses ${(x, y_w)}$, i.e., ${\piref = \argmax_{\pi}\mathbb{E}_{x, y_w \sim \mathcal{D}}\left[\log \pi(y_w \mid x)\right]}$. This approach reduces the mismatch between the actual, inaccessible reference distribution and the $\piref$ adopted by DPO. More implementation details and information on hyperparameters are available in Appendix~\ref{app:implementation}.

\section{Theoretical Examination of DPO}
Here, we present a deeper analysis of the DPO approach, offer formal justification, and highlight how the strengths of DPO address challenges faced by actor-critic methods like those used in RLHF (for example, PPO~\cite{schulman2017proximal}).

\label{sec:theory}

\subsection{Your Language Model Is Secretly a Reward Model}  
DPO manages to avoid both constructing an explicit reward and running RL by optimizing a single maximum likelihood objective. Observe that the optimization target in Eq. \ref{eq:main_eq} corresponds to a Bradley-Terry model, where rewards are parameterized as $r^*(x, y) = \beta \log\frac{\pi^*_\theta(y \mid x)}{\piref(y \mid x)}$. Here, we train our parameterized model $\pi_{\theta}$, which is mathematically equivalent to optimizing the reward model in Eq. \ref{eq:reward_model} after a variable substitution. In this section, we develop the theoretical foundations for this reparameterization, demonstrate that it does not restrict the set of possible reward models that can be learned, and prove that it enables exact retrieval of the optimal policy. We start by introducing an equivalence relation over reward functions.

\begin{definition}
Two reward functions $r(x, y)$ and $r'(x, y)$ are called equivalent if and only if there exists a function $f$ such that $r(x, y) - r'(x, y) = f(x)$.
\end{definition}
It is straightforward to verify that this defines an equivalence relation, thereby dividing the set of reward functions into equivalence classes. This allows us to present the following two lemmas:

\begin{lemma}\label{lemma:same_prefrence} Within the Plackett-Luce model, including the Bradley-Terry approach, any two reward functions that belong to the same class yield identical preference distributions.
\end{lemma}

\begin{lemma}\label{lemma:same_policy}
    Any two reward functions belonging to the same equivalence class generate identical optimal policies under the constrained RL framework.
\end{lemma}
The proofs are elementary and are provided in Appendix \ref{app:lemma1}. The first lemma highlights a well-established ambiguity within the Plackett-Luce model family \cite{plackett1975analysis}. Because of this ambiguity, it is typical to enforce additional identifiability conditions in order to obtain meaningful MLE solutions for Eq. \ref{eq:reward_model} \cite{bong2022generalized}. The second lemma establishes that every reward function within the same class produces the same optimal policy. Thus, for our main objective, it suffices to recover any representative reward function from the optimal class. We establish the following Theorem in Appendix~\ref{app:thm1}:
\begin{theorem}\label{thm:main}
    Assuming mild conditions, all equivalence classes of rewards consistent with the Plackett-Luce model (including the Bradley-Terry special case) can be written using the reparameterization ${r(x, y) = \beta \log \frac{\pi(y\mid x)}{\piref(y\mid x)}}$ for some model $\pi(y\mid x)$ and a fixed reference model $\piref(y \mid x)$.
\end{theorem}
\begin{sproof}
    Take any reward function $r(x, y)$, which determines an optimal policy $\pi_r(y \mid x)$ as defined by Eq. \ref{eq:op_policy}. We aim to show that any reward from the equivalence class of $r$ admits the stated reparameterized form. We introduce the following transformation $f$:
\begin{equation}
    f(r; \piref, \beta)(x, y) = r(x, y) - \beta\log\sum_{y}\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)
\end{equation}
This function $f$ effectively rescales the reward by subtracting the log-partition function (dependent only on $x$) associated with $\pi_r$. Since this normalization depends solely on $x$, $f(r; \piref, \beta)(x, y)$ remains within the equivalence class of $r(x, y)$. Now, if we substitute $r$ with the right-hand side of Eq.~\ref{eq:main_eq} (which is valid for any reward), it follows that $f(r; \piref, \beta)(x, y) = \beta \log \frac{\pi_r(y\mid x)}{\piref(y\mid x)}$. In other words, $f$ generates a reward of the desired form within the same equivalence class, so our proposed reparameterization maintains full generality.
\end{sproof}
Alternatively, Theorem~\ref{thm:main} precisely identifies which member of the equivalence class is chosen by the DPO reparameterization: namely, the reward function that satisfies
\begin{equation}\label{eq:lag_p}
     \sum_{y}\underbrace{\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)}_{=\pi(y\mid x)\text{, per Thm.~\ref{thm:main} reparameterization}} = 1,
\end{equation}
meaning that $\pi(y\mid x)$ is a well-defined probability distribution (i.e., non-negative and normalized).
By referencing Eq.~\ref{eq:op_policy}, we can see that Eq.~\ref{eq:lag_p} is the normalization factor (partition function) of the optimal policy under $r(x, y)$.
The central idea behind the DPO algorithm is that, by enforcing specific constraints on the under-determined Plackett-Luce (or Bradley-Terry) class of preference models, we can both retain the full space of reward models and, at the same time, make the optimal policy from Eq. \ref{eq:op_policy} analytically computable for any prompt $x$.

\subsection{Instability in Actor-Critic Methods}

Our framework can also help identify sources of instability in standard actor-critic methods commonly used in RLHF, such as PPO. Following the RLHF process, we concentrate on the RL fine-tuning phase described in Section \ref{section:prelims}. We relate this to the control-as-inference approach \cite{levine2018reinforcement} for the constrained RL formulation introduced in \ref{eq:RL}. Considering a parameterized policy $\pi_{\theta}(y\mid x)$, we minimize the KL divergence $\mathbb{D}_{\text{KL}}[\pi_{\theta}(y|x) \mid\mid \pi^*(y\mid x)]$, where $\pi^*$ is the optimal policy defined in Eq. \ref{eq:optimum_model} and is determined by the reward function $r_{\phi}(y, x)$. Through some manipulations, we arrive at the following optimization objective:

\begin{equation}\label{eq:AC}
    \max_{\pi_{\theta}}\mathbb{E}_{\pi_{\theta}(y\mid x)}\bigg[\underbrace{r_{\phi}(x, y) -\beta\log\sum_{y}\piref(y\mid x)\exp\left(\frac{1}{\beta}r_{\phi}(x, y)\right)}_{f(r_{\phi}, \piref, \beta)} - \underbrace{\beta\log\frac{\pi_{\theta}(y\mid x)}{\piref(y\mid x)}}_{\text{KL}}\bigg]
\end{equation}

This is the same loss function that prior studies have optimized \citep{ziegler2020finetuning, stiennon2022learning, bai2022training, ouyang2022training}, employing the DPO-equivalent reward for the reward family $r_{\phi}$. In this context, the normalization factor in $f(r_{\phi}, \piref, \beta)$ can be viewed as the soft value function for the reference policy $\piref$. Although this component does not change the optimal policy, omitting it can result in a policy gradient with substantial variance, leading to unstable training. To address this, a learned value function can approximate the normalization term, though this approach also presents optimization challenges. Alternatively, previous works have used a human completion as a baseline for reward normalization, effectively providing a one-sample Monte Carlo estimate of the normalization constant. In contrast, the DPO reparameterization provides a reward function that eliminates the need for any baseline.

\section{Experiments}
Here, we empirically assess how effectively DPO trains policies using preference data. We begin with a controlled text-generation scenario to investigate how well DPO balances reward maximization against keeping the KL-divergence from the reference policy low, in comparison with popular preference-based methods like PPO. Subsequently, we analyze DPO's performance on larger-scale models and more challenging RLHF benchmarks, such as dialogue and summarization. Our results indicate that DPO typically matches or outperforms robust baselines like RLHF using PPO, as well as approaches that select the best of $N$ generated outputs according to a learned reward model—all with minimal hyperparameter adjustment. Prior to discussing these findings, we outline the experimental methodology; further specifics can be found in Appendix~\ref{app:exp_details}.

\textbf{Tasks.} Our study investigates three distinct open-ended text generation tasks. Across all experiments, learning algorithms infer a policy from a preference dataset $\mathcal{D} = \bigl\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\bigr\}_{i=1}^N$. In the case of \textbf{controlled sentiment generation}, $x$ is an initial segment from an IMDb movie review \cite{maas-EtAl:2011:ACL-HLT2011}, and the policy aims to generate a continuation $y$ exhibiting positive sentiment. To enable systematic evaluation, we \textit{generate} preference pairs among completions using a pre-trained sentiment classifier, ensuring $p(\text{positive}\mid x, y_w) > p(\text{positive}\mid x, y_l)$. For SFT, GPT-2-large is further trained until convergence using the IMDB training set reviews (see App~\ref{app:sentiment_details} for more). For the \textbf{summarization} task, $x$ corresponds to a Reddit forum post, and the policy is required to produce a summary $y$ capturing the core ideas. Consistent with past work, the Reddit TL;DR summarization dataset \citep{volske-etal-2017-tl} is employed, paired with human preference data collected by \citeauthor{stiennon2022learning}. The SFT model is trained on summaries written by humans\footnote{\url{https://huggingface.co/CarperAI/openai_summarize_tldr_sft}} and leverages the TRLX RLHF framework \citep{leandro_von_werra_2023_7790115}. The human preference labels originate from \citeauthor{stiennon2022learning}, based on outputs sampled from a similar, separately-trained SFT model. Lastly, for \textbf{single-turn dialogue}, $x$ represents a user prompt, ranging from scientific queries to personal advice. Here, the policy must generate a helpful, engaging reply $y$. We utilize the Anthropic Helpful and Harmless dialogue dataset \citep{bai2022training}, comprising 170,000 dialogues between a human and an AI assistant. Each dialogue ends with two model-generated responses and an annotation specifying the human-preferred choice. In this domain, no pre-trained SFT model is available, so we fine-tune a generic language model solely on preferred outputs to obtain the SFT model.

\begin{figure}
    \centering
    \includegraphics[width=0.50\textwidth]{figures/results/frontier.pdf}
    \includegraphics[width=0.49\textwidth]{figures/results/tldr_winrate_vs_temp.pdf}
    \caption{\textbf{Left.} The trade-off frontier between expected reward and KL divergence from the reference policy. DPO consistently achieves the greatest expected reward across all KL levels, highlighting its optimization effectiveness. \textbf{Right.} TL;DR summarization win rates compared to human-created summaries, with GPT-4 acting as the judge. DPO outperforms PPO at its best for summarization tasks and is also less sensitive to variations in the sampling temperature.}
    \vspace{-2mm}
    \label{fig:frontier-tldr-main}
\end{figure}

\textbf{Evaluation.} We assess our methods using two distinct evaluation strategies. To measure how well each algorithm optimizes the constrained reward maximization objective in a controlled sentiment generation scenario, we plot the frontier of reward versus KL-divergence from the reference policy for each algorithm; this is feasible because we have access to the exact reward function (specifically, a sentiment classifier). Conversely, in practical situations where the true reward function is unknown, we compare algorithms based on their \textit{win rate} against a baseline policy, employing GPT-4 as a stand-in for human judgment to evaluate summary quality and helpfulness of responses in summarization and single-turn dialogue tasks, respectively. For summarization, the baseline is the reference summaries provided in the test set; for dialogue, the baseline consists of the preferred responses from the test dataset. Although prior research indicates that LMs can outperform standard metrics as automated evaluators \citep{Chen2023ExploringTU}, we also carry out a human evaluation to validate our choice of GPT-4 for assessment, as described in Sec.~\ref{sec:human-judgments}. Our results demonstrate that GPT-4 assessments closely match human evaluations, with agreement rates between humans and GPT-4 often equaling or exceeding those observed between different human annotators.

\textbf{Methods.} Besides DPO, we assess a variety of established methods for training language models to align with human preferences. To start, we test zero-shot prompting using \textbf{GPT-J} \citep{gpt-j} on the summarization task and 2-shot prompting with \textbf{Pythia-2.8B} \citep{biderman2023pythia} for dialogue. We also evaluate the \textbf{SFT} model, as well as \textbf{Preferred-FT}, which is a model fine-tuned via supervised learning on the selected completion $y_w$, sourced either from the SFT model (for controlled sentiment and summarization) or from a standard language model (for single-turn dialogue). Another related approach is \textbf{Unlikelihood}~\citep{welleck2019neural}, which updates the model to increase the likelihood of $y_w$ and simultaneously \textit{decrease} the likelihood of $y_l$, employing an optional scaling factor $\alpha\in[0,1]$ on the unlikelihood component. We further include \textbf{PPO} \citep{schulman2017proximal}, trained using a reward model derived from preference data, and \textbf{PPO-GT}, an oracle variant that leverages the true reward function available in the controlled sentiment scenario. For our sentiment studies, we implement two versions of PPO-GT: an off-the-shelf release \cite{leandro_von_werra_2023_7790115} and an adapted version that normalizes rewards and fine-tunes hyperparameters for enhanced results (these adjustments are also applied to standard PPO with learned rewards). Lastly, we analyze the \textbf{Best of $N$} baseline, which generates $N$ candidate outputs from the SFT model (or Preferred-FT in dialogue) and selects the top response as ranked by a reward model trained on preference data. While this strong method separates reward model performance from PPO optimization, it is computationally expensive for even moderate $N$, since $N$ completions must be sampled for each query at inference.

\subsection{How well can DPO optimize the RLHF objective?}

\begin{figure}
    \centering
    \includegraphics[width=0.50\textwidth]{figures/results/dialogue_winrate_vs_temp.pdf}
    \includegraphics[width=0.49\textwidth]{figures/results/dialogue_winrate_vs_steps.pdf}
    \caption{\textbf{Left.} GPT-4 assessed win rates for Anthropic-HH one-step dialogue; DPO is the sole approach that surpasses chosen summaries in the Anthropic-HH test dataset. \textbf{Right.} Win rates shown across various sampling temperatures throughout training. DPO consistently outperforms dataset labels during training for all tested temperatures.}
    \vspace{-2mm}
    \label{fig:dialogue-main}
\end{figure}

The KL-constrained reward maximization objective, commonly employed in standard RLHF algorithms, seeks to optimize reward while simultaneously limiting the policy's divergence from the reference policy. Consequently, when comparing different methods, it is essential to evaluate both the obtained reward and the associated KL divergence; attaining marginally higher rewards at the expense of a significantly greater KL is not always preferable. Figure~\ref{fig:frontier-tldr-main} illustrates the reward-versus-KL tradeoff for several algorithms in the sentiment classification scenario. For each algorithm, we conduct several training sessions, each with a distinct setting for policy conservativeness (target KL $\in\{3,6,9,12\}$ for PPO, $\beta \in \{0.05,0.1,1,5\}$, $\alpha\in\{0.05,0.1,0.5,1\}$ for unlikelihood, and varying random seeds for preferred-FT), resulting in a total of 22 experiments. After every 100 training updates, up to convergence, we assess each policy on a collection of test prompts, calculating both the mean reward according to the true reward metric and the mean sequence-level KL\footnote{Meaning the total of the KL-divergences computed at each time step.} relative to the reference policy $\text{KL}\left(\pi\mid \mid \piref\right)$. Our results show that DPO forms a significantly more efficient reward-KL frontier than other approaches, attaining the highest reward with relatively low KL. This finding stands out for several reasons. First, despite DPO and PPO targeting the same objective, DPO demonstrates notably higher efficiency, consistently outperforming PPO in the reward/KL space. Second, DPO achieves a superior tradeoff compared to PPO, \emph{even when PPO is provided with access to the true rewards} (PPO-GT).

\subsection{Is DPO capable of scaling to real-world preference datasets?}
\label{sec:dpo-real-datasets}
We proceed to assess the fine-tuning effectiveness of DPO on tasks involving summarization and single-turn dialogue. In the context of summarization, established automated metrics like ROUGE often show weak alignment with human judgment~\citep{stiennon2022learning}, and earlier studies report that fine-tuning language models with PPO based on human preference data leads to more compelling summaries. To evaluate different approaches, we generate completions using the test portion of the TL;DR summarization dataset, calculating the mean win rate against reference completions in the test set. For each method, completions are produced with sampling temperatures ranging from 0.0 to 1.0; the corresponding win rates appear in Figure~\ref{fig:frontier-tldr-main} (right). DPO, PPO, and Preferred-FT are all trained starting from the same GPT-J SFT model\footnote{\url{https://huggingface.co/CarperAI/openai_summarize_tldr_sft}}. Our results indicate that at temperature 0.0, DPO reaches an approximate win rate of 61\%, outperforming PPO’s peak win rate of around 57\% at the same temperature. Furthermore, DPO attains a greater maximum win rate than the best-of-$N$ baseline. It should be emphasized that DPO’s $\beta$ hyperparameter was not carefully optimized, suggesting these results may undervalue DPO’s true capabilities. Additionally, DPO demonstrates significantly greater robustness to changes in sampling temperature compared to PPO, whose performance may drop to that of the underlying GPT-J model at elevated temperatures. Preferred-FT, on the other hand, yields only marginal improvements relative to the SFT baseline. We further perform a direct comparison of DPO and PPO in human studies in Section~\ref{sec:human-judgments}, revealing that DPO completions sampled at temperature 0.25 were favored 58\% of the time over those from PPO at temperature 0.

For single-turn dialogue, we assess the various approaches using a subset of the test split from the Anthropic HH dataset \citep{bai2022training}, specifically focusing on examples containing one round of human-assistant interaction. For GPT-4 evaluation, we calculate win rates for each method by comparing them to the preferred completions in the test set. Since there is no standard SFT model for this task, we begin with a pre-trained Pythia-2.8B and fine-tune a reference model with Preferred-FT using the selected completions to align the outputs with the data distribution, before applying DPO training. We also benchmark against the strongest out of 128 Preferred-FT completions (noting that the Best of $N$ approach plateaus at $N=128$ for this setting; refer to Appendix Figure~\ref{fig:best-of-n}), as well as a 2-shot prompted variant of the base Pythia-2.8B model. Across optimal sampling temperatures, DPO matches or surpasses the performance of other approaches. Additionally, we test an RLHF model trained with PPO on the Anthropic HH dataset\footnote{\url{https://huggingface.co/reciprocate/ppo_hh_pythia-6B}} from a reputable implementation\footnote{\url{https://github.com/CarperAI/trlx/tree/main/examples/hh}}, but are unable to identify any prompt or temperature setting that outperforms the base Pythia-2.8B. Given our findings from TL;DR and that both methods maximize the same reward, we treat Best of 128 as an approximate indicator of PPO-level results. Overall, DPO stands out as the only efficient method that consistently outperforms the preferred completions on the Anthropic HH dataset, and its results are comparable to or better than the resource-intensive Best of 128 baseline. Lastly, as depicted in Figure~\ref{fig:dialogue-main}, DPO quickly reaches its top performance.

\subsection{Generalization to a new input distribution}

\begin{wraptable}{r}{0.375\textwidth}
    \small
    \vspace{-10mm}
    \begin{tabular}{ccc}
        \toprule
        & \multicolumn{2}{c}{\textbf{Win rate compared to ground truth}} \\
        \cmidrule(lr){2-3}
        \textbf{Alg.} & Temp $0$ & Temp $0.25$ \\
        \midrule
        DPO & 0.36 & 0.31 \\
        PPO & 0.26 & 0.23 \\
        \bottomrule
    \end{tabular}
    \caption{Win rates of GPT-4 against ground truth summaries on out-of-distribution CNN/DailyMail test articles.}
    \vspace{-3mm}
    \label{tab:ood}
\end{wraptable}

To further assess how PPO and DPO perform under distribution shifts, we test the policies obtained from our Reddit TL;DR summarization study on a new domain: news articles in the test set of the CNN/DailyMail dataset \citep{nallapati-etal-2016-abstractive}. We use the optimal sampling temperatures identified from TL;DR (0 and 0.25). Table~\ref{tab:ood} shows the resulting comparisons. We calculate the GPT-4 win rate relative to the reference summaries from the datasets, utilizing the same GPT-4 (C) prompt as in the Reddit TL;DR evaluation, but substituting ``forum post'' with ``news article''. On this distribution, DPO consistently surpasses PPO by a substantial margin. These findings provide preliminary evidence that DPO policies can generalize at least as effectively as PPO policies, despite not relying on the extra unlabeled Reddit TL;DR prompts that PPO leverages.

\subsection{Corroborating GPT-4 assessments with human evaluations}
\label{sec:human-judgments}
We carry out a human subject study to assess the trustworthiness of GPT-4's assessments, leveraging outcomes from the TL;DR summarization task and two distinct GPT-4 prompts. The \textbf{GPT-4 (S)} (simple) prompt asks which summary better encapsulates the key information from the post. The \textbf{GPT-4 (C)} (concise) prompt, on the other hand, asks which summary is both more informative and more succinct; this prompt is included because GPT-4, when given the \textbf{GPT-4 (S)} prompt, tends to favor longer and more redundant summaries compared to human preferences. Full prompt wording is provided in Appendix~\ref{app:prompts}. We execute three pairwise comparisons: the best-performing (DPO, temperature 0.25), the weakest (PPO, temperature 1.0), and an intermediate (SFT, temperature 0.25) method, aiming to capture a range of summary qualities. All are contrasted with PPO using greedy decoding (temperature 0). 

\begin{wraptable}{r}{0.47\textwidth}
    \centering
    \small
    \vspace{-1.5mm}
    \begin{tabular}{lccc}
    \toprule
        & \textbf{DPO} & \textbf{SFT} & \textbf{PPO-1} \\
        \cmidrule(lr){2-4}
        N respondents & 272 & 122 & 199 \\
        \midrule
        GPT-4 (S) win \% & 47 & 27 & 13 \\
        GPT-4 (C) win \% & 54 & 32 & 12 \\
        Human win \% & 58 & 43 & 17 \\
        \midrule
        GPT-4 (S)-H agree & 70 & 77 & 86 \\
        GPT-4 (C)-H agree & 67 & 79 & 85 \\
        H-H agree & 65 & - & 87 \\
        \bottomrule
    \end{tabular}
    \vspace{-1mm}
    \caption{Comparisons of human and GPT-4 win percentages, along with agreement metrics for TL;DR summarization tasks. \textbf{Agreement between humans and GPT-4 is comparable to inter-human agreement.} Each match-up pits a summary generated by the given technique against a PPO summary with temperature 0.}
    \vspace{-5mm}
    \label{tab:human_results}
\end{wraptable}

The experiment reveals that across both prompts, GPT-4's choices align with human judgments at rates similar to those observed between human raters, implying that GPT-4 can serve as an effective stand-in for human evaluation (noting that, due to a small pool of human raters, multiple ratings per sample are collected only for DPO and PPO-1). Notably, the \textbf{GPT-4 (C)} prompt tends to yield win rates more closely matching those of human annotators; thus, this prompt is selected for the primary analyses in Section~\ref{sec:dpo-real-datasets}. Further information about the human study design, including the online interface used and the roster of human participants, is available in Appendix~\ref{app:human-study}.

\section{Discussion}
Learning through preferences offers a robust and scalable approach for developing competent, aligned language models. In this work, we presented DPO, a straightforward method for preference-based language model training that does not rely on reinforcement learning. Instead of forcing the preference learning task into a conventional RL framework to utilize standard RL techniques, DPO establishes a direct correspondence between language model policies and reward functions. This correspondence allows the language model to be optimized to reflect human preferences using a basic cross-entropy loss—completely bypassing reinforcement learning and without compromising generality. DPO achieves results on par with or superior to current RLHF strategies, such as those employing PPO, all with minimal hyperparameter adjustment. Consequently, DPO substantially lowers the difficulty of training language models to adhere to human preferences.

\textbf{Limitations \& Future Work.} Our findings prompt several key directions for further investigation. One open question is how well the DPO policy generalizes to out-of-distribution data in comparison to approaches that rely on a defined reward function. Preliminary experiments indicate that DPO policies may generalize on par with those trained using PPO, but broader analysis is warranted. For instance, does leveraging self-labeling via the DPO policy also enable efficient use of unlabeled prompts? Another area worth exploring is how reward over-optimization appears within direct preference optimization, and whether the minor drop in performance observed in Figure~\ref{fig:dialogue-main}-right is a result of this phenomenon. Moreover, although our current experiments cover models up to 6B parameters, scaling DPO to much larger, state-of-the-art models remains an intriguing avenue for future exploration. In terms of evaluation, our analysis shows that GPT-4's win rates are sensitive to the choice of prompt; future studies could investigate optimal methods for eliciting reliable judgments from automated evaluators. Lastly, DPO holds potential for numerous applications beyond training language models from human feedback, such as applying it to generative modeling tasks in other domains.

\end{document}