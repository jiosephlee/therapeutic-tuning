\title{Direct Preference Optimization: Your Language Model is Secretly a Reward Model}

\begin{abstract}
Although large-scale unsupervised language models (LMs) acquire extensive world knowledge and some reasoning capabilities, their behavior is hard to precisely direct due to the inherently unsupervised manner of their training.
To gain more control over model outputs, current approaches gather human judgments comparing the quality of generations and then further train the base LM to reflect these preferences, commonly through reinforcement learning from human feedback (RLHF).
However, RLHF involves a multi-stage and frequently unstable process: first, learning a reward model to capture human preference signals, and then applying reinforcement learning to optimize the original LM for this proxy reward while constraining deviation from the pretrained model.
In this work, we present a novel reward model parameterization for RLHF that permits direct, closed-form extraction of the corresponding optimal policy, reducing the typical RLHF process to a straightforward classification objective.
Our approach, named \textit{Direct Preference Optimization} (DPO), offers stability, efficiency, and low computational requirements, eliminating the need for language model sampling or intensive hyperparameter searches during fine-tuning.
Empirical results indicate that DPO can align LMs with human preferences on par with, or better than, standard techniques. Specifically, DPO surpasses RLHF with PPO in sentiment control, and delivers comparable or higher quality on tasks such as summarization and single-turn dialogue, all while being much easier to train and implement.
\end{abstract}

\section{Introduction}
Extensive unsupervised language models (LMs) trained on massive text corpora demonstrate unexpected competencies~\citep{chowdhery2022palm, brown2020language, touvron2023llama,bubeck2023sparks}. Nonetheless, the data these models ingest is authored by humans with diverse intentions, priorities, and expertise. Not all of these are appropriate for models to emulate; for instance, although we want an AI programming assistant to \textit{recognize} frequent coding errors so it can correct them, we still prefer its code generation to reflect the (sometimes uncommon) examples of top-tier coding skill from the training set. Analogously, we may require our language model to \textit{understand} a widespread misconception—say, one believed by half the population—but we certainly would not want it to present this falsehood as truth in half of its responses! Therefore, it is vital to carefully select the model’s \emph{preferred outputs and conduct} from its broad \textit{knowledge base and skillset} in order to develop AI systems that are robust, reliable, and governable~\citep{ouyang2022training}. Although current approaches predominantly guide LMs to align with human values via reinforcement learning (RL), we will demonstrate that the RL-based objective employed by these methods can be directly and exactly optimized through a straightforward binary cross-entropy loss, substantially streamlining the preference alignment process.

\begin{figure}
    \centering
    \includegraphics[width=0.999\textwidth]{figures/diagrams/teaser.png}
    \caption{\textbf{DPO aligns with human preferences without the use of reinforcement learning.} Traditional approaches for tuning language models with human feedback involve training a reward model using a set of prompts and human-preferred response pairs, then applying RL to derive a policy that optimizes the estimated reward. In comparison, DPO optimizes the policy to satisfy these preferences directly, utilizing a straightforward classification-based objective that fits an \textit{implicit} reward function, for which the optimal policy can be explicitly solved.}
    \vspace{-2mm}
    \label{fig:teaser}
\end{figure}

Broadly speaking, current approaches teach language models to exhibit desired behaviors by leveraging carefully selected collections of human preferences that illustrate what people perceive as safe and useful responses. This process of learning from preferences follows an initial phase, where the model undergoes large-scale unsupervised pre-training on an extensive text corpus. Although the simplest form of preference learning involves supervised fine-tuning using human-annotated examples of high-quality answers, the most effective techniques center on reinforcement learning from human (or AI-generated) feedback (RLHF/RLAIF; \citep{christiano2017deep,bai2022constitutional}). RLHF strategies train a reward model based on human preference data, then employ reinforcement learning to adjust the language model’s policy so that it generates outputs with higher reward scores, while still maintaining similarity to the original model. Despite the fact that RLHF enables models to attain impressive performance in dialogue and code generation, its workflow is substantially more complicated than traditional supervised learning—requiring the training of multiple language models and repeated policy sampling during training—which leads to considerable computational overhead.

In this work, we demonstrate a method to optimize a language model directly according to human preferences, avoiding explicit reward modeling or reinforcement learning. We introduce Direct Preference Optimization (DPO), a technique that implicitly targets the same objective as traditional RLHF approaches (maximizing reward under a KL-divergence constraint), yet remains easy to implement and simple to train. Conceptually, DPO adjusts the log odds in favor of preferred responses over less preferred ones, but it introduces a dynamic, example-specific importance weighting to avert model collapse, an issue observed with basic probability ratio objectives. Like other approaches, DPO makes use of a theoretical preference model (such as the Bradley-Terry model; \cite{bradley1952rankanalysis}) that quantifies how effectively a reward function matches empirical preference judgments. The key difference is that, while conventional methods leverage the preference model to define a loss for training a reward model before training a policy on that reward, DPO employs a variable substitution to express the preference loss directly in terms of the policy itself. As a result, given a dataset of human preferences between model outputs, DPO is able to train a policy using a simple binary cross entropy loss, yielding an optimal policy for an implicit reward function tailored to the preference data.

The primary contribution of our work is Direct Preference Optimization (DPO), a straightforward algorithm for training language models from preference data without relying on reinforcement learning. Through our experiments, we demonstrate that DPO matches or surpasses the performance of established techniques—including PPO-based RLHF—for preference-based learning tasks like sentiment control, summarization, and dialogue, across models with sizes up to 6B parameters.

\section{Related Work}

Self-supervised language models of growing size demonstrate the ability to solve certain problems in a zero-shot setting \citep{radford2019language} or using just a handful of examples in their prompts \citep{gpt3,megatron,chowdhery2022palm}. Nevertheless, their effectiveness on downstream applications and their alignment with user intent are markedly enhanced by further training on datasets comprised of instructions paired with human-generated completions \citep{mishra-etal-2022-cross,sanh2022multitask,chung2022scaling,thoppilan2022lamda}. This process, known as `instruction-tuning,’ enables LLMs to generalize to novel instructions beyond those present in the instruction-tuning corpus, leading to improved practicality and flexibility \citep{chung2022scaling}. While instruction tuning has achieved notable progress, gathering \textit{relative} human assessments of response quality is typically more feasible than obtaining expert-created examples, prompting later research to refine LLMs with datasets based on human preference judgments. This has led to gains in translation \citep{kreutzer-etal-2018-reliability}, summarization \citep{stiennon2022learning,ziegler2020finetuning}, story generation \citep{ziegler2020finetuning}, and instruction-following capabilities \citep{ouyang2022training,ramamurthy2023is}. These strategies begin by training a neural reward model aligned with the collected preferences, often utilizing a preference structure such as the Bradley-Terry model \citep{bradley1952rankanalysis}, and then updating a language model to maximize this learned reward through reinforcement learning methods like REINFORCE \citep{williams1992reinforce}, proximal policy optimization (PPO; \cite{schulman2017proximal}), or related approaches \citep{ramamurthy2023is}. A related research direction employs instruction-following LLMs, further adjusted using human feedback, to autonomously produce additional synthetic preference data targeting specific traits, such as safety or harmlessness \citep{bai2022constitutional}. This is achieved with only minimal human oversight, typically in the form of a textual rubric for LLM outputs. These approaches blend two areas of research: one concerned with training language models using reinforcement learning across multiple objectives~\citep{Ranzato2015SequenceLT,paulus2018a,wu2018learning}, and another focused on general frameworks for learning from human preferences \citep{christiano2017deep,kupcsik2018learning}. Despite the benefits of utilizing relative human preferences, fine-tuning large language models via reinforcement learning continues to pose substantial practical hurdles; this study introduces a theoretically sound alternative for optimizing over relative preferences without relying on RL.

Beyond language applications, the task of learning policies from preferences has been explored in both bandit and reinforcement learning frameworks, with a variety of methods developed for these settings. When contextual bandit algorithms utilize preferences or rankings over actions instead of explicit rewards, this is referred to as the contextual dueling bandit (CDB; \cite{yue2012karmed,dudik2015contextual}). In scenarios lacking explicit rewards, the theory of CDBs uses the concept of a \textit{von Neumann winner}—a policy that, on average, beats \textit{every} alternative policy at least half of the time \citep{dudik2015contextual}—in place of the standard notion of optimality. Notably, the CDB framework assumes that preference information is acquired online, whereas policy learning from human preferences typically relies on a static, offline collection of action pairs annotated with preferences \citep{yan2022human}. Likewise, in \textit{preference-based RL} (PbRL), agents learn from binary comparisons drawn from an \textit{unknown} scoring function, replacing reward signals \citep{BusaFekete2014,ruiz2023dueling}. PbRL encompasses a range of algorithms—including those capable of leveraging previously collected preference data—but most approaches first estimate the underlying scoring (reward) function and then optimize policies against it \citep{jain2013learning,BusaFekete2014,christiano2017deep,sadigh2017active,kupcsik2018learning}. In contrast, we introduce a one-stage method that directly adjusts the policy to align with observed preferences.

\section{Preliminaries}\label{section:prelims}

We discuss the RLHF workflow as outlined by \citeauthor{ziegler2020finetuning}, and subsequently explored in \citep{stiennon2022learning, bai2022training, ouyang2022training}. This process generally involves three main stages: (1) supervised fine-tuning (SFT); (2) collecting preferences and training a reward model; and (3) optimizing using reinforcement learning.

\textbf{SFT}: RLHF usually starts by taking a pre-trained LM and applying supervised fine-tuning with carefully curated data relevant to the target application (such as dialogue or summarization), resulting in a model referred to as $\pisft$.

\textbf{Reward Modelling Phase}: In the second stage, the SFT model is given prompts $x$ and generates answer pairs $(y_1, y_2)\sim \pisft(y \mid x)$. Human annotators then review these responses and indicate their preference for one answer, denoted as $y_w\succ y_l \mid x$, where $y_w$ and $y_l$ are, respectively, the favored and less favored completions from $(y_1, y_2)$. These preferences are assumed to arise from an unknown latent reward function $r^*(y, x)$. There are several ways to formalize these preferences, with the Bradley-Terry (BT) model \cite{bradley1952rankanalysis} being especially prevalent (though more comprehensive models like Plackett-Luce \citep{plackett1975analysis, luce2012individual} can also be used if ranked responses are available). According to the BT model, the probability of human preferences $p^*$ is given by:
\begin{equation}\label{eq:bradley-terry}
    p^*(y_1\succ y_2 \mid x)=\frac{\exp\left(r^*(x, y_1)\right)}{\exp\left(r^*(x, y_1)\right) + \exp\left(r^*(x, y_2)\right)}.
\end{equation}
Given a fixed dataset of preference comparisons $\mathcal{D}=\bigl\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\bigr\}_{i=1}^N$ sampled from $p^*$, we introduce a parameterized reward model $r_{\phi}(x, y)$ and fit its parameters via maximum likelihood estimation. By casting this as a binary classification task, the negative log-likelihood objective is:
\begin{equation}\label{eq:reward_model}
    \mathcal{L}_R(r_{\phi}, \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\bigl[\log \sigma(r_{\phi}(x, y_w)- r_{\phi}(x, y_l))\bigr]
\end{equation}
where $\sigma$ is the sigmoid function. For language models, $r_{\phi}(x, y)$ is often initialized using the SFT model $\pisft(y \mid x)$, and a linear projection is appended atop the last transformer block to output a scalar reward prediction \cite{ziegler2020finetuning}. To reduce reward variance, earlier research commonly normalizes reward values such that $\mathbb{E}_{x,y\sim \mathcal{D}}\left[r_\phi(x, y)\right] = 0$ for all $x$.

\textbf{RL Fine-Tuning Phase}: In the RL fine-tuning stage, the learned reward model provides evaluation signals to the language model. As in earlier studies~\citep{jaques2017sequence, jaques2020human}, the optimization objective is formulated as:
\begin{equation}\label{eq:RL}
\max_{\pi_{\theta}}  \mathbb{E}_{x\sim \mathcal{D},\, y\sim \pi_{\theta}(y \mid x)}\Bigl[r_{\phi}(x, y)\Bigr] - \beta\,\mathbb{D}_{\textrm{KL}}\Bigl[\pi_{\theta}(y\mid x)\mid\mid \piref(y\mid x)\Bigr],
\end{equation}
where $\beta$ determines the allowable divergence from the initial reference policy $\piref$, specifically the SFT model $\pisft$. 
In actual implementation, the language model policy $\pi_\theta$ is initialized from $\pisft$. Including the KL constraint is essential: it ensures that the model does not stray excessively from the data distribution where the reward model provides reliable assessments, preserves generation variety, and helps avert mode collapse toward a single high-reward output. As language generation involves discrete outputs, the objective cannot be directly differentiated and is usually tackled with reinforcement learning algorithms. The prevailing method \citep{ziegler2020finetuning, stiennon2022learning, bai2022training, ouyang2022training} redefines the reward as ${r(x, y) = r_{\phi}(x, y) - \beta (\log \pi_{\theta}(y\mid x) - \log \piref(y\mid x))}$, which is then maximized via PPO \cite{schulman2017proximal}.

\section{Direct Preference Optimization}\label{sec:DPO}

Inspired by the difficulties of utilizing reinforcement learning algorithms for large-scale tasks like language model fine-tuning, we aim to develop a straightforward method for optimizing policies directly from preferences. In contrast to traditional RLHF approaches that first fit a reward model and subsequently optimize it through RL, our method exploits a specific reward model parameterization that allows us to analytically derive the optimal policy, thereby eliminating the need for an RL training process.

As we elaborate in the following sections, our central idea is to employ an analytic relationship between reward functions and their corresponding optimal policies, allowing us to recast a loss defined over reward models into a loss defined over policy parameters. This variable substitution technique bypasses the need to train a separate, explicit reward function while still conforming to established frameworks for modeling human preferences, such as the Bradley-Terry model. Essentially, the policy network simultaneously embodies both the language model and an implicit reward function.

\textbf{Derivation of the DPO objective.} We begin with the standard reinforcement learning objective from earlier work, Eq.~\ref{eq:RL}, employing a generic reward function $r$. As demonstrated in prior research~\citep{peters2007reinforcement, peng2019advantage, korbak2022reinforcement, go2023aligning}, it can be easily shown that the solution which optimizes the reward subject to a KL constraint, as specified in Eq.~\ref{eq:RL}, is given by:
\begin{equation}\label{eq:op_policy}
    \pi_r(y\mid x) = \frac{1}{Z(x)}\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right),
\end{equation}
where $Z(x) =\sum_{y}\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)$ acts as a normalization constant. A full derivation is included in Appendix \ref{app:derivation1}. Although one could use the maximum likelihood estimate $r_{\phi}$ in place of the actual reward $r^*$, calculating the partition function $Z(x)$ remains computationally intensive~\citep{korbak2022reinforcement, go2023aligning}, limiting practical use of this form. Instead, Eq.~\ref{eq:op_policy} can be reformulated to express the reward as a function of the optimal policy $\pi_r$, the baseline policy $\piref$, and the unknown normalizer $Z(\cdot)$. To do so, we take the logarithm of both sides of Eq.~\ref{eq:op_policy}, yielding:
\begin{equation}\label{eq:main_eq}
    r(x,y) =\beta \log \frac{\pi_r(y\mid x)}{\piref(y\mid x)} + \beta \log Z(x).
\end{equation}
Applying this change of variables to the true reward $r^*$ and its associated optimal policy $\pi^*$, we note that the Bradley-Terry model only requires the difference between the rewards for two completions: ${p^*(y_1 \succ y_2 \mid x) = \sigma(r^*(x, y_1) - r^*(x, y_2))}$. By substituting the reparameterized form from Eq.~\ref{eq:main_eq} into the preference model Eq.~\ref{eq:bradley-terry}, the terms involving the partition function drop out, leaving a preference probability in terms of only $\pi^*$ and $\piref$. Thus, under the Bradley-Terry framework, the optimal RLHF policy $\pi^*$ fulfills the preference relationship:
\begin{equation}\label{eq:objective}
    p^*(y_1\succ y_2 \mid x)=\frac{1}{1 + \exp\left(\beta \log \frac{\pi^*(y_2\mid x)}{\piref(y_2\mid x)} - \beta \log \frac{\pi^*(y_1\mid x)}{\piref(y_1\mid x)}\right)}
\end{equation}
See Appendix~\ref{app:derivation2} for the detailed steps. While Eq.~\ref{eq:objective} specifically leverages the Bradley-Terry model, equivalent derivations can be performed using the broader Plackett-Luce models~\citep{plackett1975analysis, luce2012individual}, which are further described in Appendix~\ref{app:plackett_luce_models}.

Having expressed the human preference probabilities in terms of the optimal policy rather than the reward function, we can now define a maximum likelihood objective for a parameterized policy $\pi_\theta$. In parallel with the reward modeling strategy (see Eq.~\ref{eq:reward_model}), our objective for the policy becomes:
\begin{equation}\label{eq:optimum_model}
    \mathcal{L}_\text{DPO}(\pi_{\theta}; \piref) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\left[\log \sigma \left(\beta \log \frac{\pi_{\theta}(y_w\mid x)}{\piref(y_w\mid x)} - \beta \log \frac{\pi_{\theta}(y_l\mid x)}{\piref(y_l\mid x)}\right)\right].
\end{equation}
Through this formulation, we infer an implicit reward by reparameterizing, where the resulting optimal policy is exactly $\pi_\theta$. Additionally, since this method corresponds to training a reparameterized Bradley-Terry model, it inherits theoretical guarantees, like consistency under appropriate assumptions about the distribution of preference data \cite{bong2022generalized}. Section~\ref{sec:theory} elaborates further on the theoretical aspects of DPO and compares it to related approaches.

\textbf{How does the DPO update function?} To gain a mechanistic insight into DPO, it's helpful to examine the gradient of its loss function $\mathcal{L}_\text{DPO}$. The gradient with respect to the model parameters $\theta$ can be expressed as:
\begin{multline*}\label{eq:gradient}
    \nabla_\theta \mathcal{L}_\text{DPO}(\pi_\theta;\piref) = \\ -\beta\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \bigg[\underbrace{\sigma(\hat{r}_\theta(x, y_l) - \hat{r}_\theta (x, y_w))}_\text{greater weight when reward ranking is incorrect}\bigg[\underbrace{\nabla_\theta\log \pi(y_w \mid x)}_\text{encourage $y_w$} - \underbrace{\nabla_\theta\log\pi(y_l \mid x)}_\text{discourage $y_l$}\bigg]\bigg],
\end{multline*}
with $\hat{r}_\theta(x, y) = \beta \log \frac{\pi_\theta(y \mid x)}{\piref(y \mid x)}$ representing the implicit reward derived from both the current model $\pi_\theta$ and the reference model $\piref$ (see Section~\ref{sec:theory} for further details). Conceptually, the gradient of $\mathcal{L}_\text{DPO}$ acts to raise the probability of the preferred responses $y_w$, while reducing the probability of less favored ones $y_l$. Notably, each training sample is scaled according to how much more highly the implicit reward $\hat{r}_\theta$ evaluates the less-preferred output, modulated by $\beta$. In other words, the update places more emphasis on instances where the model's internal reward function misranks the completions, accounting for the influence of the KL penalty. Our empirical studies indicate this weighting is vital; removing the coefficient—resulting in a simpler approach—can lead the language model to collapse (see Appendix Table~\ref{tab:unlikelihood_generations}).

\textbf{DPO overview.}  
The typical DPO workflow consists of: 1) Drawing completions $y_1, y_2 \sim \piref(\cdot \mid x)$ for each prompt $x$, then annotating them based on human preference to form an offline preference dataset $\mathcal{D} = \{x^{(i)}, y_w^{(i)}, y_l^{(i)}\}_{i=1}^N$; 2) Updating the language model $\pi_\theta$ by minimizing the loss function $\mathcal{L}_\text{DPO}$ using the chosen $\piref$, dataset $\mathcal{D}$, and a specified $\beta$.  
In real-world applications, it is preferable to leverage publicly released preference datasets rather than manually producing samples and collecting new human preference data. Since these datasets are typically collected using $\pisft$, we set $\piref = \pisft$ when $\pisft$ is accessible. Otherwise, if $\pisft$ is missing, we initialize $\piref$ by maximizing the likelihood over preferred completions ${(x, y_w)}$, specifically, ${\piref = \argmax_{\pi}\mathbb{E}_{x, y_w \sim \mathcal{D}}\left[\log \pi(y_w \mid x)\right]}$. This approach helps address the mismatch between the actual reference distribution, which is not available, and the $\piref$ employed in DPO. More information regarding implementation specifics and hyperparameter settings is included in Appendix~\ref{app:implementation}.

\section{Theoretical Examination of DPO}
Here, we offer additional insights into the DPO approach, establish its theoretical foundations, and discuss how the strengths of DPO address the limitations found in actor-critic techniques commonly applied in RLHF, including PPO~\cite{schulman2017proximal}.

\label{sec:theory}

\subsection{Your Language Model Is Secretly a Reward Model}  
DPO manages to avoid the need to explicitly learn a reward function and perform reinforcement learning, instead leveraging a single maximum likelihood objective for policy learning. Observe that the objective in Eq. \ref{eq:main_eq} matches the Bradley-Terry model, with the reward expressed as $r^*(x, y) = \beta \log\frac{\pi^*_\theta(y \mid x)}{\piref(y \mid x)}$. Here, the parameterized model $\pi_{\theta}$ is trained similarly to how a reward model would be optimized in Eq. \ref{eq:reward_model} after an appropriate variable substitution. In this part, we will develop the theoretical foundation of this reparameterization, demonstrate that it imposes no restrictions on the types of reward models that can be learned, and prove it enables the exact retrieval of the optimal policy. We will start by introducing an equivalence relation for reward functions.

\begin{definition}
Two reward functions $r(x, y)$ and $r'(x, y)$ are called equivalent if and only if there exists a function $f$ such that $r(x, y) - r'(x, y) = f(x)$.
\end{definition}
It is straightforward to verify that this defines an equivalence relation, dividing the collection of reward functions into distinct classes. We can now present these two lemmas:

\begin{lemma}\label{lemma:same_prefrence} Within the Plackett-Luce model, including the Bradley-Terry setup, any two reward functions belonging to the same category yield identical preference distributions.
\end{lemma}

\begin{lemma}\label{lemma:same_policy}
    Any two reward functions belonging to the same equivalence class yield identical optimal policies in the constrained RL setting.
\end{lemma}
The proofs are elementary and can be found in Appendix \ref{app:lemma1}. Lemma~\ref{lemma:same_policy} highlights a classic under-determination property of the Plackett-Luce family of models \cite{plackett1975analysis}. Because of this ambiguity, it is standard to enforce additional identifiability restrictions in order to guarantee desirable properties of the MLE solutions described by Eq. \ref{eq:reward_model} \cite{bong2022generalized}. The second lemma establishes that every reward function from a given equivalence class leads to the same optimal policy. As a consequence, for our main goal, it suffices to recover any single reward function within the optimal equivalence class. We establish the following Theorem, with proof provided in Appendix~\ref{app:thm1}:
\begin{theorem}\label{thm:main}
    Given mild conditions, all reward equivalence classes consistent with the Plackett-Luce (and, specifically, Bradley-Terry) models admit a reparameterization of the form ${r(x, y) = \beta \log \frac{\pi(y\mid x)}{\piref(y\mid x)}}$ for some model $\pi(y\mid x)$ and reference model $\piref(y \mid x)$.
\end{theorem}
\begin{sproof}
    Let $r(x, y)$ be a reward function, and let its corresponding optimal model be $\pi_r(y \mid x)$, as defined in Eq. \ref{eq:op_policy}. We demonstrate that any member of the equivalence class containing $r$ can be written in the reparameterized form above. Define the projection operator $f$ by  
\begin{equation}
    f(r; \piref, \beta)(x, y) = r(x, y) - \beta\log\sum_{y}\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)
\end{equation}
Here, $f$ serves to normalize the reward using the log-partition function of $\pi_r$. Because the normalization depends solely on $x$, $f(r; \piref, \beta)(x, y)$ remains within the same equivalence class as $r(x, y)$. Substituting $r$ with the right-hand side of Eq.~\ref{eq:main_eq} (which is valid for any reward), we get $f(r; \piref, \beta)(x, y) = \beta \log \frac{\pi_r(y\mid x)}{\piref(y\mid x)}$. Thus, the mapping $f$ generates an equivalent reward in the desired form, so our proposed reparameterization does not restrict the generality of the reward model.
\end{sproof}
Theorem~\ref{thm:main} can also be interpreted as characterizing precisely which reward function in each equivalence class is selected by the DPO reparameterization, namely, the reward function that satisfies:
\begin{equation}\label{eq:lag_p}
     \sum_{y}\underbrace{\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)}_{=\pi(y\mid x)\text{, by Theorem~\ref{thm:main} reparameterization}} = 1,
\end{equation}
meaning that $\pi(y\mid x)$ is a proper probability distribution (non-negative and sums to one).
Further, from Eq.~\ref{eq:op_policy}, it is evident that Eq.~\ref{eq:lag_p} gives the partition function for the optimal policy derived from $r(x, y)$. The main idea behind the DPO algorithm is that by enforcing certain constraints on the inherently under-constrained Plackett-Luce (and specifically Bradley-Terry) preference models, we retain the same class of recoverable reward models, but make the optimal policy in Eq. \ref{eq:op_policy} exactly computable for any prompt $x$.

\subsection{Actor-Critic Algorithm Instabilities}

Our framework can be leveraged to analyze instabilities commonly observed in actor-critic algorithms like PPO, which are widely used for RLHF. Adhering to the RLHF procedure, we concentrate on the RL fine-tuning phase as described in Section \ref{section:prelims}. This setup can be linked to the control-as-inference perspective \cite{levine2018reinforcement} when applied to the constrained RL formulation in \ref{eq:RL}. Given a policy model $\pi_{\theta}(y\mid x)$, our objective is to minimize $\mathbb{D}_{\text{KL}}[\pi_{\theta}(y|x) \mid \mid \pi^*(y\mid x)]$, where $\pi^*$ refers to the optimal policy derived in Eq. \ref{eq:optimum_model} using the reward $r_{\phi}(y, x)$. After some manipulations, the objective simplifies to:
\begin{equation}\label{eq:AC}
    \max_{\pi_{\theta}}\mathbb{E}_{\pi_{\theta}(y\mid x)}\left[\underbrace{r_{\phi}(x, y) -\beta\log\sum_{y}\piref(y\mid x)\exp\left(\frac{1}{\beta}r_{\phi}(x, y)\right)}_{f(r_{\phi}, \piref, \beta)} - \underbrace{\beta\log\frac{\pi_{\theta}(y\mid x)}{\piref(y\mid x)}}_{\text{KL}}\right]
\end{equation}
This form matches the objective used in earlier works \citep{ziegler2020finetuning, stiennon2022learning, bai2022training, ouyang2022training} for the DPO-equivalent reward function within the $r_{\phi}$ class. Here, the normalization factor in $f(r_{\phi}, \piref, \beta)$ can be interpreted as the soft value function of the reference policy $\piref$. Although this component does not influence the optimal policy, its absence may lead to high-variance policy gradients, which can destabilize training. One possible solution is to estimate this normalization via a learned value function; however, this approach often proves challenging to optimize in practice. Previous research has sometimes sidestepped this difficulty by normalizing rewards using a baseline derived from a human-generated completion, effectively treating it as a Monte Carlo approximation of the normalizing factor. By contrast, the DPO reparameterization constructs a reward function that eliminates the need for such baselines.

\section{Experiments}
Here, we empirically assess how well DPO learns policies directly from preference data. We begin with a controlled text generation scenario to investigate how effectively DPO balances optimizing reward and maintaining low KL-divergence with the reference policy, in comparison to standard preference-based methods like PPO. Subsequently, we test DPO on larger language models and more complex RLHF tasks, such as dialogue and summarization. Our findings indicate that, even with minimal hyperparameter tuning, DPO often matches or surpasses robust baselines like RLHF with PPO, and outperforms selecting the best of $N$ sampled outputs based on a learned reward model. Prior to sharing our results, we outline the experimental methodology; further specifics can be found in Appendix~\ref{app:exp_details}.

\textbf{Tasks.} Our study examines three distinct open-domain text generation challenges. Across all experiments, methods are trained to optimize a policy from a dataset of human preferences, denoted as $\mathcal{D} = \bigl\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\bigr\}_{i=1}^N$. In the \textbf{controlled sentiment generation} task, $x$ refers to an initial segment of a movie review taken from the IMDb dataset \cite{maas-EtAl:2011:ACL-HLT2011}, and the objective is for the policy to produce a continuation $y$ with positive sentiment. To ensure a controlled evaluation, we generate preference data using a pre-trained sentiment classifier that selects $y_w$ and $y_l$ such that $p(\text{positive} \mid x, y_w) > p(\text{positive} \mid x, y_l)$. For supervised fine-tuning (SFT), GPT-2-large is fine-tuned to convergence on movie reviews in the IMDB training set (details in App~\ref{app:sentiment_details}). In the \textbf{summarization} task, $x$ represents a Reddit forum post, and the model is required to produce a summary $y$ capturing the key points. As in previous work, we utilize the Reddit TL;DR summarization dataset \citep{volske-etal-2017-tl} together with human preference annotations from \citeauthor{stiennon2022learning}. The SFT baseline involves fine-tuning a model on forum post summaries written by humans\footnote{\url{https://huggingface.co/CarperAI/openai_summarize_tldr_sft}} and using the TRLX \citep{leandro_von_werra_2023_7790115} toolkit for RLHF. The preference data was collected by \citeauthor{stiennon2022learning} using outputs from a separately trained SFT model. For the \textbf{single-turn dialogue} setting, $x$ is a user prompt, which can range from scientific queries to personal advice requests, and the policy’s goal is to generate a helpful and engaging reply $y$. We rely on the Anthropic Helpful and Harmless dialogue dataset \citep{bai2022training}, which comprises 170k interactions between humans and a conversational agent. Each record concludes with two candidate responses from a large language model and a label indicating the one favored by a human annotator. As there is no available SFT model for this domain, we fine-tune a generic language model solely on the preferred completions to obtain our SFT baseline.

\begin{figure}
    \centering
    \includegraphics[width=0.50\textwidth]{figures/results/frontier.pdf}
    \includegraphics[width=0.49\textwidth]{figures/results/tldr_winrate_vs_temp.pdf}
    \caption{\textbf{Left.} The trade-off curve between expected reward and KL divergence from the reference policy. DPO achieves the highest expected reward across all KL thresholds, highlighting its optimization effectiveness. \textbf{Right.} TL;DR summarization win rates in comparison with human-generated summaries, assessed by GPT-4. DPO surpasses PPO's maximum summarization performance and shows greater stability across varying sampling temperatures.}
    \vspace{-2mm}
    \label{fig:frontier-tldr-main}
\end{figure}

\textbf{Evaluation.} Our experiments incorporate two distinct evaluation strategies. To assess how well each algorithm optimizes the constrained reward maximization objective, we first focus on the controlled sentiment generation scenario. Here, we measure each method by plotting its achievable reward versus KL-divergence from the reference policy; this trade-off curve can be computed directly, as we have access to the ground-truth reward function (a sentiment classifier). However, in practical settings where the true reward function is unknown, we instead assess each algorithm’s \textit{win rate} relative to a baseline policy. For this, we use GPT-4 as a stand-in for human judgment: in summarization tasks, GPT-4 evaluates the quality of summaries, while in single-turn dialogue, it judges response helpfulness. The baseline for summarization is the reference summaries from the test set, and for dialogue, it is the test set’s preferred response. While prior work suggests that large language models may outperform traditional metrics as automated evaluators \citep{Chen2023ExploringTU}, we further validate our reliance on GPT-4 through a human study detailed in Sec.~\ref{sec:human-judgments}. Our results indicate that GPT-4's evaluations are strongly aligned with human judgments, with human-GPT-4 agreement often matching or exceeding the consistency observed among human annotators.

\textbf{Methods.} Alongside DPO, we assess a range of established techniques for training language models to better reflect human preferences. For the summarization task, we utilize zero-shot prompting with \textbf{GPT-J} \citep{gpt-j}, while in the dialogue task, we employ 2-shot prompting using \textbf{Pythia-2.8B} \citep{biderman2023pythia}. We also evaluate the \textbf{SFT} model, as well as \textbf{Preferred-FT}, which involves supervised fine-tuning on the preferred completion $y_w$, sourced from the SFT model for sentiment control and summarization, or from a standard LM in single-turn dialogue scenarios. Another approach, known as \textbf{Unlikelihood}~\citep{welleck2019neural}, pseudo-supervised in nature, aims to maximize the likelihood of $y_w$ while explicitly decreasing the likelihood of $y_l$; here, an optional scaling factor $\alpha\in[0,1]$ is applied to the unlikelihood objective. We further include \textbf{PPO} \citep{schulman2017proximal}, which utilizes a reward model trained on preference annotations, and \textbf{PPO-GT}, a variant that leverages the true reward function available in the sentiment control experiments. For sentiment control, we test two variants of PPO-GT: a default implementation \cite{leandro_von_werra_2023_7790115}, and a modified version that normalizes the rewards and tunes hyperparameters for enhanced results; these improvements are likewise applied to PPO runs with learned rewards. Lastly, we analyze the \textbf{Best of $N$} baseline, which selects the best response among $N$ samples generated by the SFT model (or Preferred-FT in dialogue), according to a reward model trained on preference data. While this method achieves strong performance by separating reward model accuracy from PPO optimization, it is computationally expensive for even modest values of $N$ since each test query requires generating $N$ distinct completions.

\subsection{How well can DPO optimize the RLHF objective?}

\begin{figure}
    \centering
    \includegraphics[width=0.50\textwidth]{figures/results/dialogue_winrate_vs_temp.pdf}
    \includegraphics[width=0.49\textwidth]{figures/results/dialogue_winrate_vs_steps.pdf}
    \caption{\textbf{Left.} GPT-4-based win rates on Anthropic-HH single-turn dialogue; among the methods, DPO is the sole approach that surpasses the chosen summaries from the Anthropic-HH evaluation set. \textbf{Right.} Win rates tracked for various sampling temperatures throughout training. DPO consistently outperforms the reference dataset labels during training, regardless of sampling temperature.}
    \vspace{-2mm}
    \label{fig:dialogue-main}
\end{figure}

The KL-regularized reward optimization objective commonly employed in RLHF methods aims to maximize reward while constraining the policy to remain close to the reference policy. As a result, when evaluating different algorithms, it is important to consider both the attained reward and the corresponding KL divergence; obtaining marginally higher rewards at the expense of significantly increased KL is often suboptimal. Figure~\ref{fig:frontier-tldr-main} depicts the reward-KL tradeoff frontier for different methods under the sentiment task. For each algorithm, we conduct several training runs, each with a distinct policy conservativeness hyperparameter (target KL $\in\{3,6,9,12\}$ for PPO, $\beta \in \{0.05,0.1,1,5\}$, $\alpha\in\{0.05,0.1,0.5,1\}$ for unlikelihood, and varying random seeds for preferred-FT), amounting to 22 runs in total. After every 100 training iterations up to convergence, we assess each policy on a suite of test prompts, measuring both the mean reward using the true reward function and the average sequence-level KL\footnote{Specifically, the total of per-timestep KL-divergences.} relative to the reference policy $\text{KL}\left(\pi\mid \mid \piref\right)$. Our findings indicate that DPO establishes by far the most favorable frontier, attaining the greatest reward with consistently low KL. This outcome is striking for several reasons. Firstly, although DPO and PPO share the same optimization target, DPO is markedly more sample-efficient and yields a superior reward/KL tradeoff, entirely outperforming PPO. Secondly, DPO defines a stronger frontier than PPO, \emph{even when PPO is given access to ground-truth rewards} (PPO-GT).

\subsection{Can DPO handle large-scale preference datasets?}
\label{sec:dpo-real-datasets}
We proceed to assess DPO’s fine-tuning performance on tasks like summarization and single-turn dialogue. For summarization, standard automatic metrics such as ROUGE are known to show weak alignment with human judgments~\citep{stiennon2022learning}. Prior studies have demonstrated that training language models with PPO using human preference data leads to more effective summaries. To compare methods, we generate completions using the test partition of the TL;DR summarization dataset and calculate the mean win rate against reference summaries from the test set. Each method’s completions are generated using sampling temperatures ranging from 0.0 to 1.0, with win rates illustrated in Figure~\ref{fig:frontier-tldr-main} (right). DPO, PPO, and Preferred-FT all perform fine-tuning on the identical GPT-J SFT model\footnote{\url{https://huggingface.co/CarperAI/openai_summarize_tldr_sft}}. Our experiments show DPO attains a win rate close to 61\% at temperature 0.0, outperforming PPO, which peaks at roughly 57\% at the same temperature. DPO also surpasses the best of $N$ baseline in terms of maximum win rate. Importantly, we did not conduct extensive tuning of the DPO $\beta$ hyperparameter, suggesting the current results may not reflect the full potential of DPO. Additionally, DPO’s performance remains considerably more stable across different sampling temperatures compared to PPO, which can drop to the level of the original GPT-J model at higher temperatures. Preferred-FT, by contrast, does not offer meaningful improvements over the SFT baseline. We further provide a direct comparison of DPO and PPO in human evaluation experiments in Section~\ref{sec:human-judgments}, where DPO completions sampled at temperature 0.25 were favored 58\% of the time over PPO completions generated at temperature 0.

For single-turn dialogue, we assess the various approaches on a portion of the test set from the Anthropic HH dataset \citep{bai2022training}, focusing on instances with a single exchange between human and assistant. To measure win rates, GPT-4 evaluations use the test's preferred completions as references for each method. Since there is no established SFT model tailored for this task, we begin with a pre-trained Pythia-2.8B, employ Preferred-FT to fine-tune a reference model on selected completions—ensuring the outputs remain in-distribution—and subsequently train with DPO. We also benchmark against the top completion out of 128 from Preferred-FT (noting that the Best of $N$ baseline levels off at 128 completions for this scenario; see Appendix Figure~\ref{fig:best-of-n}) and compare with a 2-shot prompted Pythia-2.8B base model. Our findings reveal that DPO achieves equal or superior results at optimal temperatures for each method. Additionally, we evaluate an RLHF model trained with PPO on the Anthropic HH dataset\footnote{\url{https://huggingface.co/reciprocate/ppo_hh_pythia-6B}} using code from a reputable repository\footnote{\url{https://github.com/CarperAI/trlx/tree/main/examples/hh}}, but could not identify a prompt or temperature combination that surpasses the performance of the original Pythia-2.8B model. Drawing from TL;DR experiments and since both PPO and Best of 128 optimize the same reward, we treat Best of 128 as an approximate indicator for PPO-level results. In summary, DPO stands out as the only efficient method to surpass the preferred completions in the Anthropic HH dataset, matching or exceeding the accuracy of the more resource-intensive Best of 128 baseline. Lastly, as depicted in Figure~\ref{fig:dialogue-main}, DPO rapidly reaches its optimal performance.

\subsection{Generalization to a new input distribution}

\begin{wraptable}{r}{0.375\textwidth}
    \small
    \vspace{-10mm}
    \begin{tabular}{ccc}
        \toprule
        & \multicolumn{2}{c}{\textbf{Win rate compared to ground truth}} \\
        \cmidrule(lr){2-3}
        \textbf{Alg.} & Temp $0$ & Temp $0.25$ \\
        \midrule
        DPO & 0.36 & 0.31 \\
        PPO & 0.26 & 0.23 \\
        \bottomrule
    \end{tabular}
    \caption{GPT-4 win rates when evaluated against ground truth summaries on out-of-distribution CNN/DailyMail articles.}
    \vspace{-3mm}
    \label{tab:ood}
\end{wraptable}

To more thoroughly assess how PPO and DPO perform under distribution shifts, we test the PPO and DPO models trained on the Reddit TL;DR summarization task on a different domain: news articles from the test split of the CNN/DailyMail dataset \citep{nallapati-etal-2016-abstractive}, using the optimal sampling temperatures determined for TL;DR (0 and 0.25). The outcomes are shown in Table~\ref{tab:ood}. We calculated the GPT-4 win rate against the ground-truth summaries in these datasets, employing the same GPT-4 (C) prompt as for Reddit TL;DR, but substituting “forum post” with “news article”. On this shifted distribution, DPO still significantly surpasses the PPO policy. These findings offer preliminary support that DPO policies can generalize at least as well as PPO policies, despite DPO not having access to the extra unlabeled Reddit TL;DR prompts available to PPO.

\subsection{Comparing GPT-4 assessments to human evaluations}
\label{sec:human-judgments}
We carry out a human evaluation to test how dependable GPT-4's assessments are, using the TL;DR summarization task and two separate GPT-4 prompts. The \textbf{GPT-4 (S)} (simple) prompt requests a decision about which summary better covers the key content of the post. The \textbf{GPT-4 (C)} (concise) prompt also asks which summary is more succinct; we include this prompt since we observe that GPT-4, when using the \textbf{GPT-4 (S)} prompt, tends to favor longer and more repetitive summaries than what humans usually prefer. Full prompt details are available in Appendix~\ref{app:prompts}. We compare three different settings: the best-performing model (DPO, temp. 0.25), the lowest-performing model (PPO, temp. 1.0), and a 

\begin{wraptable}{r}{0.47\textwidth}
    \centering
    \small
    \vspace{-1.5mm}
    \begin{tabular}{lccc}
    \toprule
        & \textbf{DPO} & \textbf{SFT} & \textbf{PPO-1} \\
        \cmidrule(lr){2-4}
        N respondents & 272 & 122 & 199 \\
        \midrule
        GPT-4 (S) win \% & 47 & 27 & 13 \\
        GPT-4 (C) win \% & 54 & 32 & 12 \\
        Human win \% & 58 & 43 & 17 \\
        \midrule
        GPT-4 (S)-H agree & 70 & 77 & 86 \\
        GPT-4 (C)-H agree & 67 & 79 & 85 \\
        H-H agree & 65 & - & 87 \\
        \bottomrule
    \end{tabular}
    \vspace{-1mm}
    \caption{Win rates for humans and GPT-4, plus agreement rates on TL;DR summarization tasks. \textbf{Human raters agree with GPT-4 at similar rates as they do with each other.} In each setting, a summary from the listed method is evaluated against a summary from PPO at temperature 0.}
    \vspace{-5mm}
    \label{tab:human_results}
\end{wraptable}

middle-performing method (SFT, temp. 0.25) to span a range of output qualities. Each is compared against PPO sampled greedily (its top-performing temperature). Our findings show that, for both prompts, GPT-4's choices align with those of human judges nearly as frequently as humans agree amongst themselves, indicating that GPT-4 serves as a reasonable substitute for human evaluators (due to a limited number of human raters, we only gather multiple human judgments for DPO and PPO-1). In general, the \textbf{GPT-4 (C)} prompt gives win rates that more closely resemble those of humans; therefore, we adopt this prompt for our main analyses in Section~\ref{sec:dpo-real-datasets}. Further information regarding the human study—including the interface shown to participants and the volunteer roster—can be found in Appendix~\ref{app:human-study}.

\section{Discussion}
Preference-based learning is an effective and scalable approach for developing competent and aligned language models. In this work, we presented DPO, a straightforward training method that enables language models to learn from preferences without relying on reinforcement learning. Instead of reformulating preference learning as a traditional RL problem to leverage standard RL techniques, DPO establishes a correspondence between language model policies and reward functions, making it possible to train models to adhere to human preferences \textit{directly} using a basic cross-entropy loss, completely sidestepping reinforcement learning and without sacrificing generality. With almost no need for hyperparameter adjustment, DPO achieves performance comparable to or better than current RLHF methods, such as those utilizing PPO; consequently, DPO significantly lowers the barriers to training language models using human feedback.

\textbf{Limitations \& Future Work.} Our findings highlight several key questions for subsequent research. For instance, how well does the DPO policy generalize to out-of-distribution data compared to methods that rely on a defined reward function? Preliminary evidence indicates that DPO policies exhibit generalization comparable to models trained with PPO, but further extensive investigation is warranted. Another open question is whether leveraging self-labeling with the DPO policy can similarly harness the potential of unlabeled prompts. Furthermore, it remains to be explored how reward over-optimization appears in the context of direct preference optimization, and whether the modest decline in performance depicted in Figure~\ref{fig:dialogue-main}-right represents such an effect. Although our current evaluations are limited to models with up to 6B parameters, scaling DPO to much larger, state-of-the-art models is a promising avenue for upcoming research. In terms of assessments, we observed that win rates reported by GPT-4 can vary depending on the prompt used; thus, future work could investigate optimal strategies for obtaining reliable judgments from automated evaluators. Lastly, DPO presents a wide range of potential applications beyond refining language models from human feedback, such as training generative models across various modalities.

\end{document}