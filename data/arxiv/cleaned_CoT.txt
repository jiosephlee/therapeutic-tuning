\title{Chain-of-Thought Prompting Elicits Reasoning  in Large Language Models}

\begin{abstract}
We explore how generating a \textit{chain of thought}---a series of intermediate reasoning steps---significantly improves the ability of large language models to perform complex reasoning.
In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called \textit{chain-of-thought prompting}, where a few chain of thought demonstrations are provided as exemplars in prompting.
Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks.
The empirical gains can be striking.
For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.
\end{abstract}

\section{Introduction}
The NLP landscape has recently been revolutionized by language models \citep[][\textit{inter alia}]{peters-etal-2018-deep,devlin-etal-2019-bert,brown2020language}.
Scaling up the size of language models has been shown to confer a range of benefits, such as improved performance and sample efficiency  \citep[\textit{inter alia}]{kaplan2020scaling,brown2020language}.
However, scaling up model size alone has not proved sufficient for achieving high performance on challenging tasks such as arithmetic, commonsense, and symbolic reasoning \citep{rae2021scaling}.

This work explores how the reasoning ability of large language models can be unlocked by a simple method motivated by two ideas. %
First, techniques for arithmetic reasoning can benefit from generating natural language rationales that lead to the final answer.
Prior work has given models the ability to generate natural language intermediate steps by training from scratch \citep{ling-etal-2017-program} or finetuning a pretrained model \citep{cobbe2021training}, in addition to neuro-symbolic methods that use formal languages instead of natural language \citep{roy-roth-2015-solving, chiang-chen-2019-semantically,amini-etal-2019-mathqa, chen2019neural}.
Second, large language models offer the exciting prospect of in-context few-shot learning via \emph{prompting}.
That is, instead of finetuning a separate language model checkpoint for each new task, one can simply ``prompt'' the model with a few input--output exemplars demonstrating the task.
Remarkably, this has been successful for a range of simple question-answering tasks \citep{brown2020language}.

Both of the above ideas, however, have key limitations. For rationale-augmented training and finetuning methods, it is costly to create a large set of high quality rationales, which is much more complicated than simple input--output pairs used in normal machine learning. 
For the traditional few-shot prompting method used in \citet{brown2020language}, it works poorly on tasks that require reasoning abilities, and often does not improve substantially with increasing language model scale \citep{rae2021scaling}. 
In this paper, we combine the strengths of these two ideas in a way that avoids their limitations. Specifically, we explore the ability of language models to perform few-shot prompting for reasoning tasks, given a prompt that consists of triples: $\langle$input, \emph{chain of thought}, output$\rangle$.
A \emph{chain of thought} is a series of intermediate natural language reasoning steps that lead to the final output, and we refer to this approach as \textit{chain-of-thought prompting}. An example prompt is shown in \cref{fig:pull-figure}. %

We present empirical evaluations on arithmetic, commonsense, and symbolic reasoning benchmarks, showing that chain-of-thought prompting outperforms standard prompting, sometimes to a striking degree.
\cref{fig:pull-bar-chart} illustrates one such result---on the GSM8K benchmark of math word problems \citep{cobbe2021training}, chain-of-thought prompting with \palm{} 540B outperforms standard prompting by a large margin and achieves new state-of-the-art performance.
A prompting only approach is important because it does not require a large training dataset and because a single model checkpoint can perform many tasks without loss of generality.
This work underscores how large language models can learn via a few examples with natural language data about the task (c.f. automatically learning the patterns underlying inputs and outputs via a large training dataset).

\section{Chain-of-Thought Prompting}
Consider one's own thought process when solving a complicated reasoning task such as a multi-step math word problem. 
It is typical to decompose the problem into intermediate steps and solve each before giving the final answer: \textit{``After Jane gives 2 flowers to her mom she has 10 $\ldots$ then after she gives 3 to her dad she will have 7 $\ldots$ so the answer is 7.''}
The goal of this paper is to endow language models with the ability to generate a similar \textit{chain of thought}---a coherent series of intermediate reasoning steps that lead to the final answer for a problem.
We will show that sufficiently large language models can generate chains of thought if demonstrations of chain-of-thought reasoning are provided in the exemplars for few-shot prompting.

\cref{fig:pull-figure} shows an example of a model producing a chain of thought to solve a math word problem that it would have otherwise gotten incorrect.
The chain of thought in this case resembles a solution and can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it mimics a step-by-step thought process for arriving at the answer (and also, solutions/explanations typically come \textit{after} the final answer \citep[][\textit{inter alia}]{narang2020wt5,wiegreffe2021reframing,lampinen2022can}).

Chain-of-thought prompting has several attractive properties as an approach for facilitating reasoning in language models.
\begin{enumerate}[topsep=1pt,itemsep=0ex]%
    \item First, chain of thought, in principle, allows models to decompose multi-step problems into intermediate steps, which means that additional computation can be allocated to problems that require more reasoning steps.
    \item Second, a chain of thought provides an interpretable window into the behavior of the model, suggesting how it might have arrived at a particular answer and providing opportunities to debug where the reasoning path went wrong (although fully characterizing a model's computations that support an answer remains an open question).
    \item Third, chain-of-thought reasoning can be used for tasks such as math word problems, commonsense reasoning, and symbolic manipulation, and is potentially applicable (at least in principle) to any task that humans can solve via language.
    \item Finally, chain-of-thought reasoning can be readily elicited in sufficiently large off-the-shelf language models simply by including examples of chain of thought sequences into the exemplars of few-shot prompting.
\end{enumerate}

In empirical experiments, we will observe the utility of chain-of-thought prompting for arithmetic reasoning (\cref{sec:arithmetic-reasoning}), commonsense reasoning (\cref{sec:commonsense-reasoning}), and symbolic reasoning (\cref{sec:symbolic-reasoning}).

\section{Arithmetic Reasoning}\label{sec:arithmetic-reasoning}
We begin by considering math word problems of the form in \cref{fig:pull-figure}, which measure the arithmetic reasoning ability of language models.
Though simple for humans, arithmetic reasoning is a task where language models often struggle \citep[][\textit{inter alia}]{hendrycks2021measuring,patel-etal-2021-nlp}.
Strikingly, chain-of-thought prompting when used with the 540B parameter language model performs comparably with task-specific finetuned models on several tasks, even achieving new state of the art on the challenging GSM8K benchmark \citep{cobbe2021training}.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{main_fables/cot-examples.pdf}
\caption{
Examples of $\langle$input, chain of thought, output$\rangle$ triples for arithmetic, commonsense, and symbolic reasoning benchmarks. Chains of thought are highlighted.
Full prompts in \cref{sec:appendix-full-prompts}.
}
\vspace{-5mm}~
\label{fig:dataset-examples}
\end{figure}

\subsection{Experimental Setup}

We explore chain-of-thought prompting for various language models on multiple benchmarks.

\textbf{Benchmarks.}
We consider the following five math word problem benchmarks:
\textbf{(1)} the \textbf{GSM8K} benchmark of math word problems \citep{cobbe2021training}, 
\textbf{(2)} the \textbf{SVAMP} dataset of math word problems with varying structures \citep{patel-etal-2021-nlp},
\textbf{(3)} the \textbf{ASDiv} dataset of diverse math word problems \citep{miao-etal-2020-diverse},
\textbf{(4)} the \textbf{AQuA} dataset of algebraic word problems, and
\textbf{(5)} the \textbf{MAWPS} benchmark \citep{koncel-kedziorski-etal-2016-mawps}.
Example problems are given in Appendix \cref{tab:math-datasets}.

\textbf{Standard prompting.}
For the baseline, we consider standard few-shot prompting, popularized by \citet{brown2020language}, in which a language model is given in-context exemplars of input--output pairs before outputting a prediction for a test-time example. 
Exemplars are formatted as questions and answers. 
The model gives the answer directly, as shown in \cref{fig:pull-figure} (left).

\textbf{Chain-of-thought prompting.}
Our proposed approach is to augment each exemplar in few-shot prompting with a chain of thought for an associated answer, as illustrated in \cref{fig:pull-figure} (right). 
As most of the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars with chains of thought for prompting---\cref{fig:pull-figure} (right) shows one chain of thought exemplar, and the full set of exemplars is given in Appendix \cref{tab:appendix-mwp-prompts}. 
(These particular exemplars did not undergo prompt engineering; robustness is studied in \cref{subsec:robustness} and \cref{subsec:faq-prompt-engineering}.)
To investigate whether chain-of-thought prompting in this form can successfully elicit successful reasoning across a range of math word problems, we used this single set of eight chain of thought exemplars for all benchmarks except AQuA, which is multiple choice instead of free response.
For AQuA, we used four exemplars and solutions from the training set, as given in Appendix \cref{tab:appendix-aqua-prompt}.

\textbf{Language models.}
We evaluate five large language models.
The first is \textbf{GPT-3} \citep{brown2020language}, for which we use text-ada-001, text-babbage-001, text-curie-001, and text-davinci-002, which presumably correspond to InstructGPT models of 350M, 1.3B, 6.7B, and 175B parameters \citep{ouyang2022training}.%
The second is \textbf{\lamda{}} \citep{thoppilan2022lamda}, which has models of 422M, 2B, 8B, 68B, and 137B parameters. %
The third is \textbf{\palm{}}, which has models of 8B, 62B, and 540B parameters.
The fourth is \textbf{UL2 20B} \citep{tay2022unifying}, and the fifth is \textbf{Codex} \citep[][code-davinci-002 in the OpenAI API]{chen2021evaluating}.
We sample from the models via greedy decoding (though follow-up work shows chain-of-thought prompting can be improved by taking the majority final answer over many sampled generations \citep{wang2022self}).
For \lamda{}, we report averaged results over five random seeds, where each seed had a different randomly shuffled order of exemplars.
As \lamda{} experiments did not show large variance among different seeds, to save compute we report results for a single exemplar order for all other models.

\subsection{Results}
The strongest results of chain-of-thought prompting are summarized in \cref{fig:main-math}, with all experimental outputs for each model collection, model size, and benchmark shown in \cref{tab:all-lm-math} in the Appendix.
There are three key takeaways.
First, \cref{fig:main-math} shows that chain-of-thought prompting is an emergent ability of model scale \citep{wei2022emergent}. 
That is, chain-of-thought prompting does not positively impact performance for small models, and only yields performance gains when used with models of $\sim$100B parameters.
We qualitatively found that models of smaller scale produced fluent but illogical chains of thought, leading to lower performance than standard prompting.

\input{main_fables/main-math}
Second, chain-of-thought prompting has larger performance gains for more-complicated problems. 
For instance, for GSM8K (the dataset with the lowest baseline performance), performance more than doubled for the largest GPT and \palm{} models. 
On the other hand, for SingleOp, the easiest subset of MAWPS which only requires a single step to solve, performance improvements were either negative or very small (see Appendix \cref{tab:mawps-subsets}).

Third, chain-of-thought prompting via GPT-3 175B and \palm{} 540B compares favorably to prior state of the art, which typically finetunes a task-specific model on a labeled training dataset. 
\cref{fig:main-math} shows how \palm{} 540B uses chain-of-thought prompting to achieve new state of the art on GSM8K, SVAMP, and MAWPS (though note that standard prompting already passed the prior best for SVAMP).
On the other two datasets, AQuA and ASDiv, \palm{} with chain-of-thought prompting reaches within 2\% of the state of the art (Appendix \cref{tab:all-lm-math}).

To better understand why chain-of-thought prompting works,
we manually examined model-generated chains of thought by \lamda{} 137B for GSM8K.
Of 50 random examples where the model returned the correct final answer, all of the generated chains of thought were also logically and mathematically correct except two that coincidentally arrived at the correct answer (see \cref{subsec:correct-chain-of-thought}, and \cref{tab:appendix-gsm8k-correct-examples} for examples of correct model-generated chains of thought).
We also randomly examined 50 random samples for which the model gave the wrong answer.
The summary of this analysis is that 46\% of the chains of thought were almost correct, barring minor mistakes (calculator error, symbol mapping error, or one reasoning step missing), and that the other 54\% of the chains of thought had major errors in semantic understanding or coherence (see \cref{subsec:incorrect-chain-of-thought-analysis}). 
To provide a small insight into why scaling improves chain-of-thought reasoning ability, we performed a similar analysis of errors made by \palm{} 62B and whether those errors were fixed by scaling to \palm{} 540B.
The summary is that scaling \palm{} to 540B fixes a large portion of one-step missing and semantic understanding errors in the 62B model (see \cref{subsec:why-scale-helps}).

\subsection{Ablation Study}\label{subsec:math-ablation}

The observed benefits of using chain-of-thought prompting raises the natural question of whether the same performance improvements can be conferred via other types of prompting.
\cref{fig:bar_ablation} shows an ablation study with three variations of chain of thought described below.

\textbf{Equation only.} One reason for why chain-of-thought prompting might help is that it produces the mathematical equation to be evaluated, and so we test a variation where the model is prompted to output only a mathematical equation before giving the answer. 
\cref{fig:bar_ablation} shows that equation only prompting does not help much for GSM8K, which implies that the semantics of the questions in GSM8K are too challenging to directly translate into an equation without the natural language reasoning steps in chain of thought.
For datasets of one-step or two-step problems, however, we find that equation only prompting does improve performance, since the equation can be easily derived from the question (see Appendix \cref{tab:ablations-arithmetic}). 

\input{main_fables/ablation-bar}
\textbf{Variable compute only.}
Another intuition is that chain of thought allows the model to spend more computation (i.e., intermediate tokens) on harder problems. 
To isolate the effect of variable computation from chain-of-thought reasoning, we test a configuration where the model is prompted to output a only sequence of dots ($\ldots$) equal to the number of characters in the equation needed to solve the problem.
This variant performs about the same as the baseline, which suggests that variable computation by itself is not the reason for the success of chain-of-thought prompting, and that there appears to be utility from expressing intermediate steps via natural language.

\textbf{Chain of thought after answer.} 
Another potential benefit of chain-of-thought prompting could simply be that such prompts allow the model to better access relevant knowledge acquired during pretraining.
Therefore, we test an alternative configuration where the chain of thought prompt is only given after the answer, isolating whether the model actually depends on the produced chain of thought to give the final answer.
This variant performs about the same as the baseline, which suggests that the sequential reasoning embodied in the chain of thought is useful for reasons beyond just activating knowledge.

\input{main_fables/main-robustness}
\subsection{Robustness of Chain of Thought}\label{subsec:robustness}

Sensitivity to exemplars is a key consideration of prompting approaches---for instance, varying the permutation of few-shot exemplars can cause the accuracy of GPT-3 on SST-2 to range from near chance (54.3\%) to near state of the art (93.4\%) \citep{zhao2021calibrate}.
In this final subsection, we evaluate robustness to chains of thought written by different annotators.
In addition to the results above, which used chains of thought written by an Annotator A, two other co-authors of this paper (Annotators B and C) independently wrote chains of thought for the same few-shot exemplars (shown in \cref{sec:appendix-alternate-annotators}).
Annotator A also wrote another chain of thought that was more concise than the original, following the style of solutions given in \citet{cobbe2021training}.\footnote{For instance, whereas original chain of thought uses several short sentences (\textit{``'There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29.''}), the concise chain of thought would read \textit{``5 * 4 = 20 new computers were added. So there are 9 + 20 = 29 new computers in the server room now''}.}

\cref{fig:bar-robustness-analysis} shows these results for \lamda{} 137B on GSM8K and MAWPS (ablation results for other datasets are given in Appendix \cref{tab:ablations-arithmetic} / \cref{tab:ablations-commonsense-symbolic}).
Although there is variance among different chain of thought annotations, as would be expected when using exemplar-based prompting \citep{le-scao-rush-2021-many,reynolds2021prompt,zhao2021calibrate}, all sets of chain of thought prompts outperform the standard baseline by a large margin. %
This result implies that successful use of chain of thought does not depend on a particular linguistic style.

To confirm that successful chain-of-thought prompting works for other sets of exemplars, we also run experiments with three sets of eight exemplars randomly sampled from the GSM8K training set, an independent source (examples in this dataset already included reasoning steps like a chain of thought).\footnote{We sample examples $\leq 60$ tokens to fit into our input context window, and also limit the examples to $\leq 2$ steps to solve for a fair comparison with the eight exemplars that we composed.}
\cref{fig:bar-robustness-analysis} shows that these prompts performed comparably with our manually written exemplars, also substantially outperforming standard prompting.

In addition to robustness to annotators, independently-written chains of thought, different exemplars, and various language models, we also find that chain-of-thought prompting for arithmetic reasoning is robust to different exemplar orders and varying numbers of exemplars (see \cref{subsec:faq-prompt-engineering}).

\section{Commonsense Reasoning}\label{sec:commonsense-reasoning}

Although chain of thought is particularly suitable for math word problems, the language-based nature of chain of thought actually makes it applicable to a broad class of commonsense reasoning problems, which involve reasoning about physical and human interactions under the presumption of general background knowledge.
Commonsense reasoning is key for interacting with the world and is still beyond the reach of current natural language understanding systems \citep{talmor2022commonsenseqa}.

\textbf{Benchmarks.}
We consider five datasets covering a diverse range of commonsense reasoning types.
The popular \textbf{CSQA} \citep{talmor-etal-2019-commonsenseqa} asks commonsense questions about the world involving complex semantics that often require prior knowledge.
\textbf{StrategyQA} \citep{geva-etal-2021-aristotle} requires models to infer a multi-hop strategy to answer questions.
We choose two specialized evaluation sets from the BIG-bench effort \citep{bigbench}: \textbf{Date} Understanding, which involves inferring a date from a given context, and \textbf{Sports} Understanding, which involves determining whether a sentence relating to sports is plausible or implausible.
Finally, the \textbf{SayCan} dataset \citep{ahn2022can} involves mapping a natural language instruction to a sequence of robot actions from a discrete set.
\cref{fig:dataset-examples} shows examples with chain of thought annotations for all datasets.

\textbf{Prompts.}
We follow the same experimental setup as the prior section. 
For CSQA and StrategyQA, we randomly selected examples from the training set and manually composed chains of thought for them to use as few-shot exemplars.
The two BIG-bench tasks do not have training sets, so we selected the first ten examples as exemplars in the evaluation set as few-shot exemplars and report numbers on the rest of the evaluation set.
For SayCan, we use six examples from the training set used in \citet{ahn2022can} and also manually composed chains of thought.

\textbf{Results.}
\cref{fig:commonsense-results} highlights these results for \palm{} (full results for \lamda{}, GPT-3, and different model scales are shown in \cref{tab:all-lm-commonsense}).
For all tasks, scaling up model size improved the performance of standard prompting; chain-of-thought prompting led to further gains, with improvements appearing to be largest for \palm{} 540B.
With chain-of-thought prompting, \palm{} 540B achieved strong performance relative to baselines, outperforming the prior state of the art on StrategyQA (75.6\% vs 69.4\%) and outperforming an unaided sports enthusiast on sports understanding (95.4\% vs 84\%).
These results demonstrate that chain-of-thought prompting can also improve performance on tasks requiring a range of commonsense reasoning abilities (though note that gain was minimal on CSQA).

\input{main_fables/main-commonsense}

\input{main_fables/main-symbolic}
\section{Symbolic Reasoning}\label{sec:symbolic-reasoning}
Our final experimental evaluation considers symbolic reasoning, which is simple for humans but potentially challenging for language models.
We show that chain-of-thought prompting not only enables language models to perform symbolic reasoning tasks that are challenging in the standard prompting setting, but also facilitates length generalization to inference-time inputs longer than those seen in the few-shot exemplars.

\paragraph{Tasks.}
We use the following two toy tasks.
\begin{itemize}[leftmargin=*,topsep=0pt]
    \itemsep0em 
    \item \textbf{Last letter concatenation.}
    This task asks the model to concatenate the last letters of words in a name (e.g., \textit{``Amy Brown''} $\rightarrow$ \textit{``yn''}). 
    It is a more challenging version of first letter concatenation, which language models can already perform without chain of thought.\footnote{We tested 10 common names using GPT-3 \texttt{davinci} and it got all but one correct.}
    We generate full names by randomly concatenating names from the top one-thousand first and last names from name census data ({\small{\url{https://namecensus.com/}}}).
    \item \textbf{Coin flip.}
    This task asks the model to answer whether a coin is still heads up after people either flip or don't flip the coin (e.g., \textit{``A coin is heads up. Phoebe flips the coin. Osvaldo does not flip the coin. Is the coin still heads up?''} $\rightarrow$ \textit{``no''}).
\end{itemize}

As the construction of these symbolic reasoning tasks is well-defined, for each task we consider an \textit{in-domain} test set for which examples had the same number of steps as the training/few-shot exemplars, as well as an \textit{out-of-domain} (OOD) test set, for which evaluation examples had more steps than those in the exemplars. 
For last letter concatenation, the model only sees exemplars of names with two words, and then performs last letter concatenation on names with 3 and 4 words.\footnote{For names of length longer than 2 words, we concatenate multiple first and last names together.}
We do the same for the number of potential flips in the coin flip task. 
Our experimental setup uses the same methods and models as in the prior two sections.
We again manually compose chains of thought for the few-shot exemplars for each task, which are given in \cref{fig:dataset-examples}.

\paragraph{Results.}
The results of these in-domain and OOD evaluations are shown in \cref{fig:symbolic-main} for \palm{}, with results for \lamda{} shown in Appendix \cref{tab:all-lm-symbolic}.
With \palm{} 540B, chain-of-thought prompting leads to almost 100\% solve rates (note that standard prompting already solves coin flip with \palm{} 540, though not for \lamda{} 137B).
Note that these in-domain evaluations are ``toy tasks'' in the sense that perfect solution structures are already provided by the chains of thought in the few-shot exemplars; all the model has to do is repeat the same steps with the new symbols in the test-time example.
And yet, small models still fail---the ability to perform abstract manipulations on unseen symbols for these three tasks only arises at the scale of 100B model parameters.

As for the OOD evaluations, standard prompting fails for both tasks.
With chain-of-thought prompting, language models achieve upward scaling curves (though performance is lower than in the in-domain setting).
Hence, chain-of-thought prompting facilitates length generalization beyond seen chains of thought for language models of sufficient scale.

\section{Discussion}\label{sec:discussion}

We have explored chain-of-thought prompting as a simple mechanism for eliciting multi-step reasoning behavior in large language models.
We first saw that chain-of-thought prompting improves performance by a large margin on arithmetic reasoning, yielding improvements that are much stronger than ablations and robust to different annotators, exemplars, and language models (\cref{sec:arithmetic-reasoning}).
Next, experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought reasoning makes it generally applicable (\cref{sec:commonsense-reasoning}).
Finally, we showed that for symbolic reasoning, chain-of-thought prompting facilitates OOD generalization to longer sequence lengths (\cref{sec:symbolic-reasoning}).
In all experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language model. 
No language models were finetuned in the process of writing this paper.

The emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme \citep{wei2022emergent}.
For many reasoning tasks where standard prompting has a flat scaling curve, chain-of-thought prompting leads to dramatically increasing scaling curves.
Chain-of-thought prompting appears to expand the set of tasks that large language models can perform successfully---in other words, our work underscores that standard prompting only provides a lower bound on the capabilities of large language models.
This observation likely raises more questions than it answers---for instance, how much more can we expect reasoning ability to improve with a further increase in model scale?
What other prompting methods might expand the range of tasks that language models can solve?

As for limitations, we first qualify that although chain of thought emulates the thought processes of human reasoners, this does not answer whether the neural network is actually ``reasoning,'' which we leave as an open question.
Second, although the cost of manually augmenting exemplars with chains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for finetuning (though this could potentially be surmounted with synthetic data generation, or zero-shot generalization).
\orange{Third, there is no guarantee of correct reasoning paths, which can lead to both correct and incorrect answers; improving factual generations of language models is an open direction for future work \citep[][\textit{inter alia}]{rashkin2021measuring,ye2022unreliability,wiegreffe2021reframing}.}
Finally, the emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in real-world applications; further research could explore how to induce reasoning in smaller models.

\section{Related Work}
This work is inspired by many research areas, which we detail in an extended related work section (\cref{sec:extended-related-work}). Here we describe two directions and associated papers that are perhaps most relevant.

The first relevant direction is using intermediate steps to solve reasoning problems. \cite{ling-etal-2017-program} pioneer the idea of using natural language rationales to solve math word problems through a series of intermediate steps. Their work is a remarkable contrast to the literature using formal languages to reason \citep{roy-2016-reasoning, chiang-chen-2019-semantically, amini-etal-2019-mathqa, chen2019neural}.  \citet{cobbe2021training} extend \citet{ling-etal-2017-program} by creating a larger dataset and using it to finetune a pretrained language model rather than training a model from scratch. In the domain of program synthesis, \cite{nye2021show} leverage language models to predict the final outputs of Python programs via first line-to-line predicting the intermediate computational results, and show that their step-by-step prediction method performs better than directly predicting the final outputs.  

Naturally, this paper also relates closely to the large body of recent work on prompting.
Since the popularization of few-shot prompting as given by \citet{brown2020language}, several general approaches have improved the prompting ability of models, such as automatically learning prompts \citep{lester-etal-2021-power} or giving models instructions describing a task \citep{wei2021finetuned,sanh2021multitask,ouyang2022training}.
Whereas these approaches improve or augment the input part of the prompt (e.g., instructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the outputs of language models with a chain of thought.

\section{Conclusions}
We have explored chain-of-thought prompting as a simple and broadly applicable method for enhancing reasoning in language models. 
Through experiments on arithmetic, symbolic, and commonsense reasoning, we find that chain-of-thought reasoning is an emergent property of model scale that allows sufficiently large language models to perform reasoning tasks that otherwise have flat scaling curves.
Broadening the range of reasoning tasks that language models can perform will hopefully inspire further work on language-based approaches to reasoning. 

\end{document}