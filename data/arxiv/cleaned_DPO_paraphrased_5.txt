\title{Direct Preference Optimization: Your Language Model is Secretly a Reward Model}

\begin{abstract}
Large unsupervised language models (LMs) acquire extensive world knowledge and some reasoning abilities, but controlling their behavior precisely is challenging due to their fully unsupervised training process. Current techniques for improving steerability involve gathering human feedback on the relative quality of model outputs and further tuning the unsupervised LM to follow these preferences, frequently using reinforcement learning from human feedback (RLHF). RLHF, however, is a complicated and sometimes unstable approach, requiring the training of a reward model to reflect human judgments and then using reinforcement learning to adapt the LM in a way that maximizes this learned reward, all while preventing the model from diverging excessively from its original behavior. In this work, we propose a novel parameterization for the reward model in RLHF that enables us to derive the associated optimal policy in closed form, allowing the standard RLHF task to be addressed with a simple classification objective. The new method, named \textit{Direct Preference Optimization} (DPO), is stable, efficient, and computationally efficient, removing the necessity for LM sampling during fine-tuning or extensive hyperparameter tuning. Empirical results demonstrate that DPO can adapt LMs to follow human preferences as well or better than existing methods. In particular, DPO surpasses PPO-based RLHF in sentiment control and matches or improves the quality of outputs in summarization and single-turn dialogue, all while being much easier to implement and train.
\end{abstract}

\section{Introduction}
Large-scale unsupervised language models (LMs), trained on extensive corpora, have demonstrated unexpected abilities~\citep{chowdhery2022palm, brown2020language, touvron2023llama, bubeck2023sparks}. Despite this, the data used for training these models originates from humans with diverse intentions, expertise, and objectives. Many of these human behaviors and skills are not necessarily those we wish to replicate; for instance, we may require our AI programming assistant to \textit{recognize} frequent coding errors to provide corrections, but when producing code, we want it to emulate the (possibly infrequent) examples of expert programming within the dataset. Likewise, while it is useful for a language model to \textit{know about} a widely held misconception—say, one believed by half the population—we clearly do not want the model to state this misconception as fact in half of the relevant responses. Essentially, it is vital to distinguish and select the \emph{preferred behaviors and answers} of a model from its broad spectrum of \textit{knowledge and competencies} to construct AI systems that are reliable, effective, and governable \citep{ouyang2022training}. Although current approaches typically align language models with human preferences via reinforcement learning (RL), we demonstrate that the RL-based objective currently in use can actually be solved exactly by optimizing a straightforward binary cross-entropy loss, thereby streamlining the process of preference learning.

\begin{figure}
    \centering
    \includegraphics[width=0.999\textwidth]{figures/diagrams/teaser.png}
    \caption{\textbf{DPO aligns models with human preferences without relying on reinforcement learning.} Traditional approaches to fine-tuning language models with human input involve first training a reward model using a collection of prompts and human comparisons between response pairs, followed by applying RL to optimize a policy with respect to the learned reward. In contrast, DPO skips RL and instead directly optimizes the policy to reflect the preferences, utilizing a straightforward classification loss and constructing an \textit{implicit} reward model whose optimal policy can be analytically derived.}
    \vspace{-2mm}
    \label{fig:teaser}
\end{figure}

Broadly speaking, current techniques encourage desired behaviors in language models by leveraging carefully selected collections of human preferences that reflect what people consider to be safe and useful. This preference learning phase follows an initial stage where the model undergoes large-scale unsupervised pre-training on extensive textual data. Although the simplest strategy for preference learning is supervised fine-tuning using high-quality human-generated responses, the most effective approaches have proven to be reinforcement learning from human (or AI) feedback (RLHF/RLAIF; \citep{christiano2017deep,bai2022constitutional}). In RLHF, a reward model is trained on data containing human preferences, and reinforcement learning is then applied to adjust a language model policy so that it generates responses that receive high rewards, while still remaining reasonably close to the pre-trained model. Despite the fact that RLHF yields models with remarkable skills in dialogue and programming tasks, its workflow is significantly more intricate than that of supervised learning, requiring the training of several language models and the inclusion of policy sampling during training, which leads to substantial computational overhead.

In this work, we demonstrate how a language model can be optimized to align with human preferences directly, without the need for explicit reward modeling or reinforcement learning. We introduce Direct Preference Optimization (DPO), an algorithm that implicitly targets the same objective as current RLHF techniques—namely, reward maximization subject to a KL-divergence penalty—but with a much simpler implementation and training process. Essentially, the DPO update boosts the relative log-likelihood of preferred responses over non-preferred ones while employing a dynamic importance weight for each example, which helps to avoid the model degradation observed with a straightforward probability ratio objective. Similar to prior methods, DPO is grounded in a theoretical preference framework (for instance, the Bradley-Terry model; \cite{bradley1952rankanalysis}) that evaluates the alignment between a reward function and empirical preference data. However, rather than utilizing the preference model to formulate a preference loss for training a reward model and subsequently optimizing a policy with that reward, DPO reparameterizes the problem so that the preference loss is expressed directly in terms of the policy. As a result, when provided with a dataset containing human preferences over model outputs, DPO can train a policy with a simple binary cross-entropy loss, yielding the optimal policy corresponding to an implicit reward function learned from preference data.

The central contribution of our work is Direct Preference Optimization (DPO), an intuitive algorithm that enables training of language models from preference data without relying on reinforcement learning. Through our experiments, we demonstrate that DPO matches or exceeds the performance of established approaches—including PPO-based RLHF—for preference-based tasks such as sentiment adjustment, summarization, and conversational modeling, utilizing language models containing up to 6B parameters.

\section{Related Work}

Self-supervised language models of increasing size have demonstrated the ability to perform certain tasks in a zero-shot setting \citep{radford2019language} or when given a few examples as prompts \citep{gpt3,megatron,chowdhery2022palm}. Nevertheless, their effectiveness on downstream applications and their alignment with user goals can be greatly enhanced by further training on collections of instructions paired with human-generated completions \citep{mishra-etal-2022-cross,sanh2022multitask,chung2022scaling,thoppilan2022lamda}. This process, known as `instruction-tuning’, empowers LLMs to handle instructions beyond those seen in the training data, boosting their overall usefulness \citep{chung2022scaling}. While instruction tuning has yielded promising results, gathering \textit{relative} human assessments of response quality is usually more feasible than curating expert-written examples. Consequently, recent research has utilized datasets reflecting human preferences to further fine-tune LLMs, which has improved their capabilities in areas like translation \citep{kreutzer-etal-2018-reliability}, summarization \citep{stiennon2022learning,ziegler2020finetuning}, creative writing \citep{ziegler2020finetuning}, and following instructions \citep{ouyang2022training,ramamurthy2023is}. Typically, these approaches begin by training a neural reward model to match the preference data—often using a model such as Bradley-Terry \citep{bradley1952rankanalysis}—and subsequently updating the language model with reinforcement learning to optimize this reward, commonly using algorithms like REINFORCE \citep{williams1992reinforce}, proximal policy optimization (PPO; \cite{schulman2017proximal}), or related variants \citep{ramamurthy2023is}. A related stream of research uses LLMs already instruction-tuned with human feedback to automatically create synthetic preference datasets for specific traits, such as safety or harmlessness \citep{bai2022constitutional}, relying on human-provided rubrics to supervise the model’s annotations in a weak manner. These approaches lie at the intersection of two research areas: the use of reinforcement learning to optimize language models for diverse objectives~\citep{Ranzato2015SequenceLT,paulus2018a,wu2018learning}, and general-purpose strategies for learning from human preference signals \citep{christiano2017deep,kupcsik2018learning}. Although optimizing for relative preferences using human judgments is conceptually appealing, applying reinforcement learning to fine-tune large language models remains a considerable challenge in practice; this work introduces a theoretically grounded alternative for optimizing with relative preferences that does not rely on RL.

Beyond linguistic applications, policy learning from preferences has been explored within both bandit and reinforcement learning frameworks, leading to the development of numerous methods. When contextual bandit algorithms utilize preferences or action rankings instead of explicit rewards, the problem is referred to as a contextual dueling bandit (CDB; \cite{yue2012karmed,dudik2015contextual}). Without access to absolute reward signals, the theory for CDBs replaces the standard optimal policy concept with the \textit{von Neumann winner}: a policy that is expected to outperform \textit{any} alternative policy at least half the time \citep{dudik2015contextual}. Notably, in CDBs, preference information is provided online, whereas, in the context of learning from human preferences, models are typically trained on a static, offline collection of preference-labeled action pairs \citep{yan2022human}. Likewise, \textit{preference-based RL} (PbRL) relies on binary preference signals produced by an unknown 'scoring' function, rather than direct rewards \citep{BusaFekete2014,ruiz2023dueling}. A variety of PbRL algorithms exist, with some capable of leveraging off-policy preference datasets. However, these approaches usually require an explicit estimation of the underlying scoring or reward function prior to policy optimization \citep{jain2013learning,BusaFekete2014,christiano2017deep,sadigh2017active,kupcsik2018learning}. In contrast, we introduce a one-step policy learning method that directly optimizes the policy to conform to preference data.

\section{Preliminaries}\label{section:prelims}

We summarize the RLHF workflow as outlined by \citeauthor{ziegler2020finetuning}, with further developments in \citep{stiennon2022learning, bai2022training, ouyang2022training}. This process generally consists of three main steps: (1) supervised fine-tuning (SFT), (2) collecting preferences and training a reward model, and (3) optimization using reinforcement learning.

\textbf{SFT}: RLHF usually starts by using supervised learning to further train a pre-trained LM on curated data specific to target tasks (such as dialogue or summarization), resulting in a model denoted as $\pisft$.

\textbf{Reward Modeling Stage}: During this stage, the SFT model is given prompts $x$ and generates answer pairs $(y_1, y_2)\sim \pisft(y \mid x)$. These answer pairs are evaluated by human annotators, who indicate a preference between the two, expressed as $y_w\succ y_l \mid x$, where $y_w$ and $y_l$ represent the chosen and not-chosen responses, respectively. The underlying preferences are assumed to result from an unknown reward model $r^*(y, x)$, which is not directly observable. To model such preferences, several methods are available, with the Bradley-Terry (BT) \cite{bradley1952rankanalysis} framework being widely used (though more general Plackett-Luce models \citep{plackett1975analysis, luce2012individual} can be incorporated if multiple ranked options are present). The BT model formalizes the human preference probability $p^*$ as:
\begin{equation}\label{eq:bradley-terry}
    p^*(y_1\succ y_2 \mid x)=\frac{\exp\left(r^*(x, y_1)\right)}{\exp\left(r^*(x, y_1)\right) + \exp\left(r^*(x, y_2)\right)}.
\end{equation}
Given a fixed comparison dataset $\mathcal{D}=\bigl\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\bigr\}_{i=1}^N$ sampled according to $p^*$, a parameterized reward model $r_{\phi}(x, y)$ can be fit by maximizing the likelihood of observed preferences. Treating this as a binary classification problem, the negative log-likelihood is:
\begin{equation}\label{eq:reward_model}
    \mathcal{L}_R(r_{\phi}, \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\bigl[\log \sigma(r_{\phi}(x, y_w)- r_{\phi}(x, y_l))\bigr]
\end{equation}
where $\sigma$ refers to the logistic sigmoid function. In large language models, $r_{\phi}(x, y)$ is usually initialized from the SFT model $\pisft(y \mid x)$, with an additional linear head attached to the last transformer layer to output a scalar reward estimate \cite{ziegler2020finetuning}. To stabilize the reward signal and reduce variance, it is common practice to normalize reward values, enforcing $\mathbb{E}_{x,y\sim \mathcal{D}}\left[r_\phi(x, y)\right] = 0$ for every $x$.

\textbf{RL Fine-Tuning Phase}: In the reinforcement learning fine-tuning stage, the reward function previously trained is employed to generate feedback for the language model. As in earlier studies~\citep{jaques2017sequence, jaques2020human}, the optimization task is formalized as
\begin{equation}\label{eq:RL}
\max_{\pi_{\theta}}  \mathbb{E}_{x\sim \mathcal{D}, y\sim \pi_{\theta}(y \mid x)}\bigl[r_{\phi}(x, y)\bigr] - \beta\mathbb{D}_{\textrm{KL}}\bigl[\pi_{\theta}(y\mid x)\mid \mid \piref(y\mid x)\bigr],
\end{equation}
where $\beta$ acts as a regularization coefficient, limiting divergence from the reference policy $\piref$, which corresponds to the starting SFT model $\pisft$. 
Typically, the language model policy $\pi_\theta$ is initialized with $\pisft$. Including the KL-divergence constraint is crucial, as it limits the model from straying too far from data distributions where the reward model remains reliable, while also preserving output diversity and avoiding mode-collapse into a small set of high-reward responses. Since text generation is discrete, the above objective is non-differentiable, so reinforcement learning methods are commonly used for optimization. The prevailing strategy~\citep{ziegler2020finetuning, stiennon2022learning, bai2022training, ouyang2022training} constructs a reward function of the form ${r(x, y) = r_{\phi}(x, y) -\beta (\log \pi_{\theta}(y\mid x) - \log \piref(y\mid x))}$, and maximizes it using PPO~\cite{schulman2017proximal}.

\section{Direct Preference Optimization}\label{sec:DPO}

Driven by the difficulties of using reinforcement learning algorithms for large-scale applications like language model fine-tuning, we aim to develop a straightforward method for policy optimization that works directly with preferences. In contrast to traditional RLHF techniques that first fit a reward function and subsequently optimize it with RL, our method utilizes a specific reward model parameterization, allowing us to determine its optimal policy analytically, without the need for an RL training procedure. 

As we will elaborate on shortly, our central idea is to exploit an analytical relationship between reward functions and their corresponding optimal policies, which allows us to reframe a loss defined over reward functions as a loss over policies. This change-of-variables technique eliminates the requirement to explicitly train a separate reward model, yet still maintains compatibility with widely-used preference models such as the Bradley-Terry model. Ultimately, the policy network embodies both the language model and the implicit reward mechanism.

\textbf{Derivation of the DPO objective.} Our starting point is the standard RL objective from prior literature, Eq.~\ref{eq:RL}, given a generic reward function $r$. As established in earlier works~\citep{peters2007reinforcement, peng2019advantage, korbak2022reinforcement, go2023aligning}, it follows easily that the solution to the KL-regularized reward maximization problem in Eq.~\ref{eq:RL} can be written as:
\begin{equation}\label{eq:op_policy}
    \pi_r(y\mid x) = \frac{1}{Z(x)}\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right),
\end{equation}%
with $Z(x) =\sum_{y}\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)$ serving as the normalization constant (partition function). The complete proof is included in Appendix \ref{app:derivation1}. Even after substituting the MLE estimate $r_\phi$ for the true reward $r^*$, computing the partition function $Z(x)$ remains computationally expensive~\citep{korbak2022reinforcement, go2023aligning}, which poses practical challenges for using this formulation. Yet, Eq.~\ref{eq:op_policy} can be algebraically rearranged to rewrite the reward function in terms of the optimal policy $\pi_r$, the reference distribution $\piref$, and the (unknown) partition function $Z(\cdot)$. In particular, by applying the logarithm to both sides of Eq.~\ref{eq:op_policy} and rearranging terms, we arrive at:
\begin{equation}\label{eq:main_eq}
    r(x,y) =\beta \log \frac{\pi_r(y\mid x)}{\piref(y\mid x)} + \beta \log Z(x).
\end{equation}
This reparameterization can be applied both to the true reward $r^*$ and the associated optimal policy $\pi^*$. Importantly, the Bradley-Terry model relies only on the difference between rewards for two responses: ${p^*(y_1 \succ y_2 \mid x) = \sigma(r^*(x, y_1) - r^*(x, y_2))}$. Plugging the expression for $r^*(x,y)$ from Eq.~\ref{eq:main_eq} into the preference model Eq.~\ref{eq:bradley-terry}, the normalization term $Z(x)$ cancels out, letting us write the preference probability solely in terms of $\pi^*$ and $\piref$. Consequently, the optimal RLHF policy $\pi^*$ in the Bradley-Terry setting adheres to:
\begin{equation}\label{eq:objective}
    p^*(y_1\succ y_2 \mid x)=\frac{1}{1 + \exp\left(\beta \log \frac{\pi^*(y_2\mid x)}{\piref(y_2\mid x)} - \beta \log \frac{\pi^*(y_1\mid x)}{\piref(y_1\mid x)}\right)}
\end{equation}
For the full derivation, refer to Appendix~\ref{app:derivation2}. While Eq.~\ref{eq:objective} is based on the Bradley-Terry approach, analogous formulations can be found for more general Plackett-Luce models~\citep{plackett1975analysis, luce2012individual}, as demonstrated in Appendix~\ref{app:plackett_luce_models}.

Having expressed the probability of human preference data using the optimal policy rather than the reward function, we can now establish a maximum likelihood objective tailored to a parameterized policy $\pi_\theta$. Similar to the reward model-based framework (see Eq.~\ref{eq:reward_model}), the policy objective is written as:
\begin{equation}\label{eq:optimum_model}
    \mathcal{L}_\text{DPO}(\pi_{\theta}; \piref) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\left[\log \sigma \left(\beta \log \frac{\pi_{\theta}(y_w\mid x)}{\piref(y_w\mid x)} - \beta \log \frac{\pi_{\theta}(y_l\mid x)}{\piref(y_l\mid x)}\right)\right].
\end{equation}
With this formulation, we are essentially learning an implicit reward through a different parameterization, wherein the optimal policy corresponds directly to $\pi_\theta$. Additionally, as this method amounts to fitting a reparameterized Bradley-Terry model, it possesses theoretical guarantees, such as consistency under appropriate assumptions regarding the distribution of preference data \cite{bong2022generalized}. Further discussion of DPO's theoretical aspects and how they relate to previous studies appears in Section~\ref{sec:theory}.

\textbf{How does the DPO update function?} To gain a mechanistic insight into DPO, it is helpful to examine the gradient of the DPO loss function $\mathcal{L}_\text{DPO}$. The gradient with respect to model parameters $\theta$ is given by:
\begin{multline*}\label{eq:gradient}
    \nabla_\theta \mathcal{L}_\text{DPO}(\pi_\theta;\piref) = \\ -\beta\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \bigg[\underbrace{\sigma(\hat{r}_\theta(x, y_l) - \hat{r}_\theta(x, y_w))}_\text{larger effect if reward ranking is inaccurate}\bigg[\underbrace{\nabla_\theta\log \pi(y_w \mid x)}_\text{encourage $y_w$} - \underbrace{\nabla_\theta\log\pi(y_l \mid x)}_\text{discourage $y_l$}\bigg]\bigg],
\end{multline*}
where $\hat{r}_\theta(x, y) = \beta \log \frac{\pi_\theta(y \mid x)}{\piref(y \mid x)}$ denotes the implicit reward as defined by the model $\pi_\theta$ and the reference $\piref$ (see Section~\ref{sec:theory} for more details). Conceptually, the gradient update for $\mathcal{L}_\text{DPO}$ pushes the model to assign higher probability to preferred completions $y_w$ and lower probability to less preferred completions $y_l$. Crucially, each data point’s influence is scaled by how much more the implicit reward function $\hat{r}_\theta$ favors the dispreferred response, modulated by $\beta$—in other words, by how much the model misorders the completions, given the regularization set by the KL term. Empirically, this form of weighting is important: removing this factor, as in a basic version of the update, can lead to model collapse (see Appendix Table~\ref{tab:unlikelihood_generations}).

\textbf{Overview of DPO.}  
The standard DPO workflow involves: 1) For each prompt $x$, generate completions $y_1, y_2 \sim \piref(\cdot \mid x)$, then annotate them with human preference labels to build an offline preferences dataset $\mathcal{D} = \{x^{(i)}, y_w^{(i)}, y_l^{(i)}\}_{i=1}^N$; and 2) train the language model $\pi_\theta$ by minimizing the DPO objective $\mathcal{L}_\text{DPO}$, given $\piref$, $\mathcal{D}$, and the chosen $\beta$.  
In real-world scenarios, it is preferable to leverage existing publicly released preference datasets instead of producing completions and collecting human feedback from scratch. Because these datasets are typically generated with $\pisft$, we set $\piref = \pisft$ when this is available. If $\pisft$ cannot be obtained, we fit $\piref$ by maximizing the likelihood over preferred completions ${(x, y_w)}$, i.e., ${\piref = \argmax_{\pi}\mathbb{E}_{x, y_w \sim \mathcal{D}}\left[\log \pi(y_w \mid x)\right]}$. This approach helps address the distribution mismatch between the true, unavailable reference distribution and the $\piref$ employed in DPO. Additional implementation specifics and hyperparameter settings are described in Appendix~\ref{app:implementation}.

\section{Theoretical Examination of DPO}
Here, we offer additional insights into the DPO approach, establish its theoretical foundations, and connect the benefits of DPO to shortcomings found in actor-critic methods employed for RLHF (including PPO~\cite{schulman2017proximal}).

\label{sec:theory}

\subsection{Your Language Model Functions as a Reward Model in Disguise}

DPO circumvents the need to explicitly fit a reward function or apply reinforcement learning, instead relying on a single maximum likelihood objective. Notice that the optimization target in Eq. \ref{eq:main_eq} matches the Bradley-Terry model, where the reward is parameterized as $r^*(x, y) = \beta \log\frac{\pi^*_\theta(y \mid x)}{\piref(y \mid x)}$. We directly optimize our parametric policy $\pi_{\theta}$, which is equivalent to optimizing the reward model in Eq. \ref{eq:reward_model} after an appropriate change of variables. In this section, we will develop the theoretical framework for this reparameterization, demonstrate that it imposes no restrictions on the family of reward models that can be learned, and prove that the optimal policy can be exactly recovered. To start, we introduce an equivalence relationship between reward functions.

\begin{definition}
Two reward functions $r(x, y)$ and $r'(x, y)$ are called equivalent if and only if there exists a function $f$ such that $r(x, y)-r'(x, y) = f(x)$.     
\end{definition}
It is straightforward to verify that this forms an equivalence relation, dividing the collection of reward functions into distinct classes. The following two lemmas can be stated:

\begin{lemma}\label{lemma:same_prefrence} Within the Plackett-Luce model, including the Bradley-Terry preference framework, any two reward functions belonging to the same category result in identical preference distributions.
\end{lemma}

\begin{lemma}\label{lemma:same_policy}
    Any two reward functions belonging to the same equivalence class yield identical optimal policies for the constrained RL problem.
\end{lemma}
The proofs are elementary and are provided in Appendix \ref{app:lemma1}. The first lemma highlights a common under-identification challenge inherent to the Plackett-Luce family of models \cite{plackett1975analysis}. Because of this ambiguity, it is standard practice to enforce additional identifiability conditions in order to obtain meaningful guarantees for the MLE solutions to Eq. \ref{eq:reward_model} \cite{bong2022generalized}. The second lemma asserts that every reward function within a given equivalence class induces the same optimal policy; therefore, for our purposes, it is sufficient to recover any reward function representative from that optimal class. We establish the following Theorem in Appendix~\ref{app:thm1}:
\begin{theorem}\label{thm:main}
    Assuming mild regularity conditions, each reward equivalence class compatible with the Plackett-Luce (and specifically the Bradley-Terry) models can be expressed in the form ${r(x, y) = \beta \log \frac{\pi(y\mid x)}{\piref(y\mid x)}}$ for a suitable model $\pi(y\mid x)$ and a fixed reference model $\piref(y \mid x)$.
\end{theorem}
\begin{sproof}
    Let $r(x, y)$ be an arbitrary reward function with its corresponding optimal policy $\pi_r(y \mid x)$, defined as in Eq. \ref{eq:op_policy}. We aim to show that a reward function from the equivalence class of $r$ admits the proposed reparameterized representation. Introduce the mapping $f$ as  
\begin{equation}
    f(r; \piref, \beta)(x, y) = r(x, y) - \beta\log\sum_{y}\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)
\end{equation}
This mapping $f$ normalizes the original reward by subtracting the log-partition function associated with $\pi_r$. Since this normalization depends solely on the context $x$, $f(r; \piref, \beta)(x, y)$ resides within the equivalence class of $r(x, y)$. Substituting $r$ with the right-hand side of Eq.~\ref{eq:main_eq} (which is valid for any reward), we find $f(r; \piref, \beta)(x, y) = \beta \log \frac{\pi_r(y\mid x)}{\piref(y\mid x)}$. Therefore, the mapping $f$ produces a canonical member of $r$'s equivalence class in the desired reparameterized form, and the generality of our reward model is preserved under this transformation.
\end{sproof}
Alternatively, Theorem~\ref{thm:main} can be interpreted as characterizing the unique reward in each equivalence class selected by the DPO reparameterization—specifically, the one for which:
\begin{equation}\label{eq:lag_p}
     \sum_{y}\underbrace{\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)}_{=\pi(y\mid x)\text{, following Thm.~\ref{thm:main} reparam.}} = 1,
\end{equation}
meaning that $\pi(y\mid x)$ constitutes a valid probability distribution (i.e., non-negative and normalized).
Furthermore, by referencing Eq.~\ref{eq:op_policy}, we observe that Eq.~\ref{eq:lag_p} precisely defines the partition function for the optimal policy corresponding to $r(x, y)$.
The central insight behind the DPO algorithm is that it is possible to introduce constraints within the otherwise under-determined Plackett-Luce (and particularly Bradley-Terry) class of preference models so that the family of reward models remains expressive, yet the optimal policy formula in Eq. \ref{eq:op_policy} becomes explicitly computable for any prompt $x$.

\subsection{Instability of Actor-Critic Algorithms}
Our framework can also be applied to analyze the instabilities commonly observed in actor-critic algorithms used within RLHF setups, including PPO. We adhere to the RLHF process and specifically examine the RL fine-tuning phase as described in Section \ref{section:prelims}. The framework allows us to relate this problem to the control as inference perspective \cite{levine2018reinforcement} applied to the constrained RL formulation in \ref{eq:RL}. Here, we use a parameterized policy $\pi_{\theta}(y\mid x)$ and aim to minimize $\mathbb{D}_{\text{KL}}[\pi_{\theta}(y|x) \mid \mid \pi^*(y\mid x)]$, with $\pi^*$ defined as the optimal policy in Eq. \ref{eq:optimum_model} based on the reward function $r_{\phi}(y, x)$. After some algebraic manipulation, this leads to the following optimization goal:
\begin{equation}\label{eq:AC}
    \max_{\pi_{\theta}}\mathbb{E}_{\pi_{\theta}(y\mid x)}\left[\underbrace{r_{\phi}(x, y) -\beta\log\sum_{y}\piref(y\mid x)\exp\left(\frac{1}{\beta}r_{\phi}(x, y)\right)}_{f(r_{\phi}, \piref, \beta)} - \underbrace{\beta\log\frac{\pi_{\theta}(y\mid x)}{\piref(y\mid x)}}_{\text{KL}}\right]
\end{equation}
This objective matches those previously utilized in works such as \citep{ziegler2020finetuning, stiennon2022learning, bai2022training, ouyang2022training}, where the DPO-equivalent reward function is employed for the $r_{\phi}$ class of rewards. In this framework, the normalization component in $f(r_{\phi}, \piref, \beta)$ can be seen as the soft value function corresponding to the reference policy $\piref$. Although this normalization does not change the optimal policy, omitting it can result in the policy gradient having large variance, which can destabilize the learning process. One approach to address this normalization is by learning a value function, though optimizing such a value function may also be challenging. In practice, earlier works have normalized the reward by referencing a human completion baseline, which effectively acts as a Monte-Carlo estimate (using a single sample) for the normalization term. On the other hand, the DPO reparameterization produces a reward function that is baseline-free and does not rely on any normalization estimates.

\section{Experiments}
In this section, we experimentally assess how well DPO can learn policies directly from preference data. We begin by exploring, within a controlled text-generation framework, the question: how effectively does DPO balance reward maximization and preservation of KL-divergence with the reference policy compared to popular preference optimization methods like PPO? Subsequently, we test DPO's scalability and effectiveness on larger models and more challenging RLHF tasks such as summarization and dialogue. Our findings indicate that with minimal hyperparameter tuning, DPO frequently matches or outperforms robust baselines, including RLHF with PPO, and surpasses strategies such as selecting the best out of $N$ sampled responses based on a trained reward model. Prior to detailing these results, we outline our experimental methodology; further specifics are provided in Appendix~\ref{app:exp_details}.

\textbf{Tasks.} Our study investigates three distinct open-ended text generation tasks. For each task, the methods aim to learn a policy using a preference dataset $\mathcal{D}=\bigl\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\bigr\}_{i=1}^N$. In \textbf{controlled sentiment generation}, $x$ serves as the beginning of a movie review sampled from the IMDb dataset \cite{maas-EtAl:2011:ACL-HLT2011}, and the objective is for the policy to generate a continuation $y$ expressing positive sentiment. To enable controlled evaluation, preference pairs are automatically constructed by using a pre-trained sentiment classifier to score generated texts, ensuring $p(\text{positive}\mid x,y_w)>p(\text{positive}\mid x,y_l)$. For SFT, GPT-2-large is further trained until convergence on IMDB train set reviews (see App~\ref{app:sentiment_details} for more information). In \textbf{summarization}, $x$ corresponds to a Reddit forum post; the task requires producing a concise summary $y$ that captures the post’s main points. As in previous research, we adopt the Reddit TL;DR dataset \citep{volske-etal-2017-tl} as well as human preference annotations collected by \citeauthor{stiennon2022learning}. Here, SFT uses a model tuned on human-authored Reddit post summaries\footnote{\url{https://huggingface.co/CarperAI/openai_summarize_tldr_sft}} within the TRLX \citep{leandro_von_werra_2023_7790115} RLHF toolkit. The preference data consists of responses to a different, but comparably trained, SFT model, as assembled by \citeauthor{stiennon2022learning}. Lastly, in the \textbf{single-turn dialogue} scenario, $x$ is a user prompt—ranging from astrophysics queries to personal advice requests. The policy’s goal is to respond with an engaging and informative $y$. For this, we utilize the Anthropic Helpful and Harmless dialogue dataset \citep{bai2022training}, which contains 170,000 human–assistant conversations, each concluding with two model-generated replies and a human-annotated preferred choice. Since a pre-trained SFT model is unavailable for this data, we construct one by fine-tuning a generic language model solely on preferred completions.

\begin{figure}
    \centering
    \includegraphics[width=0.50\textwidth]{figures/results/frontier.pdf}
    \includegraphics[width=0.49\textwidth]{figures/results/tldr_winrate_vs_temp.pdf}
    \caption{\textbf{Left.} The curve displays the trade-off between expected reward and KL divergence with the reference policy. DPO achieves the greatest expected reward across all KL levels, highlighting its optimization effectiveness. \textbf{Right.} TL;DR summarization win rates against human-authored summaries, as judged by GPT-4. DPO outperforms the best results from PPO on the summarization task and remains more stable across different sampling temperatures.}
    \vspace{-2mm}
    \label{fig:frontier-tldr-main}
\end{figure}

\textbf{Evaluation.} We employ two distinct strategies for evaluation in our experiments. To assess how well each algorithm optimizes the constrained reward maximization objective, we measure the reward-KL-divergence frontier for every algorithm in the controlled sentiment generation scenario; this is feasible since we have access to the actual reward function, given by a sentiment classifier. On the other hand, in practical scenarios where the true reward function is unavailable, we rely on the \textit{win rate} metric versus a baseline policy, utilizing GPT-4 as a stand-in for human judgment of summary quality and response helpfulness within summarization and single-turn dialogue tasks, respectively. In the summarization task, the baseline consists of the test set reference summaries; for dialogue, we use the human-preferred response from the test data. Although prior research indicates that LMs may serve as more reliable automated evaluators compared to traditional metrics \citep{Chen2023ExploringTU}, we complement our evaluation with a human study in Sec.~\ref{sec:human-judgments} to validate the use of GPT-4 for scoring. Our findings reveal a strong correlation between GPT-4 and human ratings, with human-GPT-4 agreement generally matching or surpassing inter-annotator agreement among humans.

\textbf{Methods.} Alongside DPO, we assess a range of established methods for aligning language model outputs with human preferences. For the summarization task, we apply zero-shot prompting with \textbf{GPT-J} \citep{gpt-j}, and for the dialogue task, we utilize 2-shot prompting using \textbf{Pythia-2.8B} \citep{biderman2023pythia}. We also include the \textbf{SFT} model, and \textbf{Preferred-FT}, which involves supervised fine-tuning on the selected completion $y_w$—sourced either from the SFT model (for sentiment and summarization) or from a general language model (in the case of single-turn dialogue). Another related approach is \textbf{Unlikelihood}~\citep{welleck2019neural}, which encourages the model to increase the likelihood of $y_w$ while explicitly \textit{reducing} the likelihood of $y_l$, regulated by an optional coefficient $\alpha\in[0,1]$ on the unlikelihood component. We further evaluate \textbf{PPO} \citep{schulman2017proximal}, where the reward is learned from preference annotations, as well as \textbf{PPO-GT}, an oracle method that directly leverages the true reward function in the controlled sentiment scenario. For sentiment tasks, we experiment with two PPO-GT variants: an out-of-the-box version \cite{leandro_von_werra_2023_7790115} and a custom version featuring reward normalization and improved hyperparameter tuning—these customizations are also incorporated into standard PPO with learned rewards. Lastly, we implement the \textbf{Best of $N$} baseline, which generates $N$ samples from the SFT model (or Preferred-FT for dialogue), then chooses the top response according to a reward model trained on preference data. While this method can yield strong results by separating reward modeling from PPO, it is not computationally feasible for larger $N$ since it demands generating $N$ completions for each test query.

\subsection{How well can DPO optimize the RLHF objective?}

\begin{figure}
    \centering
    \includegraphics[width=0.50\textwidth]{figures/results/dialogue_winrate_vs_temp.pdf}
    \includegraphics[width=0.49\textwidth]{figures/results/dialogue_winrate_vs_steps.pdf}
    \caption{\textbf{Left.} GPT-4 win rates on Anthropic-HH single-turn dialogue; among all methods, only DPO surpasses the performance of selected summaries on the Anthropic-HH evaluation set. \textbf{Right.} Win rates plotted for various sampling temperatures during training. DPO consistently outperforms the dataset labels across training epochs, regardless of the sampling temperature used.}
    \vspace{-2mm}
    \label{fig:dialogue-main}
\end{figure}

The KL-constrained reward maximization objective commonly employed in RLHF methods aims to maximize reward while simultaneously limiting the policy’s divergence from the reference policy. Consequently, when evaluating different algorithms, it is important to consider both the obtained reward and the KL divergence; slightly increasing the reward at the expense of a much larger KL is generally not preferable. Figure~\ref{fig:frontier-tldr-main} presents the tradeoff between reward and KL for several algorithms in the sentiment task. For each algorithm, we conduct several training runs, each using a distinct hyperparameter value to control policy conservativeness (target KL $\in\{3,6,9,12\}$ for PPO, $\beta \in \{0.05,0.1,1,5\}$, $\alpha\in\{0.05,0.1,0.5,1\}$ for unlikelihood, and different random seeds for preferred-FT), resulting in 22 total runs. At every 100 training steps until convergence, we assess the policies on a test set of prompts, measuring both the mean reward according to the true reward function and the mean sequence-level KL\footnote{Specifically, the total of the KL-divergences at each timestep.} relative to the reference policy $\text{KL}\left(\pi\mid \mid \piref\right)$. Our experiments show that DPO achieves a much more favorable frontier, obtaining the highest rewards while maintaining low KL values. This finding is particularly striking for several reasons. First, both DPO and PPO target the same objective, yet DPO is substantially more efficient, and its reward/KL tradeoff strictly outperforms PPO. Second, DPO yields a superior frontier compared to PPO, \emph{even when PPO is given access to ground truth rewards} (PPO-GT).

\subsection{Is DPO applicable to large-scale real preference datasets?}
\label{sec:dpo-real-datasets}
We now assess the fine-tuning capabilities of DPO on tasks like summarization and single-turn dialogue. For summarization, automated metrics such as ROUGE often do not correlate well with human judgments~\citep{stiennon2022learning}, and earlier studies have demonstrated that fine-tuning language models with PPO on human-labeled preferences can yield superior summaries. To compare different approaches, we generate completions on the TL;DR summarization test split and calculate the mean win rate versus reference outputs from the test set. Samples for all methods are generated using various temperatures ranging from 0.0 to 1.0, and corresponding win rates are displayed in Figure~\ref{fig:frontier-tldr-main} (right). DPO, PPO, and Preferred-FT are all fine-tuned from the same GPT-J SFT model\footnote{\url{https://huggingface.co/CarperAI/openai_summarize_tldr_sft}}. At a temperature of 0.0, DPO achieves a win rate of roughly 61\%, surpassing PPO's best performance of about 57\% at its own optimal temperature of 0.0. DPO also attains a greater peak win rate than the best-of-$N$ baseline. It’s important to highlight that the $\beta$ hyperparameter for DPO was not extensively tuned, so the method's actual effectiveness may be even higher. Additionally, DPO maintains stronger performance across different sampling temperatures compared to PPO, whose results can drop to the level of the base GPT-J model at higher temperatures. Preferred-FT yields only marginal gains over the SFT baseline. For further comparison, we conduct human evaluations of DPO and PPO in Section~\ref{sec:human-judgments}, finding that DPO outputs at temperature 0.25 are preferred in 58\% of cases over PPO samples at temperature 0.

For single-turn dialogue, we assess the various approaches on the subset of the Anthropic HH dataset \citep{bai2022training} test split that features a single round of human-assistant interaction. In the GPT-4 assessments, we use the test set’s preferred completions as ground truth to calculate win rates across different methods. Due to the absence of a standard SFT model for this benchmark, we begin with a pre-trained Pythia-2.8B model, apply Preferred-FT to fine-tune a reference model using the selected completions (ensuring the outputs stay within the model’s distribution), and subsequently train using DPO. Comparisons are also made to the best outcome among 128 Preferred-FT completions (noting that the Best of $N$ baseline saturates at 128 samples for this task; refer to Appendix Figure~\ref{fig:best-of-n}), as well as to a 2-shot prompted variant of the Pythia-2.8B base model. We observe that DPO achieves results on par with or exceeding those of the other methods when using the optimal temperature settings for each. Additionally, we test an RLHF model, trained with PPO on the Anthropic HH dataset \footnote{\url{https://huggingface.co/reciprocate/ppo_hh_pythia-6B}} and sourced from a recognized implementation \footnote{\url{https://github.com/CarperAI/trlx/tree/main/examples/hh}}, but we are unable to identify a prompting strategy or sampling temperature that outperforms the base Pythia-2.8B model. Drawing from our findings with TL;DR, and since both strategies optimize the same reward signal, we take Best of 128 as an approximate stand-in for PPO-level effectiveness. In summary, DPO stands out as the only method that both efficiently uses computation and surpasses the preferred completions in the Anthropic HH dataset, matching or outperforming the more resource-intensive Best of 128 baseline. Lastly, Figure~\ref{fig:dialogue-main} illustrates that DPO reaches optimal performance in relatively few training steps.

\subsection{Generalization to a new input distribution}

\begin{wraptable}{r}{0.375\textwidth}
    \small
    \vspace{-10mm}
    \begin{tabular}{ccc}
        \toprule
        & \multicolumn{2}{c}{\textbf{Victory percentage against ground truth}} \\
        \cmidrule(lr){2-3}
        \textbf{Algorithm} & Temp $0$ & Temp $0.25$ \\
        \midrule
        DPO & 0.36 & 0.31 \\
        PPO & 0.26 & 0.23 \\
        \bottomrule
    \end{tabular}
    \caption{GPT-4 performance (win rates) compared to ground truth summaries on out-of-distribution CNN/DailyMail article inputs.}
    \vspace{-3mm}
    \label{tab:ood}
\end{wraptable}

To additionally assess how PPO and DPO perform under distribution shifts, we tested the PPO and DPO policies obtained from our Reddit TL;DR summarization study on a new domain: the test portion of the CNN/DailyMail news article dataset \citep{nallapati-etal-2016-abstractive}, employing the optimal sampling temperatures from TL;DR (0 and 0.25). Table~\ref{tab:ood} displays the outcomes. We evaluated the GPT-4 win rate against ground-truth summaries from the datasets, utilizing the same GPT-4 (C) prompt as with Reddit TL;DR, but substituting the phrase ``forum post'' for ``news article''. On this unseen distribution, DPO still substantially outperforms PPO. These findings provide preliminary support that DPO policies can generalize at least as well as PPO policies, despite DPO lacking access to the extra unlabeled Reddit TL;DR prompts used by PPO.

\subsection{Cross-checking GPT-4 evaluations with human assessments}
\label{sec:human-judgments}
We carry out a human evaluation to assess the trustworthiness of GPT-4's scoring, utilizing results from the TL;DR summarization task and two distinct GPT-4 prompt formulations. The \textbf{GPT-4 (S)} (simple) prompt requests a judgment on which summary captures the key information from the post more effectively. The \textbf{GPT-4 (C)} (concise) prompt asks which summary is both better and more concise, included because GPT-4, under the \textbf{GPT-4 (S)} prompt, seems to favor longer and more redundant summaries compared to human preferences. Complete prompt texts are provided in Appendix~\ref{app:prompts}. We examine three pairings: the best-performing method (DPO, temp. 0.25), the worst-performing (PPO, temp. 1.0), and an intermediate method (SFT, temp. 0.25), selected to reflect a spectrum of summary qualities. Each of these is compared to PPO with temperature 0 (greedy sampling).

\begin{wraptable}{r}{0.47\textwidth}
    \centering
    \small
    \vspace{-1.5mm}
    \begin{tabular}{lccc}
    \toprule
        & \textbf{DPO} & \textbf{SFT} & \textbf{PPO-1} \\
        \cmidrule(lr){2-4}
        N respondents & 272 & 122 & 199 \\
        \midrule
        GPT-4 (S) win \% & 47 & 27 & 13 \\
        GPT-4 (C) win \% & 54 & 32 & 12 \\
        Human win \% & 58 & 43 & 17 \\
        \midrule
        GPT-4 (S)-H agree & 70 & 77 & 86 \\
        GPT-4 (C)-H agree & 67 & 79 & 85 \\
        H-H agree & 65 & - & 87 \\
        \bottomrule
    \end{tabular}
    \vspace{-1mm}
    \caption{Win rates and agreement between human and GPT-4 judgments on TL;DR summarization tasks. \textbf{Agreement levels between humans and GPT-4 are comparable to inter-human agreement.} Each match-up compares a given method’s summary with a PPO (temperature 0) summary.}
    \vspace{-5mm}
    \label{tab:human_results}
\end{wraptable}

Our analysis shows that, using both prompts, GPT-4’s assessments align with those of human evaluators at nearly the same rate that human raters agree among themselves. This indicates that GPT-4 can serve as a suitable stand-in for human judgment (multiple human ratings were only gathered for the DPO and PPO-1 matchups due to the limited pool of human evaluators). Among the two prompts, the \textbf{GPT-4 (C)} prompt yields win rates that are more in line with human preferences; thus, we rely on this prompt for the main results presented in Section~\ref{sec:dpo-real-datasets}. Further details regarding the human experiment, such as the rater interface and a list of participants, are available in Appendix~\ref{app:human-study}.

\section{Discussion}
Preference-based learning offers a robust and scalable method for developing competent, aligned language models. In this work, we introduced DPO, a straightforward approach for training language models using preferences, bypassing the need for reinforcement learning. Instead of reformulating the preference learning task to fit within a conventional RL framework and employing standard RL techniques, DPO establishes a correspondence between reward functions and language model policies, which allows direct optimization towards human preferences. This is accomplished using a basic cross-entropy loss, entirely sidestepping reinforcement learning and without compromising generality. Even with minimal hyperparameter adjustment, DPO matches or surpasses the performance of current RLHF techniques, such as those utilizing PPO; as a result, DPO significantly lowers the barriers to training language models based on human preferences.

\textbf{Limitations \& Future Work.} Our findings highlight multiple avenues for further investigation. For instance, how does the DPO policy's ability to generalize to out-of-distribution data compare to that of approaches trained with explicit reward functions? Preliminary observations indicate DPO policies may generalize on par with PPO-based models, though a broader analysis is necessary. Another question is whether self-labeling during DPO training can leverage unlabeled prompts as effectively. Additionally, it remains unclear how reward over-optimization appears within the direct preference optimization paradigm, and whether the slight drop in performance shown in Figure~\ref{fig:dialogue-main}-right is an example of this effect. We have only tested models with up to 6B parameters; scaling DPO to much larger, cutting-edge models represents a promising path for upcoming research. In terms of evaluation, we observed that win rates assessed by GPT-4 can depend on the prompt phrasing, suggesting future work should address methods for reliably eliciting high-quality judgments from automated evaluators. Lastly, there is significant potential to apply DPO beyond language modeling from human preferences, such as adapting it to train generative models across other data modalities.

\end{document}