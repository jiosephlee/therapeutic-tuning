\title{Direct Preference Optimization: Your Language Model is Secretly a Reward Model}

\begin{abstract}
Although large unsupervised language models (LMs) acquire extensive world knowledge and exhibit some reasoning capabilities, exerting fine-grained control over their outputs remains challenging, mainly because their training is entirely unsupervised. 
Current approaches to improve this controllability gather human judgments comparing the quality of model outputs and then adapt the LM to match these preferences, typically using reinforcement learning from human feedback (RLHF).
Yet, RLHF involves a complex and sometimes unstable process, which starts by learning a reward model from human preference data, and subsequently uses reinforcement learning to optimize the LM for this reward while avoiding excessive divergence from the original model.
In this work, we propose a novel reward model parameterization for RLHF that enables direct extraction of the associated optimal policy in closed form, thereby allowing the conventional RLHF problem to be addressed with a simple classification loss function.
Our resulting method, termed \textit{Direct Preference Optimization} (DPO), offers stability, strong empirical results, and efficient computation by removing the necessity for language model sampling during training and reducing the demand for hyperparameter tuning.
Experimental results indicate that DPO tunes LMs to align with human preferences as effectively, or more so, than existing approaches. In particular, DPO outperforms PPO-based RLHF for sentiment control, and matches or exceeds response quality in tasks such as summarization and single-turn dialogue, all while being much easier to implement and train.
\end{abstract}

\section{Introduction}
Large unsupervised language models (LMs) trained on extensive corpora develop unexpected capabilities~\citep{chowdhery2022palm, brown2020language, touvron2023llama, bubeck2023sparks}. Despite their impressive abilities, these models are built from data produced by humans with diverse intentions, objectives, and competencies. Not all of these human traits are desirable for the model to replicate; for instance, although it is useful for an AI coding assistant to \textit{recognize} common programming errors to correct them, we still prefer the model to generate code that reflects the (sometimes infrequent) high-standard coding proficiency present in its training corpus. Likewise, while we might want a language model to \textit{know} about a widespread misconception believed by half the population, it is certainly undesirable for the model to assert this misconception as truth in half of the related queries! Put differently, it is vital to differentiate the model's \emph{preferred answers and conduct} from the broader \textit{range of knowledge and skills} it possesses to ensure AI systems are reliable, effective, and manageable~\citep{ouyang2022training}. Although most current approaches direct LMs toward aligning with human preferences through reinforcement learning (RL), we demonstrate that the RL-driven objective commonly employed can actually be solved exactly with a straightforward binary cross-entropy objective, significantly streamlining the preference optimization process.

\begin{figure}
    \centering
    \includegraphics[width=0.999\textwidth]{figures/diagrams/teaser.png}
    \caption{\textbf{DPO aligns with human preferences without relying on reinforcement learning.} Conventional approaches for fine-tuning language models with human feedback involve first training a reward model on a collection of prompts and preference data comparing response pairs, then employing RL to discover a policy that optimizes the resulting reward signal. In contrast, DPO sidesteps RL by directly targeting the policy that most closely aligns with the preferences, using a straightforward classification loss. This process fits an \textit{implicit} reward function, from which the optimal policy can be analytically derived.}
    \vspace{-2mm}
    \label{fig:teaser}
\end{figure}

Broadly speaking, current techniques teach language models desired behaviors by utilizing carefully selected human preference datasets that reflect behaviors regarded as safe and beneficial by people. This process of preference learning follows an initial phase of extensive unsupervised pre-training on a massive corpus of text. Although the most direct method involves supervised fine-tuning on exemplary human-provided responses, the most effective family of approaches centers on reinforcement learning from human or AI feedback (RLHF/RLAIF; \citep{christiano2017deep,bai2022constitutional}). RLHF strategies construct a reward model based on collected human preferences, then apply reinforcement learning to train a language model policy to generate outputs rated highly by the reward model, while maintaining close alignment with the original pre-trained model. Despite the remarkable conversational and coding proficiency achieved by RLHF models, the overall RLHF workflow is substantially more intricate than standard supervised approaches, as it requires training multiple language models and repeatedly sampling from the policy during training—resulting in notable computational expense.

In this work, we present a method to directly tune a language model to match human preferences, bypassing both explicit reward modeling and reinforcement learning. We introduce Direct Preference Optimization (DPO), an approach that implicitly targets the same goal as standard RLHF techniques—namely, maximizing reward under a KL-divergence constraint—but is notably easier to implement and train. At a high level, DPO modifies the log probability ratio between favored and unfavored outputs, utilizing a dynamic, per-sample importance weighting scheme that avoids the model collapse seen with straightforward probability ratio objectives. Similar to prior methods, DPO employs a theoretical preference model (such as the Bradley-Terry model; \cite{bradley1952rankanalysis}) to assess how effectively a reward function reflects observed preference data. Yet, rather than leveraging the preference model to define a loss for training a reward function and then separately training a policy to optimize that reward, DPO applies a variable substitution to express the preference loss directly in terms of the policy. Using a set of human preference annotations for model outputs, DPO can thus optimize the model policy by minimizing a binary cross-entropy loss, yielding an optimal policy for an implicit reward function inferred from the preference data.

The primary contribution of our work is Direct Preference Optimization (DPO), a straightforward approach that enables training language models from preferences without relying on reinforcement learning. Through our experiments, we demonstrate that DPO matches or exceeds the effectiveness of established techniques, such as PPO-based RLHF, for preference-based learning in applications like sentiment control, summarization, and conversation, utilizing models containing up to 6B parameters.

\section{Related Work}

Self-supervised language models with increasing capacity have demonstrated the ability to solve certain tasks without prior examples (zero-shot) \citep{radford2019language} or by utilizing a handful of examples within prompts (few-shot) \citep{gpt3,megatron,chowdhery2022palm}. Nevertheless, further fine-tuning these models on corpora comprised of instructions and human-generated responses greatly enhances their performance on downstream applications and their alignment with user intentions \citep{mishra-etal-2022-cross,sanh2022multitask,chung2022scaling,thoppilan2022lamda}. This process, known as `instruction-tuning,' allows large language models (LLMs) to better generalize to unseen instructions outside the original training distribution, thereby making them more broadly useful \citep{chung2022scaling}. 

Although instruction tuning has proven effective, collecting \textit{relative} human judgments comparing response quality is typically less labor-intensive than gathering expert-annotated demonstrations. As a result, subsequent research has adapted LLMs using datasets that capture human preferences, yielding improvements in tasks such as translation \citep{kreutzer-etal-2018-reliability}, summarization \citep{stiennon2022learning,ziegler2020finetuning}, narrative generation \citep{ziegler2020finetuning}, and instruction adherence \citep{ouyang2022training,ramamurthy2023is}. These approaches generally involve first training a reward model, often based on preference aggregation methods like the Bradley-Terry model \citep{bradley1952rankanalysis}, to interpret human preference data, and then applying reinforcement learning methods—including REINFORCE \citep{williams1992reinforce}, proximal policy optimization (PPO; \cite{schulman2017proximal}), or related techniques \citep{ramamurthy2023is}—to further fine-tune the language model for higher reward. 

A related research direction employs LLMs, already instruction-tuned via human feedback, to synthesize additional preference data focusing on specific criteria such as safety or harmlessness \citep{bai2022constitutional}. In these cases, human oversight is limited to providing textual guidelines used to score model outputs. This synthesis represents a merging of two lines of work: one line focuses on applying reinforcement learning to language model training for diverse objectives~\citep{Ranzato2015SequenceLT,paulus2018a,wu2018learning}, while the other addresses more general approaches to learning from human preferences \citep{christiano2017deep,kupcsik2018learning}. 

While optimizing language models using relative human feedback is promising, applying reinforcement learning to fine-tune large language models presents significant practical hurdles; this work introduces a principled method for optimizing relative preferences without relying on reinforcement learning.

Beyond language-related tasks, learning policies from preferences has been examined in both bandit and reinforcement learning frameworks, with numerous strategies introduced. When contextual bandits are trained using preferences or action rankings rather than reward signals, the problem is referred to as the contextual dueling bandit (CDB; \cite{yue2012karmed,dudik2015contextual}). Without explicit rewards, the theoretical framework for CDBs replaces the standard notion of an optimal policy with the concept of a \textit{von Neumann winner}: a policy whose expected probability of outperforming \textit{any} other policy is no less than 50\% \citep{dudik2015contextual}. Nevertheless, CDBs assume that preference feedback is available in an online manner, while preference learning from humans usually involves training on a static, offline collection of preference-annotated action pairs \citep{yan2022human}. In a similar vein, \textit{preference-based RL} (PbRL) relies on binary preferences derived from an \textit{unknown} ‘scoring’ function, rather than direct reward values \citep{BusaFekete2014,ruiz2023dueling}. PbRL encompasses a variety of algorithms, including those capable of leveraging off-policy preference data, but these often begin by modeling the hidden scoring (or reward) function before optimizing against it \citep{jain2013learning,BusaFekete2014,christiano2017deep,sadigh2017active,kupcsik2018learning}. In contrast, we introduce a one-step policy learning method that directly trains a policy to align with the given preferences.

\section{Preliminaries}\label{section:prelims}

We summarize the RLHF framework described by \citeauthor{ziegler2020finetuning}, and further developed in \citep{stiennon2022learning, bai2022training, ouyang2022training}. This approach typically consists of three main stages: (1) supervised fine-tuning (SFT); (2) gathering preferences and training a reward model; and (3) optimizing using reinforcement learning.

\textbf{SFT}: In RLHF, the process usually starts with supervised fine-tuning of a pre-trained language model on curated data relevant to the target application (such as dialogue or summarization), resulting in the model $\pisft$.

\textbf{Reward Modelling Stage}: During the second stage, the SFT model is given prompts $x$ to generate pairs of responses $(y_1, y_2)\sim \pisft(y \mid x)$. These pairs are shown to human annotators, who select a preferred response, indicated as $y_w\succ y_l \mid x$, where $y_w$ is the favored answer and $y_l$ is the less favored from $(y_1, y_2)$. It is presumed that these preferences arise from an underlying but unknown reward function $r^*(y, x)$. Several techniques exist for modeling these preferences, with the Bradley-Terry (BT) model \cite{bradley1952rankanalysis} being widely used (although more general models like Plackett-Luce \citep{plackett1975analysis, luce2012individual} can also be employed if multiple rankings are available). In the BT model, the human preference probability $p^*$ is formulated as:
\begin{equation}\label{eq:bradley-terry}
    p^*(y_1\succ y_2 \mid x)=\frac{\exp\left(r^*(x, y_1)\right)}{\exp\left(r^*(x, y_1)\right) + \exp\left(r^*(x, y_2)\right)}.
\end{equation}
Given a fixed dataset of preference comparisons $\mathcal{D}=\bigl\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\bigr\}_{i=1}^N$ sampled according to $p^*$, a reward model $r_{\phi}(x, y)$ can be defined and its parameters learned via maximum likelihood estimation. Viewing this as a binary classification problem, the loss is given by the negative log-likelihood:
\begin{equation}\label{eq:reward_model}
    \mathcal{L}_R(r_{\phi}, \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\bigl[\log \sigma(r_{\phi}(x, y_w)- r_{\phi}(x, y_l))\bigr]
\end{equation}
where $\sigma$ denotes the logistic sigmoid. For language models, $r_{\phi}(x, y)$ is commonly initialized from the SFT model $\pisft(y \mid x)$ and augmented with a linear head atop the final transformer layer to yield a single reward scalar \cite{ziegler2020finetuning}. To promote a reward function with reduced variance, earlier works normalize rewards to satisfy $\mathbb{E}_{x,y\sim \mathcal{D}}\left[r_\phi(x, y)\right] = 0$ for all $x$.

\textbf{RL Fine-Tuning Step}: In the RL fine-tuning stage, the previously trained reward function is leveraged to give feedback to the language model. Building on earlier studies~\citep{jaques2017sequence, jaques2020human}, the training objective is defined as
\begin{equation}\label{eq:RL}
\max_{\pi_{\theta}}  \mathbb{E}_{x\sim \mathcal{D},\, y\sim \pi_{\theta}(y \mid x)}\Big[r_{\phi}(x, y)\Big] - \beta\mathbb{D}_{\textrm{KL}}\left[\pi_{\theta}(y\mid x)\, \|\, \piref(y\mid x)\right],
\end{equation}
where $\beta$ regulates how much the policy can diverge from a reference policy $\piref$, which corresponds to the initial SFT model $\pisft$. Generally, the policy $\pi_\theta$ for the language model is initialized using $\pisft$. Including this constraint is crucial, as it keeps the model from straying too far from the data distribution that the reward model was trained on, preserves diversity in outputs, and helps avoid collapsing to a few high-reward responses. Since text generation is inherently discrete, the objective above is non-differentiable and is commonly optimized through reinforcement learning methods. The prevalent strategy~\citep{ziegler2020finetuning, stiennon2022learning, bai2022training, ouyang2022training} formulates the reward as ${r(x, y) = r_{\phi}(x, y) -\beta (\log \pi_{\theta}(y\mid x) - \log \piref(y\mid x))}$, and maximizes it using PPO \cite{schulman2017proximal}.

\section{Direct Preference Optimization}\label{sec:DPO}

Inspired by the difficulties in deploying reinforcement learning techniques to large-scale tasks like language model fine-tuning, we aim to establish a straightforward method for policy optimization that operates directly on preference data. In contrast to traditional RLHF strategies that first train a reward model and then perform reinforcement learning, our method utilizes a specific form of reward model parameterization, allowing us to recover the optimal policy analytically, eliminating the need for a reinforcement learning loop. 

As we will elaborate shortly, our main insight lies in exploiting a closed-form correspondence between reward functions and their optimal policies, allowing us to reframe the loss on reward functions as an equivalent loss on policies. This variable substitution method sidesteps the need to train an explicit, separate reward model, yet still accommodates prevailing models of human preference, such as the Bradley-Terry framework. Effectively, the policy network jointly embodies both the language model and the underlying (implicit) reward structure.

\textbf{Derivation of the DPO objective.} We begin with the standard RL objective, as used in previous work, given in Eq.~\ref{eq:RL}, with a general reward function $r$. As shown in prior research~\citep{peters2007reinforcement, peng2019advantage, korbak2022reinforcement, go2023aligning}, the KL-regularized reward maximization problem in Eq.~\ref{eq:RL} yields an optimal policy of the form:
\begin{equation}\label{eq:op_policy}
    \pi_r(y\mid x) = \frac{1}{Z(x)}\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right),
\end{equation}
where $Z(x) = \sum_{y} \piref(y\mid x) \exp\left(\frac{1}{\beta} r(x, y)\right)$ acts as the normalization factor. Full details are available in Appendix \ref{app:derivation1}. Even when substituting the maximum likelihood estimate $r_{\phi}$ for the true reward $r^*$, computing the partition function $Z(x)$ remains computationally intensive~\citep{korbak2022reinforcement, go2023aligning}, making this form unwieldy for practical use. However, Eq.~\ref{eq:op_policy} can be rearranged to re-express the reward function in relation to its optimal policy $\pi_r$, the reference policy $\piref$, and the unknown normalization term $Z(\cdot)$. By applying the logarithm to both sides of Eq.~\ref{eq:op_policy} and simplifying, we derive:
\begin{equation}\label{eq:main_eq}
    r(x,y) = \beta \log \frac{\pi_r(y\mid x)}{\piref(y\mid x)} + \beta \log Z(x).
\end{equation}
This transformation can be used for the true reward $r^*$ and its associated optimal policy $\pi^*$. Importantly, the Bradley-Terry model only depends on reward differences between two outputs, i.e., ${p^*(y_1 \succ y_2 \mid x) = \sigma(r^*(x, y_1) - r^*(x, y_2))}$. Plugging the above reparametrization (Eq.~\ref{eq:main_eq}) for $r^*(x, y)$ into the preference model (Eq.~\ref{eq:bradley-terry}), the partition function $Z(x)$ cancels out, leaving the human preference probability dependent solely on $\pi^*$ and $\piref$. As a result, the optimal RLHF policy $\pi^*$ under the Bradley-Terry assumption satisfies the following:
\begin{equation}\label{eq:objective}
    p^*(y_1\succ y_2 \mid x)=\frac{1}{1 + \exp\left(\beta \log \frac{\pi^*(y_2\mid x)}{\piref(y_2\mid x)} - \beta \log \frac{\pi^*(y_1\mid x)}{\piref(y_1\mid x)}\right)}
\end{equation}
See Appendix~\ref{app:derivation2} for the complete derivation. Although Eq.~\ref{eq:objective} employs the Bradley-Terry preference model, the same reasoning extends to more general Plackett-Luce frameworks~\citep{plackett1975analysis, luce2012individual}, as detailed in Appendix~\ref{app:plackett_luce_models}.

With the probability of human preferences now expressed in terms of the optimal policy instead of the reward function, we can define a maximum likelihood objective for a parameterized policy $\pi_\theta$. In parallel with the approach for reward modeling (see Eq.~\ref{eq:reward_model}), our objective for the policy is given by:
\begin{equation}\label{eq:optimum_model}
    \mathcal{L}_\text{DPO}(\pi_{\theta}; \piref) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\left[\log \sigma \left(\beta \log \frac{\pi_{\theta}(y_w\mid x)}{\piref(y_w\mid x)} - \beta \log \frac{\pi_{\theta}(y_l\mid x)}{\piref(y_l\mid x)}\right)\right].
\end{equation}
In this formulation, we learn an implicit reward through an alternative parameterization, where the optimal policy corresponds directly to $\pi_\theta$. Additionally, because our method is mathematically equivalent to fitting a reparametrized Bradley-Terry model, it inherits certain theoretical guarantees, such as consistency under appropriate assumptions on the distribution of preference data \cite{bong2022generalized}. Section~\ref{sec:theory} explores further theoretical aspects of DPO and its connections to related research.

\textbf{What is the effect of the DPO update?} To gain a mechanistic perspective of DPO, it helps to examine the gradient of its loss function $\mathcal{L}_\text{DPO}$. The gradient with respect to the model parameters $\theta$ is given by:
\begin{multline*}\label{eq:gradient}
    \nabla_\theta \mathcal{L}_\text{DPO}(\pi_\theta;\piref) = \\ -\beta\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[\underbrace{\sigma(\hat{r}_\theta(x, y_l) - \hat{r}_\theta (x, y_w))}_\text{emphasizes misranked rewards}\left[\underbrace{\nabla_\theta\log \pi(y_w \mid x)}_\text{promote $y_w$} - \underbrace{\nabla_\theta\log\pi(y_l \mid x)}_\text{penalize $y_l$}\right]\right],
\end{multline*}
Here, $\hat{r}_\theta(x, y) = \beta \log \frac{\pi_\theta(y \mid x)}{\piref(y \mid x)}$ represents the reward inferred from the language model $\pi_\theta$ in comparison to the reference model $\piref$ (discussed further in Section~\ref{sec:theory}). Conceptually, the loss gradient for $\mathcal{L}_\text{DPO}$ encourages the model to assign higher probabilities to preferred completions $y_w$ and reduce the probabilities of less-preferred ones $y_l$. The influence of each example depends on how much the implicit reward model $\hat{r}_\theta$ mistakenly ranks dispreferred outputs above the preferred ones, adjusted by the KL constraint strength $\beta$. This weighting is essential; as shown in our experiments, omitting this factor can lead to model collapse (see Appendix Table~\ref{tab:unlikelihood_generations}), highlighting its significance.

\textbf{Overview of DPO.}  
The standard DPO workflow includes: 1) For each prompt $x$, draw completions $y_1, y_2 \sim \piref(\cdot \mid x)$, then annotate them with human preference labels to create a static preference dataset $\mathcal{D} = \{x^{(i)}, y_w^{(i)}, y_l^{(i)}\}_{i=1}^N$; 2) train the language model $\pi_\theta$ by minimizing $\mathcal{L}_\text{DPO}$ based on $\piref$, $\mathcal{D}$, and a chosen $\beta$.  
In most scenarios, it is preferable to use existing public preference datasets rather than collect fresh human feedback and samples. If these datasets were created using $\pisft$, we set $\piref = \pisft$ by default. If $\pisft$ is missing, we instead estimate $\piref$ by maximizing the likelihood of preferred completions $(x, y_w)$: specifically, ${\piref = \argmax_{\pi}\mathbb{E}_{x, y_w \sim \mathcal{D}}\left[\log \pi(y_w \mid x)\right]}$. This method reduces the mismatch between the unavailable true reference distribution and the proxy $\piref$ adopted in DPO. For more implementation specifics and hyperparameter settings, see Appendix~\ref{app:implementation}.

\section{Theoretical Examination of DPO}
Here, we offer a deeper analysis of the DPO technique, supply theoretical justification, and connect the benefits of DPO to the shortcomings found in actor-critic algorithms utilized in RLHF, such as PPO~\cite{schulman2017proximal}.

\label{sec:theory}

\subsection{Your Language Model Functions as a Hidden Reward Model}

DPO manages to avoid both training an explicit reward model and running RL by instead optimizing a single maximum likelihood objective. Observe that the objective in Eq. \ref{eq:main_eq} is mathematically identical to a Bradley-Terry model that uses the reward parameterization $r^*(x, y) = \beta \log\frac{\pi^*_\theta(y \mid x)}{\piref(y \mid x)}$. Here, our task is to optimize the parametric policy $\pi_{\theta}$, which, under an appropriate variable substitution, is equivalent to optimizing the reward model as in Eq. \ref{eq:reward_model}. In this section, we will lay out the theoretical foundations of this reparameterization, demonstrate that it imposes no restrictions on the set of reward models that can be learned, and show that it enables exact recovery of the optimal policy. We begin by introducing an equivalence relation for reward functions.

\begin{definition}
Two reward functions $r(x, y)$ and $r'(x, y)$ are considered equivalent if and only if there exists a function $f$ such that $r(x, y) - r'(x, y) = f(x)$.
\end{definition}
It is straightforward to verify that this forms an equivalence relation, thereby dividing the collection of reward functions into distinct classes. The following two lemmas can be stated:

\begin{lemma}\label{lemma:same_prefrence} Within the Plackett-Luce model—and specifically the Bradley-Terry framework—any two reward functions belonging to the same class generate identical preference distributions.
\end{lemma}

\begin{lemma}\label{lemma:same_policy}
    If two reward functions belong to the same equivalence class, they will produce identical optimal policies within the constrained RL framework.
\end{lemma}
The proofs are direct and can be found in Appendix \ref{app:lemma1}. The first lemma highlights a well-known ambiguity present in the Plackett-Luce model family \cite{plackett1975analysis}. This ambiguity, or lack of identifiability, often requires us to introduce additional constraints to guarantee meaningful maximum likelihood estimation in Eq. \ref{eq:reward_model} \cite{bong2022generalized}. The second lemma asserts that every reward function from a given equivalence class leads to the same optimal policy; consequently, for our main objective, we focus on identifying any single representative reward function from the optimal class. The following theorem, which we prove in Appendix~\ref{app:thm1}, formalizes this:

\begin{theorem}\label{thm:main}
    Given mild conditions, any reward class compatible with Plackett-Luce (and specifically, Bradley-Terry) models can be characterized via the reparameterization ${r(x, y) = \beta \log \frac{\pi(y\mid x)}{\piref(y\mid x)}}$ for some model $\pi(y\mid x)$ and a specified reference model $\piref(y \mid x)$.
\end{theorem}
\begin{sproof}
    Take an arbitrary reward function $r(x, y)$, which yields the optimal policy $\pi_r(y \mid x)$ as defined in Eq. \ref{eq:op_policy}. We aim to show that a reward function from $r$'s equivalence class can be expressed using the above reparameterization. Define the projection $f$ by  
\begin{equation}
    f(r; \piref, \beta)(x, y) = r(x, y) - \beta\log\sum_{y}\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)
\end{equation}
This operator $f$ normalizes the reward by subtracting the log-partition function over $y$ for the given $x$. Because the normalization is solely a function of $x$, $f(r; \piref, \beta)(x, y)$ remains in the same equivalence class as $r(x, y)$. Now, substituting $r$ from the right-hand side of Eq.~\ref{eq:main_eq} (valid for any reward function), we have $f(r; \piref, \beta)(x, y) = \beta \log \frac{\pi_r(y\mid x)}{\piref(y\mid x)}$. Thus, $f$ generates a representative in $r$'s equivalence class with the targeted structure, indicating that adopting this reparameterization does not limit the generality of our reward model.
\end{sproof}

Alternatively, Theorem~\ref{thm:main} can be interpreted as uniquely pinpointing which reward function, within an equivalence class, the DPO reparameterization yields—namely, the reward function for which:
\begin{equation}\label{eq:lag_p}
     \sum_{y}\underbrace{\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)}_{=\pi(y\mid x)\text{, by Theorem~\ref{thm:main} reparam.}} = 1,
\end{equation}
meaning $\pi(y\mid x)$ defines a valid probability distribution (all probabilities are non-negative and sum to 1).
Looking back at Eq.~\ref{eq:op_policy}, we see that Eq.~\ref{eq:lag_p} corresponds to the partition function of the optimal policy generated by the reward function $r(x, y)$. The central insight behind the DPO algorithm is that we can place suitable constraints on the under-determined Plackett-Luce (and Bradley-Terry) preference models to retain the expressiveness of the reward model class, while ensuring that the optimal policy in Eq. \ref{eq:op_policy} remains analytically computable for every context $x$.

\subsection{Instability of Actor-Critic Algorithms}
Our framework also allows us to analyze why traditional actor-critic methods, like PPO—commonly employed for RLHF—can suffer from instability. We adhere to the RLHF workflow, concentrating on the RL fine-tuning stage described in Section \ref{section:prelims}. By leveraging the control-as-inference perspective \cite{levine2018reinforcement}, we relate our constrained RL setting (see \ref{eq:RL}) to this framework. Assuming a parameterized policy $\pi_{\theta}(y\mid x)$, we aim to minimize $\mathbb{D}_{\text{KL}}[\pi_{\theta}(y|x)\|\pi^*(y\mid x)]$, where $\pi^*$ is the optimal solution from Eq. \ref{eq:optimum_model} arising from the reward $r_{\phi}(y, x)$. Through some algebraic manipulation, this leads us to the following optimization target:
\begin{equation}\label{eq:AC}
    \max_{\pi_{\theta}}\mathbb{E}_{\pi_{\theta}(y\mid x)}\left[\underbrace{r_{\phi}(x, y) -\beta\log\sum_{y}\piref(y\mid x)\exp\left(\frac{1}{\beta}r_{\phi}(x, y)\right)}_{f(r_{\phi}, \piref, \beta)} - \underbrace{\beta\log\frac{\pi_{\theta}(y\mid x)}{\piref(y\mid x)}}_{\text{KL}}\right]
\end{equation}
This is equivalent to the objective maximized in earlier works \citep{ziegler2020finetuning, stiennon2022learning, bai2022training, ouyang2022training} that use the DPO-equivalent reward with the reward function $r_{\phi}$. In this formulation, the normalization component inside $f(r_{\phi}, \piref, \beta)$ can be seen as the soft value function with respect to the reference policy $\piref$. Although this normalization does not alter the optimal policy, omitting it can greatly increase the variance of policy gradient estimates, thus destabilizing training. This normalization can be approximated with a learned value function, but such an approach can itself be tricky to tune. Alternatively, previous studies have adopted reward normalization based on a human-completion baseline, which acts as a single-sample Monte Carlo approximation for this normalization term. The DPO parameterization, however, produces a reward formulation that avoids the need for these baselines altogether.

\section{Experiments}
Here, we empirically investigate DPO's capability to train policies using preference data alone. To begin, in a controlled text-generation environment, we examine how well DPO balances reward optimization and KL-divergence minimization with the reference policy, in comparison to established preference-based learning methods like PPO. We then assess DPO on larger-scale models and more challenging RLHF problems, such as dialogue and summarization. Our findings indicate that, with minimal hyperparameter adjustment, DPO generally matches or exceeds the performance of robust baselines such as RLHF with PPO, as well as surpassing approaches that select the best of $N$ generated samples using a learned reward. Prior to presenting these findings, we outline our experimental methodology; further specifics can be found in Appendix~\ref{app:exp_details}.

\textbf{Tasks.} Our study investigates three distinct open-ended text generation tasks. Across all tasks, algorithms are trained to optimize a policy using a dataset of preference comparisons $\mathcal{D}=\bigl\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\bigr\}_{i=1}^N$. In \textbf{controlled sentiment generation}, $x$ represents an initial snippet from a movie review sourced from the IMDb dataset \cite{maas-EtAl:2011:ACL-HLT2011}, and the objective is to generate $y$ exhibiting positive sentiment. For this setting, we synthetically construct preference pairs for outputs using a pre-trained sentiment classifier, selecting pairs where $p(\text{positive}\mid x,y_w)>p(\text{positive}\mid x,y_l)$ for evaluation control. For SFT, GPT-2-large is fine-tuned to convergence on the IMDB dataset's training reviews (see App~\ref{app:sentiment_details} for more information). In the \textbf{summarization} task, $x$ is a Reddit forum post, with the policy tasked to produce a summary $y$ covering the main points. We follow established approaches, using the Reddit TL;DR summarization corpus \citep{volske-etal-2017-tl} together with human preference annotations from \citeauthor{stiennon2022learning}. The SFT baseline is trained on summaries written by humans\footnote{\url{https://huggingface.co/CarperAI/openai_summarize_tldr_sft}}, and the TRLX framework \citep{leandro_von_werra_2023_7790115} is used for RLHF training. The human preference data provided by \citeauthor{stiennon2022learning} is based on generations from a different, though similarly-trained, SFT model. Lastly, for \textbf{single-turn dialogue}, $x$ stands for a user prompt, which might range from scientific questions to personal advice requests. Here, the model must generate a helpful and engaging reply $y$; we leverage the Anthropic Helpful and Harmless dialogue dataset \citep{bai2022training}, which consists of 170k human-AI conversation transcripts. Each dialogue ends with two candidate responses from a large but unspecified language model, along with a label indicating the human-preferred answer. Since no pre-trained SFT model exists for this task, we construct the SFT baseline by fine-tuning a general-purpose language model only on preferred completions.

\begin{figure}
    \centering
    \includegraphics[width=0.50\textwidth]{figures/results/frontier.pdf}
    \includegraphics[width=0.49\textwidth]{figures/results/tldr_winrate_vs_temp.pdf}
    \caption{\textbf{Left.} The tradeoff curve between expected reward and KL divergence from the reference policy is shown. DPO attains the highest expected reward across all levels of KL, highlighting its optimization effectiveness. \textbf{Right.} TL;DR summarization win rates compared to human-written summaries, assessed with GPT-4. DPO surpasses PPO's optimal summarization performance, while also displaying greater resilience to variations in sampling temperature.}
    \vspace{-2mm}
    \label{fig:frontier-tldr-main}
\end{figure}

\textbf{Evaluation.} We employ two distinct methods for evaluation in our experiments. To assess how well each algorithm optimizes the constrained reward maximization goal, we use a controlled sentiment generation setting where we measure each algorithm by its trade-off curve between attained reward and KL-divergence relative to the reference policy; this is possible since we have access to the true reward function (a sentiment classifier). In practical scenarios, however, the true reward function is unavailable; thus, we measure performance by the \textit{win rate} against a baseline policy, employing GPT-4 to approximate human judgments of summary quality and response usefulness in summarization and single-turn dialogue tasks, respectively. For summarization, the baseline is the reference summaries in the test data; for dialogue, it is the preferred response found in the test set. Although prior work indicates that LMs may serve as superior automated evaluators compared to traditional metrics \citep{Chen2023ExploringTU}, we additionally run a human study to validate our reliance on GPT-4 for evaluation in Sec.~\ref{sec:human-judgments}. Our results show that GPT-4’s ratings have a strong correlation with human assessments, with human-GPT-4 agreement typically matching or surpassing inter-annotator consistency.

\textbf{Methods.} Beyond DPO, we assess multiple established methods for aligning language models with human preferences. For the summarization task, we use zero-shot prompting with \textbf{GPT-J} \citep{gpt-j}, and for the dialogue task, we apply 2-shot prompting using \textbf{Pythia-2.8B} \citep{biderman2023pythia}. Additionally, we include the \textbf{SFT} model and \textbf{Preferred-FT}, which is trained through supervised fine-tuning on the preferred completion $y_w$—sourced either from SFT in controlled sentiment and summarization, or from a general language model in the single-turn dialogue scenario. We also examine the \textbf{Unlikelihood} approach~\citep{welleck2019neural}, a pseudo-supervised method that increases the model's likelihood for $y_w$ while actively decreasing it for $y_l$, with an optional scaling factor $\alpha\in[0,1]$ for the unlikelihood objective. Our evaluation includes \textbf{PPO} \citep{schulman2017proximal} trained with a reward function inferred from human preferences, as well as \textbf{PPO-GT}, an oracle variant that utilizes the ground truth reward in the controlled sentiment case. For sentiment tasks, we experiment with two PPO-GT variants: a standard implementation \cite{leandro_von_werra_2023_7790115} and an enhanced version that applies reward normalization and optimizes hyperparameters for improved outcomes (these improvements are also adopted in standard PPO with learned rewards). Finally, we implement the \textbf{Best of $N$} baseline, where $N$ candidate completions are generated from the SFT model (or Preferred-FT in dialogue), and the top response is selected based on a reward model trained on human preference data. While this method often yields strong results, it separates the reward model from PPO training and is computationally expensive for even moderate $N$, as it necessitates generating $N$ responses per prompt at evaluation time.

\subsection{How well can DPO optimize the RLHF objective?}

\begin{figure}
    \centering
    \includegraphics[width=0.50\textwidth]{figures/results/dialogue_winrate_vs_temp.pdf}
    \includegraphics[width=0.49\textwidth]{figures/results/dialogue_winrate_vs_steps.pdf}
    \caption{\textbf{Left.} Dialogue win rates assessed by GPT-4 for Anthropic-HH one-step prompts; DPO stands out as the sole approach surpassing chosen summaries within the Anthropic-HH evaluation set. \textbf{Right.} Win rates for various sampling temperatures throughout training. The advantage of DPO over the dataset labels remains relatively consistent as training progresses, regardless of the sampling temperature used.}
    \vspace{-2mm}
    \label{fig:dialogue-main}
\end{figure}

The KL-penalized reward optimization objective commonly employed in standard RLHF methods seeks to maximize reward while simultaneously limiting how much the policy diverges from a reference policy. As a result, when evaluating different algorithms, it is important to consider both the obtained reward and the KL divergence; obtaining marginally higher reward at the expense of much greater KL is not always preferable. Figure~\ref{fig:frontier-tldr-main} illustrates the tradeoff frontier between reward and KL for a variety of algorithms on the sentiment task. For each algorithm, we perform several training runs, varying the policy conservativeness hyperparameter in each (target KL $\in\{3,6,9,12\}$ for PPO, $\beta \in \{0.05,0.1,1,5\}$, $\alpha\in\{0.05,0.1,0.5,1\}$ for unlikelihood, and random seeds for preferred-FT), resulting in 22 total runs. After every 100 training steps up to convergence, we evaluate each policy on a set of test prompts, measuring the mean reward according to the true reward function and the mean sequence-level KL divergence\footnote{Specifically, the sum of KL-divergences at each timestep.} to the reference policy, $\text{KL}\left(\pi\mid \mid \piref\right)$. Our findings indicate that DPO yields the most favorable frontier by a wide margin, delivering the highest reward with low KL divergence. This outcome is striking for a couple of reasons. First, both DPO and PPO optimize the same objective, yet DPO is clearly more efficient—DPO’s reward/KL balance consistently outperforms PPO. Second, DPO’s tradeoff curve surpasses that of PPO, \emph{even when PPO is provided with ground truth rewards} (PPO-GT).

\subsection{Can DPO scale to real preference datasets?}
\label{sec:dpo-real-datasets}
We proceed to assess the performance of DPO fine-tuning on tasks involving summarization and single-turn dialogue. In the summarization setting, automated metrics like ROUGE often show weak correlation with human judgments~\citep{stiennon2022learning}. Previous research has shown that fine-tuning language models with PPO using human preference data leads to more desirable summaries. To compare different approaches, we generate model outputs using the test portion of the TL;DR summarization dataset, then calculate the average rate at which these outputs are favored over the reference completions in the test split. For each approach, completions are sampled across a range of temperatures from 0.0 to 1.0, and the resulting win rates are depicted in Figure~\ref{fig:frontier-tldr-main} (right). DPO, PPO, and Preferred-FT are all applied as fine-tuning methods to the same GPT-J SFT model\footnote{\url{https://huggingface.co/CarperAI/openai_summarize_tldr_sft}}. Our results show that DPO reaches a win rate of around 61\% at temperature 0.0, outperforming PPO, which achieves about 57\% at its best sampling temperature (also 0.0). DPO additionally yields a higher peak win rate than the best-of-$N$ baseline. It is important to highlight that we did not conduct extensive tuning of DPO’s $\beta$ hyperparameter, so the reported outcomes may not fully reflect its capabilities. Furthermore, DPO demonstrates greater resilience to increases in sampling temperature relative to PPO, which tends to regress to the baseline GPT-J model’s level of performance at higher temperatures. Preferred-FT, on the other hand, does not show a marked improvement over the original SFT model. Finally, we directly compare DPO and PPO with human raters in Section~\ref{sec:human-judgments}; there, DPO outputs sampled at temperature 0.25 are chosen over PPO outputs at temperature 0 in 58\% of cases.

For single-turn dialogue tasks, we assess the various approaches using the portion of the Anthropic HH dataset \citep{bai2022training} test set that involves a single exchange between a human and an assistant. To calculate win rates for each method, GPT-4 assessments use the test set’s preferred completions as the gold standard. Since a standard SFT model does not exist for this benchmark, we begin with the pre-trained Pythia-2.8B model. We then apply Preferred-FT to finetune a reference model on the selected completions to ensure the outputs remain in-distribution, and subsequently train with DPO. For comparison, we also evaluate the best result out of 128 Preferred-FT completions (the Best of $N$ baseline, which we observed saturates at $N=128$ for this task; refer to Appendix Figure~\ref{fig:best-of-n}) and a 2-shot prompted baseline using the Pythia-2.8B base model. Our experiments reveal that DPO matches or surpasses other methods at their optimal temperature settings. Additionally, we test an RLHF model, trained with PPO on the Anthropic HH dataset \footnote{\url{https://huggingface.co/reciprocate/ppo_hh_pythia-6B}} and released by a recognized source \footnote{\url{https://github.com/CarperAI/trlx/tree/main/examples/hh}}; however, we are unable to find a prompt or sampling temperature that consistently outperforms the original Pythia-2.8B. Given our findings on TL;DR and since both approaches optimize the same reward objective, we use the Best of 128 metric as a proxy for the performance of PPO-trained models. In summary, DPO stands out as the only method that is both computationally efficient and capable of exceeding the preferred completions in the Anthropic HH dataset, while also delivering performance on par with or better than the resource-intensive Best of 128 baseline. Lastly, as shown in Figure~\ref{fig:dialogue-main}, DPO achieves its peak performance after relatively few training steps.

\subsection{Generalization to a new input distribution}

\begin{wraptable}{r}{0.375\textwidth}
    \small
    \vspace{-10mm}
    \begin{tabular}{ccc}
        \toprule
        & \multicolumn{2}{c}{\textbf{Win rate compared to ground truth}} \\
        \cmidrule(lr){2-3}
        \textbf{Alg.} & Temp $0$ & Temp $0.25$ \\
        \midrule
        DPO & 0.36 & 0.31 \\
        PPO & 0.26 & 0.23 \\
        \bottomrule
    \end{tabular}
    \caption{GPT-4 win percentages against ground truth summaries on out-of-distribution CNN/DailyMail articles.}
    \vspace{-3mm}
    \label{tab:ood}
\end{wraptable}

To further assess how PPO and DPO perform under distribution shifts, we test the policies trained in our Reddit TL;DR summarization experiment on a new domain: the test set of news articles from the CNN/DailyMail dataset \citep{nallapati-etal-2016-abstractive}. We use the optimal sampling temperatures identified for TL;DR (0 and 0.25). Table~\ref{tab:ood} displays the findings. The GPT-4 win rate was calculated against the reference summaries from the dataset, employing the same GPT-4 (C) prompt used for Reddit TL;DR, with the term ``forum post'' changed to ``news article''. On this new distribution, DPO maintains a clear advantage over PPO. These results offer preliminary support that DPO policies generalize at least as effectively as PPO, even though DPO does not leverage the extra unlabeled Reddit TL;DR prompts utilized by PPO.

\subsection{Corroborating GPT-4 assessments with human evaluations}
\label{sec:human-judgments}
We carry out a human evaluation to test the dependability of GPT-4's decisions, relying on outcomes from the TL;DR summarization task and two unique GPT-4 prompt styles. The \textbf{GPT-4 (S)} (simple) prompt requests a judgment on which summary best captures the main information from a post. In contrast, the \textbf{GPT-4 (C)} (concise) prompt additionally asks which summary is more succinct; we assess this prompt since GPT-4, when using the \textbf{GPT-4 (S)} prompt, demonstrates a tendency to select longer, more redundant summaries compared to human raters. Complete prompt texts can be found in Appendix~\ref{app:prompts}. Three sets of comparisons are performed—using the top-performing approach (DPO, temperature 0.25), the lowest-performing (PPO, temperature 1.0), and a
\begin{wraptable}{r}{0.47\textwidth}
    \centering
    \small
    \vspace{-1.5mm}
    \begin{tabular}{lccc}
    \toprule
        & \textbf{DPO} & \textbf{SFT} & \textbf{PPO-1} \\
        \cmidrule(lr){2-4}
        N respondents & 272 & 122 & 199 \\
        \midrule
        GPT-4 (S) win \% & 47 & 27 & 13 \\
        GPT-4 (C) win \% & 54 & 32 & 12 \\
        Human win \% & 58 & 43 & 17 \\
        \midrule
        GPT-4 (S)-H agree & 70 & 77 & 86 \\
        GPT-4 (C)-H agree & 67 & 79 & 85 \\
        H-H agree & 65 & - & 87 \\
        \bottomrule
    \end{tabular}
    \vspace{-1mm}
    \caption{Comparison of win rates and per-judgment agreement between humans and GPT-4 on TL;DR summarization tasks. \textbf{Human-GPT-4 agreement is on par with human-human agreement.} Each row presents results from pairing a summary generated by the listed technique against a PPO summary with temperature 0.}
    \vspace{-5mm}
    \label{tab:human_results}
\end{wraptable} 
method from the middle of the performance spectrum (SFT, temperature 0.25)—to encompass a wide range of output qualities. All variants are compared to PPO outputs sampled greedily (the top-performing temperature setting for PPO). Our findings reveal that, regardless of prompt, GPT-4 aligns with human evaluators roughly as frequently as humans agree with each other, indicating that GPT-4 can reliably stand in for human judges (because of the small pool of human raters, multiple independent human judgments are collected only for DPO and PPO-1 matchups). In general, the \textbf{GPT-4 (C)} prompt yields win rates that better reflect human preferences, so we adopt this prompt for principal results shown in Section~\ref{sec:dpo-real-datasets}. Further information about the human study—such as the web interface shown to participants and the roster of human volunteers—is included in Appendix~\ref{app:human-study}.

\section{Discussion}
Preference-based learning offers a robust and scalable approach for developing advanced, aligned language models. In this work, we presented DPO, a straightforward training method that enables language models to learn from preferences without relying on reinforcement learning techniques. Instead of adapting the preference learning task to fit traditional RL frameworks and leveraging standard RL tools, DPO establishes a correspondence between language model policies and reward functions, allowing the model to be trained to reflect human preferences \textit{directly} using a simple cross-entropy objective, without the complexities of reinforcement learning or sacrificing generality. DPO matches or surpasses the performance of existing RLHF methods, including those using PPO, with minimal hyperparameter adjustment. Consequently, DPO significantly lowers the difficulty of training language models in accordance with human preferences.

\textbf{Limitations \& Future Directions.} Our findings open up several key avenues for further investigation. One key question is how well the DPO policy can generalize to out-of-distribution data, especially in comparison to approaches that utilize explicit reward functions. Our preliminary experiments indicate that DPO policies achieve a level of generalization comparable to PPO-based approaches, though more extensive analysis is required to confirm this. Another area of interest is whether self-labeling with DPO policies can facilitate the effective utilization of unlabeled prompts in training. Additionally, understanding how reward over-optimization presents itself within the direct preference optimization paradigm remains an open issue—it's unclear, for instance, if the modest performance drop observed in Figure~\ref{fig:dialogue-main}-right is an example of this effect. While our experiments focus on models with up to 6B parameters, scaling DPO to models several orders of magnitude larger presents a promising research opportunity. In terms of evaluation, we observe that GPT-4-derived win rates are sensitive to prompt phrasing; future research could investigate optimal prompting strategies to obtain reliable automated judgments. Lastly, there are numerous potential applications for DPO beyond leveraging human preferences to train language models, such as in the training of generative models across different modalities.

\end{document}