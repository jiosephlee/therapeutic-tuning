\title{Direct Preference Optimization: Your Language Model is Secretly a Reward Model}

\begin{abstract}
Although large-scale unsupervised language models (LMs) acquire extensive world knowledge and display some reasoning abilities, exerting precise influence over their outputs remains challenging due to the entirely unsupervised nature of their pretraining. Current approaches to improve model steerability rely on collecting human judgments comparing model outputs, then fine-tuning the unsupervised LM to reflect these preferences—typically using reinforcement learning from human feedback (RLHF). Yet, RLHF is a complicated and often unstable process: it involves first training a reward model to represent human preferences, then using reinforcement learning to adjust the LM for higher estimated reward while avoiding significant deviation from its original behavior. In this work, we propose a novel way to parameterize the reward model in RLHF, which enables direct computation of the optimal policy in closed form. This lets us address the standard RLHF objective using a simple classification loss alone. Our resulting method, termed \textit{Direct Preference Optimization} (DPO), is stable, effective, and computationally efficient—it avoids the need for language model sampling during fine-tuning and sidesteps extensive hyperparameter tuning. Experimental results demonstrate that DPO can fine-tune language models to better match human preferences, performing on par with or better than prior approaches. Specifically, DPO outperforms PPO-based RLHF for controlling output sentiment, and matches or surpasses quality in summarization and single-turn dialogue, all while being much simpler to implement and train.
\end{abstract}

\section{Introduction}
Large-scale unsupervised language models (LMs) trained on massive corpora have demonstrated remarkable and unexpected capabilities~\citep{chowdhery2022palm, brown2020language, touvron2023llama, bubeck2023sparks}. Despite this, these models are exposed to data authored by humans who possess diverse aims, values, and expertise. Not all of these human intentions or skill levels are ideal for the model to replicate; for instance, though an AI coding assistant should \textit{recognize} frequent programming errors to help rectify them, we nonetheless prefer the model to favor the rare, high-quality coding demonstrated in the dataset when producing code. In a similar vein, we want a language model to be \textit{cognizant} of widespread misconceptions, such as those held by half the population, but do not wish for it to repeat these inaccuracies in half of its relevant outputs. To put it differently, the ability to selectively elicit a model's \emph{intended behaviors and outputs} from its broad \textit{repertoire of knowledge and skills} is fundamental for developing AI systems that are reliable, effective, and governable \citep{ouyang2022training}. Although current approaches generally align LMs with human judgments via reinforcement learning (RL), we demonstrate that the RL-driven objective at the core of many such techniques can in fact be directly optimized using a straightforward binary cross-entropy loss, greatly streamlining the preference learning workflow.

\begin{figure}
    \centering
    \includegraphics[width=0.999\textwidth]{figures/diagrams/teaser.png}
    \caption{\textbf{DPO aligns models with human preferences without reinforcement learning.} Conventional strategies for tuning language models using human feedback typically first train a reward model on a collection of prompts and pairwise human preference labels, and then apply RL to optimize a policy for the induced reward. DPO, however, directly optimizes a policy to best align with the observed preferences via a straightforward classification loss, implicitly learning a reward model from which the optimal policy can be analytically derived.}
    \vspace{-2mm}
    \label{fig:teaser}
\end{figure}

Broadly speaking, current approaches impart desired behaviors to language models by leveraging carefully constructed collections of human preferences, which reflect what people consider safe and useful. This preference learning process follows an initial phase of large-scale, unsupervised pre-training on expansive text corpora. The simplest method for preference learning is supervised fine-tuning on human-annotated, high-quality example responses. However, the most effective techniques to date fall under the category of reinforcement learning from human (or AI-generated) feedback (RLHF/RLAIF; \citep{christiano2017deep,bai2022constitutional}). RLHF frameworks train a reward model on data reflecting human preferences and subsequently apply reinforcement learning to adjust the language model policy so that it generates outputs with higher predicted rewards, all while ensuring the model does not deviate too much from its pre-trained state. Despite RLHF’s ability to yield models with remarkable conversational and programming skills, its overall pipeline is much more involved than standard supervised approaches, as it requires training several LMs and conducting policy sampling within the training loop, leading to considerable computational overhead.

In this work, we demonstrate how to directly train a language model to align with human preferences, bypassing explicit reward modeling and reinforcement learning. We introduce Direct Preference Optimization (DPO), an approach that implicitly targets the same objective as conventional RLHF methods (maximizing reward under a KL-divergence constraint), yet remains easy to implement and efficient to train. Conceptually, DPO adjusts the model by boosting the relative log-likelihood of preferred over non-preferred outputs, while employing an adaptive, per-example importance weight to avoid the model collapse seen with a basic probability ratio loss. As with previous approaches, DPO makes use of a theoretical preference model (e.g., the Bradley-Terry model; \cite{bradley1952rankanalysis}) to evaluate how well a reward function matches observed preference data. However, in contrast to earlier methods that apply the preference model to train a reward predictor before optimizing a policy with respect to it, DPO leverages a change of variables to express the preference loss directly in terms of the policy itself. With a collection of human preference judgments over responses, DPO can thus optimize a policy using a straightforward binary cross-entropy loss, yielding an optimal policy for an implicit reward shaped by the preference data.

The primary contribution of our work is Direct Preference Optimization (DPO), a straightforward algorithm that eliminates the need for reinforcement learning when training language models based on preference data. Through our experiments, we demonstrate that DPO performs on par with, or better than, current approaches—such as PPO-based RLHF—when learning from preference signals in tasks like sentiment control, summarization, and dialogue, utilizing language models containing up to 6B parameters.

\section{Related Work}

Increasingly large self-supervised language models are capable of solving certain tasks in a zero-shot manner \citep{radford2019language} or with only a few prompt examples \citep{gpt3,megatron,chowdhery2022palm}. Nonetheless, their effectiveness on downstream applications and alignment with user instructions can be greatly enhanced by additional fine-tuning on instruction datasets paired with human-generated completions \citep{mishra-etal-2022-cross,sanh2022multitask,chung2022scaling,thoppilan2022lamda}. This process, known as 'instruction tuning,' enables large language models (LLMs) to handle instructions beyond those seen during fine-tuning, generally boosting their practical utility \citep{chung2022scaling}. Although instruction tuning has proven successful, collecting \textit{comparative} human evaluations of model outputs is often easier than obtaining high-quality expert examples, leading recent work to focus on fine-tuning LLMs with human preference datasets, which has enhanced their abilities in translation \citep{kreutzer-etal-2018-reliability}, summarization \citep{stiennon2022learning,ziegler2020finetuning}, narrative generation \citep{ziegler2020finetuning}, and instruction-following \citep{ouyang2022training,ramamurthy2023is}. These approaches first train a reward model to predict human preferences using, for example, the Bradley-Terry model \citep{bradley1952rankanalysis}, and then update the language model to maximize this reward signal using reinforcement learning strategies such as REINFORCE \citep{williams1992reinforce}, proximal policy optimization (PPO; \cite{schulman2017proximal}), or related methods \citep{ramamurthy2023is}. Related research uses instruction-tuned LLMs augmented with human feedback to produce additional synthetic preference data focused on qualities like safety or non-harmfulness \citep{bai2022constitutional}, relying on light human supervision in the form of textual rubrics for labeling by the LLM. These techniques represent the intersection of two research areas: training language models with reinforcement learning for various tasks~\citep{Ranzato2015SequenceLT,paulus2018a,wu2018learning}, and broader frameworks for preference-based learning from humans \citep{christiano2017deep,kupcsik2018learning}. While employing relative human preferences is appealing, fine-tuning large language models via reinforcement learning remains challenging in practice; this work introduces a theoretically grounded method for optimizing relative preferences without the use of reinforcement learning.

Beyond the domain of language, the study of learning policies from preferences has been explored in both bandit problems and reinforcement learning, resulting in several proposed methodologies. When preferences or rankings over actions, instead of direct rewards, are used in contextual bandit learning, the problem is called the contextual dueling bandit (CDB; \cite{yue2012karmed,dudik2015contextual}). Since absolute rewards are unavailable in CDBs, theoretical analysis replaces the classical optimal policy with the concept of a \textit{von Neumann winner}—a policy that, on average, wins against \textit{every} other policy at least half the time \citep{dudik2015contextual}. Nonetheless, CDBs usually assume access to online preference feedback, whereas human preference learning often involves training from a fixed offline set of preference-labeled action pairs \citep{yan2022human}. In the same vein, \textit{preference-based RL} (PbRL) learns from pairwise preferences produced by an unknown scoring function, instead of relying on explicit reward signals \citep{BusaFekete2014,ruiz2023dueling}. There are a range of PbRL algorithms, including those capable of utilizing off-policy preference data, but most of them require first modeling the underlying scoring function (the reward model) and then performing optimization over it \citep{jain2013learning,BusaFekete2014,christiano2017deep,sadigh2017active,kupcsik2018learning}. In contrast, we introduce a single-stage approach to policy learning that aims to directly optimize the policy according to the observed preferences.

\section{Preliminaries}\label{section:prelims}

We summarize the RLHF process described by \citeauthor{ziegler2020finetuning}, as well as subsequent works \citep{stiennon2022learning, bai2022training, ouyang2022training}. This process typically consists of three main steps: (1) supervised fine-tuning (SFT), (2) collecting preferences and training a reward model, and (3) optimization via reinforcement learning.

\textbf{SFT}: RLHF usually starts with supervised fine-tuning of a pre-trained LM on carefully curated data relevant to the target task(s) (such as dialogue or summarization), resulting in a model denoted as $\pisft$.

\textbf{Reward Modeling Stage}: During this phase, the SFT model is given prompts $x$ and generates answer pairs $(y_1, y_2)\sim \pisft(y \mid x)$. These answer pairs are then shown to human annotators, who indicate their preference for one response over the other, represented as $y_w\succ y_l \mid x$, where $y_w$ and $y_l$ signify the chosen and rejected responses from $(y_1, y_2)$, respectively. It is presumed that these preferences arise from an underlying, unobservable reward function $r^*(y, x)$. Various methods exist to represent such preferences, with the Bradley-Terry (BT) \cite{bradley1952rankanalysis} model being widely utilized (alternatively, the more general Plackett-Luce ranking models \citep{plackett1975analysis, luce2012individual} can be used if multiple ranked responses are available). According to the BT model, the probability distribution for human preference $p^*$ is given by:
\begin{equation}\label{eq:bradley-terry}
    p^*(y_1\succ y_2 \mid x)=\frac{\exp\left(r^*(x, y_1)\right)}{\exp\left(r^*(x, y_1)\right) + \exp\left(r^*(x, y_2)\right)}.
\end{equation}
Given a fixed dataset of comparisons $\mathcal{D}=\bigl\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\bigr\}_{i=1}^N$ sampled according to $p^*$, we parameterize a reward model $r_{\phi}(x, y)$ and fit its parameters by maximizing the likelihood. By interpreting this as a binary classification, the associated negative log-likelihood loss becomes:
\begin{equation}\label{eq:reward_model}
    \mathcal{L}_R(r_{\phi}, \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\bigl[\log \sigma(r_{\phi}(x, y_w)- r_{\phi}(x, y_l))\bigr]
\end{equation}
where $\sigma$ denotes the logistic sigmoid. In language modeling applications, $r_{\phi}(x, y)$ is commonly initialized using the SFT model $\pisft(y \mid x)$, and a linear projection is appended to the top transformer layer to yield a single scalar reward score \cite{ziegler2020finetuning}. To reduce reward estimation variance, prior work often standardizes the reward outputs so that $\mathbb{E}_{x,y\sim \mathcal{D}}\left[r_\phi(x, y)\right] = 0$ for every $x$.

\textbf{RL Fine-Tuning Phase}: In the reinforcement learning (RL) stage, the reward function previously learned is utilized to guide the language model via feedback. Building on earlier research~\citep{jaques2017sequence, jaques2020human}, the optimization objective can be expressed as
\begin{equation}\label{eq:RL}
\max_{\pi_{\theta}}  \mathbb{E}_{x\sim \mathcal{D}, y\sim \pi_{\theta}(y \mid x)}\Bigl[r_{\phi}(x, y)\Bigr] - \beta\mathbb{D}_{\textrm{KL}}\Bigl[\pi_{\theta}(y\mid x)\,\|\,\piref(y\mid x)\Bigr],
\end{equation}
where $\beta$ is a scaling factor that limits divergence from the reference policy $\piref$, which is the original SFT model $\pisft$. In implementation, the policy $\pi_\theta$ is typically initialized from $\pisft$. Imposing this regularization is crucial as it ensures the policy remains close to the distribution where the reward model is valid, helps preserve output diversity, and avoids collapse to a single optimal response. Since language generation involves discrete tokens, the objective is non-differentiable and is normally solved using reinforcement learning algorithms. The prevailing method \citep{ziegler2020finetuning, stiennon2022learning, bai2022training, ouyang2022training} defines the reward as ${r(x, y) = r_{\phi}(x, y) -\beta (\log \pi_{\theta}(y\mid x) - \log \piref(y\mid x))}$, which is then maximized using PPO \cite{schulman2017proximal}.

\section{Direct Preference Optimization}\label{sec:DPO}

Driven by the difficulties in scaling reinforcement learning algorithms to complex tasks like language model fine-tuning, we aim to develop a straightforward method for optimizing policies based directly on preferences. Rather than following traditional RLHF techniques that learn a reward function and subsequently maximize it with RL, our method employs a specific reward model parameterization, making it possible to derive the optimal policy analytically, thereby eliminating the need for iterative RL training. 

As we will explain in detail, our main insight is to utilize an analytical relationship that maps reward functions to their corresponding optimal policies. This relationship allows us to reformulate loss functions defined over reward models into equivalent loss functions over policies. By using this change-of-variables strategy, we bypass the necessity of explicitly training a separate reward model, yet still operate within established frameworks for human preference modeling, such as the Bradley-Terry model. Ultimately, the policy network encodes both the language model and the implicit reward signal.

\textbf{Deriving the DPO objective.} We begin from the same RL objective utilized in earlier research, Eq.~\ref{eq:RL}, with a general reward function $r$. In line with prior work~\citep{peters2007reinforcement, peng2019advantage, korbak2022reinforcement, go2023aligning}, it is straightforward to demonstrate that the optimal policy for the KL-regularized reward maximization objective in Eq.~\ref{eq:RL} can be written as:
\begin{equation}\label{eq:op_policy}
    \pi_r(y\mid x) = \frac{1}{Z(x)}\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right),
\end{equation}%
where $Z(x) =\sum_{y}\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)$ serves as the normalization constant, or partition function. For a full derivation, refer to Appendix~\ref{app:derivation1}. Even when employing an MLE estimate $r_{\phi}$ of the true reward function $r^*$, estimating the partition function $Z(x)$ is computationally intensive~\citep{korbak2022reinforcement, go2023aligning}, rendering this formulation challenging to use directly. Nonetheless, by manipulating Eq.~\ref{eq:op_policy}, we can re-express the reward function in terms of its optimal policy $\pi_r$, the reference policy $\piref$, and the unknown $Z(\cdot)$. Specifically, by taking the logarithm of both sides of Eq.~\ref{eq:op_policy} and rearranging, we get:
\begin{equation}\label{eq:main_eq}
    r(x,y) =\beta \log \frac{\pi_r(y\mid x)}{\piref(y\mid x)} + \beta \log Z(x).
\end{equation}
This reparameterization can be used for the true reward $r^*$ and the corresponding optimal policy $\pi^*$. Conveniently, the Bradley-Terry model depends only on the difference in reward values for two outputs, that is, ${p^*(y_1 \succ y_2 \mid x) = \sigma(r^*(x, y_1) - r^*(x, y_2))}$. Plugging the reparameterized $r^*(x,y)$ from Eq.~\ref{eq:main_eq} into the preference model Eq.~\ref{eq:bradley-terry}, the partition function $Z(x)$ cancels out, allowing us to write the probability of human preference using only $\pi^*$ and $\piref$. As a result, the optimal RLHF policy $\pi^*$ under the Bradley-Terry model is characterized by the preference relation:
\begin{equation}\label{eq:objective}
    p^*(y_1\succ y_2 \mid x)=\frac{1}{1 + \exp\left(\beta \log \frac{\pi^*(y_2\mid x)}{\piref(y_2\mid x)} - \beta \log \frac{\pi^*(y_1\mid x)}{\piref(y_1\mid x)}\right)}
\end{equation}
See Appendix~\ref{app:derivation2} for the detailed derivation. Although Eq.~\ref{eq:objective} is derived for the Bradley-Terry model, equivalent results can be obtained for the broader Plackett-Luce family of models~\citep{plackett1975analysis, luce2012individual}, as presented in Appendix~\ref{app:plackett_luce_models}.

Having expressed the probability of human preferences in terms of the optimal policy instead of the reward model, we can now define a maximum likelihood objective for a parameterized policy $\pi_\theta$. Drawing a parallel to the reward modeling approach (see Eq.~\ref{eq:reward_model}), the policy objective is given by:
\begin{equation}\label{eq:optimum_model}
    \mathcal{L}_\text{DPO}(\pi_{\theta}; \piref) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\left[\log \sigma \left(\beta \log \frac{\pi_{\theta}(y_w\mid x)}{\piref(y_w\mid x)} - \beta \log \frac{\pi_{\theta}(y_l\mid x)}{\piref(y_l\mid x)}\right)\right].
\end{equation}
In this manner, we are fitting an implicit reward through a different parameterization, for which $\pi_\theta$ is the optimal policy. Additionally, since this method effectively fits a reparameterized Bradley-Terry model, it benefits from various theoretical guarantees, such as consistency given appropriate assumptions about the distribution of preference data \cite{bong2022generalized}. Theoretical aspects of DPO and its connections to related approaches are further examined in Section~\ref{sec:theory}.

\textbf{What is the effect of the DPO update?} To gain a mechanistic perspective on DPO, it is informative to examine the gradient of the loss function $\mathcal{L}_\text{DPO}$. The gradient with respect to the parameters $\theta$ is given by:
\begin{multline*}
    \nabla_\theta \mathcal{L}_\text{DPO}(\pi_\theta;\piref) = \\ -\beta\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[\underbrace{\sigma(\hat{r}_\theta(x, y_l) - \hat{r}_\theta(x, y_w))}_\text{assigns greater emphasis when reward prediction errs}\left[\underbrace{\nabla_\theta\log \pi(y_w \mid x)}_\text{pushes up probability of $y_w$} - \underbrace{\nabla_\theta\log\pi(y_l \mid x)}_\text{reduces probability of $y_l$}\right]\right],
\end{multline*}
where $\hat{r}_\theta(x, y) = \beta \log \frac{\pi_\theta(y \mid x)}{\piref(y \mid x)}$ denotes the reward implicitly determined by the language model $\pi_\theta$ and the reference model $\piref$ (see Section~\ref{sec:theory} for details). Conceptually, the gradient of $\mathcal{L}_\text{DPO}$ boosts the chance of selecting favored completions $y_w$ while reducing the chance of disfavored ones $y_l$. Notably, each example’s contribution is scaled according to how much the implicit reward function $\hat{r}_\theta$ mistakenly ranks the less preferred completion higher, modulated by the factor $\beta$—that is, by the severity with which the implicit reward reverses the preferred ordering, while reflecting the KL constraint’s strength. Our findings indicate that this weighting is crucial; omitting it (i.e., using a simple version without this scaling term) often leads the language model to deteriorate in performance (see Appendix Table~\ref{tab:unlikelihood_generations}).

\textbf{DPO overview.}  
The standard DPO workflow consists of: 1) Generating response pairs $y_1, y_2 \sim \piref(\cdot \mid x)$ for each prompt $x$, annotating them with human feedback to form the preference dataset $\mathcal{D} = \{x^{(i)}, y_w^{(i)}, y_l^{(i)}\}_{i=1}^N$, and 2) training the language model $\pi_\theta$ by minimizing $\mathcal{L}_\text{DPO}$ with respect to the chosen $\piref$, the dataset $\mathcal{D}$, and a specified $\beta$.  
In real-world scenarios, it is preferable to utilize existing public preference datasets, rather than collecting new samples and human annotations. Because these datasets are typically created using $\pisft$, we set $\piref = \pisft$ when possible. If $\pisft$ does not exist, we instead define $\piref$ as the model that maximizes the likelihood of the preferred responses ${(x, y_w)}$, meaning ${\piref = \argmax_{\pi}\mathbb{E}_{x, y_w \sim \mathcal{D}}\left[\log \pi(y_w \mid x)\right]}$. This approach helps reduce the mismatch between the unknown true reference distribution and the $\piref$ used for DPO training. More specifics about implementation choices and hyperparameters are discussed in Appendix~\ref{app:implementation}.

\section{Theoretical Examination of DPO}
Here, we offer a deeper analysis of the DPO approach, supply theoretical justification, and discuss how DPO addresses shortcomings found in actor-critic methods employed in RLHF, like PPO~\cite{schulman2017proximal}.

\label{sec:theory}

\subsection{Your Language Model Functions as a Reward Model}  
DPO manages to avoid both explicitly modeling a reward function and using RL for policy optimization by employing a single maximum likelihood objective. Observe that the objective in Eq. \ref{eq:main_eq} corresponds to a Bradley-Terry model, where rewards are parameterized as $r^*(x, y) = \beta \log\frac{\pi^*_\theta(y \mid x)}{\piref(y \mid x)}$. We directly optimize our parametric model $\pi_{\theta}$, mirroring the reward model optimization in Eq. \ref{eq:reward_model} after applying a suitable change of variables. In this section, we will develop the theoretical framework for this reparameterization, demonstrate that it does not restrict the set of reward models that can be learned, and show it enables precise recovery of the optimal policy. We start by introducing an equivalence relation for reward functions.

\begin{definition}
Two reward functions $r(x, y)$ and $r'(x, y)$ are considered equivalent if and only if there exists a function $f$ such that $r(x, y) - r'(x, y) = f(x)$.
\end{definition}
It is straightforward to verify that this forms an equivalence relation, which divides the set of reward functions into equivalence classes. The following two lemmas can thus be stated:

\begin{lemma}\label{lemma:same_prefrence} Within the Plackett-Luce model, and especially the Bradley-Terry preference structure, any pair of reward functions from an identical class generates an equivalent distribution over preferences.
\end{lemma}

\begin{lemma}\label{lemma:same_policy}
    Any two reward functions belonging to the same equivalence class result in the identical optimal policy for the constrained RL setup.
\end{lemma}
The proofs are direct and thus relegated to Appendix \ref{app:lemma1}. The initial lemma highlights a classic under-determination issue inherent in the Plackett-Luce model family \cite{plackett1975analysis}. Owing to this ambiguity, additional identifiability conditions are generally required to ensure guarantees on maximum likelihood estimates derived from Eq. \ref{eq:reward_model} \cite{bong2022generalized}. The second lemma asserts that all reward functions within a given class produce the same optimal policy. Therefore, our ultimate objective is solely to recover any representative reward function from the target class. The following Theorem is proven in Appendix~\ref{app:thm1}:
\begin{theorem}\label{thm:main}
    Under mild conditions, every reward class consistent with the Plackett-Luce (including Bradley-Terry as a special case) models can be parameterized by ${r(x, y) = \beta \log \frac{\pi(y\mid x)}{\piref(y\mid x)}}$ for some model $\pi(y\mid x)$ and reference model $\piref(y\mid x)$.
\end{theorem}
\begin{sproof}
    Let $r(x, y)$ be any reward function, inducing an optimal model $\pi_r(y \mid x)$ as defined in Eq. \ref{eq:op_policy}. We will demonstrate that a reward function from the equivalence class of $r$ can be cast into the proposed parameterization. Define the projection $f$ by  
\begin{equation}
    f(r; \piref, \beta)(x, y) = r(x, y) - \beta\log\sum_{y}\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)
\end{equation}
The operator $f$ normalizes the reward by subtracting the log-partition function of $\pi_r$. Since this normalization depends only on $x$, $f(r; \piref, \beta)(x, y)$ remains in the equivalence class of $r(x, y)$. Substituting $r$ with the right-hand side of Eq.~\ref{eq:main_eq} (valid for any reward), we find $f(r; \piref, \beta)(x, y) = \beta \log \frac{\pi_r(y\mid x)}{\piref(y\mid x)}$. Therefore, the projection $f$ constructs an element from the equivalence class of $r$ in the desired form, implying no loss of generality with this reparameterization in the reward model.
\end{sproof}
Alternatively, Theorem~\ref{thm:main} can be interpreted as selecting a unique reward function within each equivalence class via the DPO reparameterization—specifically, the reward that fulfills:
\begin{equation}\label{eq:lag_p}
     \sum_{y}\underbrace{\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)}_{=\pi(y\mid x)\text{, by Theorem~\ref{thm:main} reparameterization}} = 1,
\end{equation}
i.e., $\pi(y\mid x)$ forms a proper probability distribution (non-negative and sums to 1).
Moreover, referencing Eq.~\ref{eq:op_policy}, it is evident that Eq.~\ref{eq:lag_p} corresponds to the partition function of the optimal policy derived from $r(x, y)$.
The central idea of the DPO algorithm is to introduce constraints to the under-specified Plackett-Luce (and particularly Bradley-Terry) preference models, so that while maintaining the scope of feasible reward models, the optimal policy defined in Eq. \ref{eq:op_policy} becomes analytically tractable for every prompt $x$.

\subsection{Instability of Actor-Critic Algorithms}

Our framework can also be leveraged to analyze why typical actor-critic methods, like PPO, may exhibit instability during RLHF. We adhere to the RLHF workflow, concentrating on the RL fine-tuning phase described in Section \ref{section:prelims}. There are clear ties to the control as inference perspective \cite{levine2018reinforcement} applied to the constrained RL setting in \ref{eq:RL}. Here, we employ a parameterized policy $\pi_{\theta}(y\mid x)$ and attempt to minimize $\mathbb{D}_{\text{KL}}[\pi_{\theta}(y|x) \mid\mid \pi^*(y\mid x)]$, where $\pi^*$ represents the optimal policy from Eq. \ref{eq:optimum_model} determined by the reward $r_{\phi}(y, x)$. Some algebraic manipulation provides the following maximization objective:
\begin{equation}\label{eq:AC}
    \max_{\pi_{\theta}}\mathbb{E}_{\pi_{\theta}(y\mid x)}\left[\underbrace{r_{\phi}(x, y) -\beta\log\sum_{y}\piref(y\mid x)\exp\left(\frac{1}{\beta}r_{\phi}(x, y)\right)}_{f(r_{\phi}, \piref, \beta)} - \underbrace{\beta\log\frac{\pi_{\theta}(y\mid x)}{\piref(y\mid x)}}_{\text{KL}}\right]
\end{equation}
This is precisely the objective targeted in several previous studies \citep{ziegler2020finetuning, stiennon2022learning, bai2022training, ouyang2022training}, using the DPO-equivalent reward for the reward family $r_{\phi}$. In this context, the normalization expression in $f(r_{\phi}, \piref, \beta)$ can be understood as the soft value function corresponding to the reference policy $\piref$. Although this component does not impact the optimal policy, omitting it can result in high-variance policy gradients and thus lead to unstable training. One way to address this normalization is to approximate it with a learned value function, but this itself can present optimization challenges. Alternatively, previous work has used a reward normalization approach based on a human completion baseline, effectively serving as a Monte Carlo estimator (with a single sample) of the normalization factor. In contrast, by utilizing the DPO reparameterization, the derived reward function eliminates the need for such baselines entirely.

\section{Experiments}
In this section, we experimentally assess how effectively DPO can learn policies directly from preference data. We begin with a controlled text-generation environment to investigate how well DPO balances maximizing reward against minimizing the KL-divergence from the reference policy, in comparison to established preference learning approaches like PPO. We then test DPO on larger model architectures and more complex RLHF tasks, including summarization and conversational dialogue. Our results indicate that, with minimal hyperparameter adjustment, DPO typically matches or surpasses strong baselines such as RLHF with PPO, as well as approaches that select the best out of $N$ sampled trajectories according to a learned reward model. Prior to reporting these findings, we outline the experimental framework; further information is available in Appendix~\ref{app:exp_details}.

\textbf{Tasks.} We evaluate our methods on three distinct open-ended text generation tasks. For each task, the algorithms are trained to learn a policy from a dataset of preference annotations, denoted as $\mathcal{D}=\bigl\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\bigr\}_{i=1}^N$. In the \textbf{controlled sentiment generation} task, $x$ represents the beginning segment of a movie review sampled from the IMDb corpus \cite{maas-EtAl:2011:ACL-HLT2011}, and the goal is for the policy to produce a continuation $y$ that expresses positive sentiment. To facilitate a controlled comparison, we synthesize preference pairs over generated outputs using a pre-trained sentiment classifier, ensuring $p(\text{positive}\mid x, y_w) > p(\text{positive}\mid x, y_l)$. For SFT, we continue training GPT-2-large on the training portion of the IMDB reviews dataset until it converges (additional information in App~\ref{app:sentiment_details}). For the \textbf{summarization} task, $x$ consists of a Reddit forum post, and the objective is to generate a concise summary $y$ capturing the central themes of the post. In line with previous research, we utilize the Reddit TL;DR summarization corpus \citep{volske-etal-2017-tl} paired with human preference judgments collected by \citeauthor{stiennon2022learning}. Our SFT model is obtained by further training on human-authored post summaries\footnote{\url{https://huggingface.co/CarperAI/openai_summarize_tldr_sft}} and employing the TRLX \citep{leandro_von_werra_2023_7790115} library for RLHF. The preference data used was created by \citeauthor{stiennon2022learning} from model outputs of a comparably trained but distinct SFT model. In the third task, \textbf{single-turn dialogue}, $x$ represents a user prompt, which may include anything from scientific queries to personal advice requests. Here, the policy is tasked with producing a relevant and engaging reply $y$. We make use of the Anthropic Helpful and Harmless dialogue dataset \citep{bai2022training}, which consists of 170,000 conversations between people and AI assistants, with each dialogue concluding with two responses generated by a large (unspecified) language model and a label identifying which was preferred by the human annotator. Since no SFT model pre-trained on this task is available, we construct one by fine-tuning a general-purpose language model exclusively on responses that received the human preference.

\begin{figure}
    \centering
    \includegraphics[width=0.50\textwidth]{figures/results/frontier.pdf}
    \includegraphics[width=0.49\textwidth]{figures/results/tldr_winrate_vs_temp.pdf}
    \caption{\textbf{Left.} The trade-off curve of expected reward against KL divergence to the reference policy. DPO achieves the largest expected reward at every KL level, highlighting its optimization effectiveness. \textbf{Right.} Win rates for TL;DR summarization compared to human-generated summaries, as measured by GPT-4. DPO not only surpasses PPO's highest win rate on summarization tasks, but also displays greater stability with respect to changes in sampling temperature.}
    \vspace{-2mm}
    \label{fig:frontier-tldr-main}
\end{figure}

\textbf{Evaluation.} We employ two distinct methods to assess performance in our experiments. To examine how well each algorithm optimizes the constrained reward maximization objective, we evaluate them in the controlled sentiment generation setting by plotting the trade-off between achieved reward and KL-divergence from the reference policy; this trade-off is measurable because the true reward function (a sentiment classifier) is available. Conversely, in practical scenarios where the ground-truth reward function is inaccessible, we assess algorithms based on their \textit{win rate} compared to a baseline policy. Here, GPT-4 serves as a stand-in for human evaluation when assessing summary quality for summarization and response helpfulness in single-turn dialogue tasks. In the summarization experiments, we compare outputs to reference summaries from the test set, while for dialogue, the baseline consists of the human-preferred responses in the dataset. Although prior research indicates that LMs can provide more reliable automated evaluations than standard metrics \citep{Chen2023ExploringTU}, we supplement this with a human study, as detailed in Sec.~\ref{sec:human-judgments}, to validate our reliance on GPT-4 for evaluation. Our findings show GPT-4's assessments are highly correlated with human judgments, with agreement levels between humans and GPT-4 that are comparable to or exceed agreement between human annotators.

\textbf{Methods.} Beyond DPO, we assess various established methods for aligning language model outputs with human preferences. For baseline comparisons, we employ zero-shot prompting with \textbf{GPT-J} \citep{gpt-j} on the summarization task, and use 2-shot prompting with \textbf{Pythia-2.8B} \citep{biderman2023pythia} for dialogue generation. Additionally, we examine the \textbf{SFT} model and \textbf{Preferred-FT}, which refers to a model fine-tuned via supervised learning on the preferred completion $y_w$, derived from the SFT model in sentiment-controlled and summarization settings, or from a generic language model in single-turn dialogue. Another supervised-like strategy is the \textbf{Unlikelihood} approach~\citep{welleck2019neural}, which encourages the policy to increase the likelihood of $y_w$ while simultaneously decreasing that of $y_l$, modulated by an optional unlikelihood weight $\alpha\in[0,1]$. We further evaluate \textbf{PPO} \citep{schulman2017proximal}, leveraging a reward model trained on preference data, as well as \textbf{PPO-GT}, an oracle variant that uses a ground truth reward function in the sentiment control context. In our sentiment-related trials, two PPO-GT implementations are explored: a standard out-of-the-box solution \cite{leandro_von_werra_2023_7790115} and a version with normalized rewards and enhanced hyperparameter tuning for better results (these enhancements are also applied to standard PPO runs with learned rewards). Lastly, we include the \textbf{Best of $N$} baseline, where $N$ completions are sampled from the SFT model (or Preferred-FT for dialogue), and the highest-rewarded response, as determined by a reward model trained on preference data, is selected. Despite strong performance, this strategy is computationally expensive, since it necessitates generating $N$ completions for each test prompt.

\subsection{How well can DPO optimize the RLHF objective?}

\begin{figure}
    \centering
    \includegraphics[width=0.50\textwidth]{figures/results/dialogue_winrate_vs_temp.pdf}
    \includegraphics[width=0.49\textwidth]{figures/results/dialogue_winrate_vs_steps.pdf}
    \caption{\textbf{Left.} GPT-4 estimated win rates for Anthropic-HH one-step dialogue; DPO stands out as the sole approach that surpasses the selected summaries on the Anthropic-HH evaluation set. \textbf{Right.} Win rates are shown for a range of sampling temperatures as training progresses. DPO’s margin over dataset labels remains consistent during training across different sampling temperatures.}
    \vspace{-2mm}
    \label{fig:dialogue-main}
\end{figure}

The KL-constrained reward maximization objective commonly used in RLHF algorithms seeks to maximize reward while simultaneously limiting the policy's divergence from the reference policy. As a result, when evaluating and comparing different algorithms, it is important to consider both the obtained reward and the corresponding KL divergence; obtaining marginally higher reward at the expense of significantly increased KL is generally undesirable. Figure~\ref{fig:frontier-tldr-main} depicts the trade-off between reward and KL for a range of algorithms in the sentiment classification scenario. For each algorithm, we perform several training runs, each with a distinct policy conservativeness hyperparameter (target KL $\in\{3,6,9,12\}$ for PPO, $\beta \in \{0.05,0.1,1,5\}$ for DPO, and $\alpha\in\{0.05,0.1,0.5,1\}$ for unlikelihood, and various random seeds for preferred-FT), resulting in a total of 22 runs. After every 100 training updates up to convergence, we evaluate every policy on a fixed set of test prompts, measuring both the mean reward according to the true reward function and the mean sequence-level KL\footnote{Specifically, this refers to summing the KL-divergence over all time steps in a sequence.} relative to the reference policy $\text{KL}\left(\pi\mid \mid \piref\right)$. Our findings indicate that DPO yields a clearly superior frontier, obtaining the highest rewards while maintaining low KL values. This is particularly significant for several reasons. First, although DPO and PPO both optimize the same objective, DPO is markedly more efficient, with its reward/KL trade-off strictly outclassing that of PPO. Second, DPO produces a better frontier than PPO, \emph{even in cases where PPO is provided access to ground-truth rewards} (PPO-GT).

\subsection{Can DPO handle real-world preference datasets at scale?}
\label{sec:dpo-real-datasets}
We proceed to assess how DPO performs when fine-tuning on tasks like summarization and single-turn dialogue. In the context of summarization, it is known that automatic metrics such as ROUGE often fail to align well with human preference judgments~\citep{stiennon2022learning}. Previous research demonstrates that fine-tuning language models with PPO guided by human feedback yields more effective summaries. To compare approaches, we sample outputs on the TL;DR summarization dataset’s test split, then measure the average win rate versus the reference summaries from the test set. For all evaluated methods, completions are generated across temperatures ranging from 0.0 to 1.0, with the resulting win rates presented in Figure~\ref{fig:frontier-tldr-main} (right). DPO, PPO, and Preferred-FT are all trained using the same GPT-J SFT checkpoint\footnote{\url{https://huggingface.co/CarperAI/openai_summarize_tldr_sft}}. Our results indicate that DPO attains a win rate of roughly 61\% at temperature 0.0, outperforming PPO, which reaches around 57\% at its best-performing temperature of 0.0. Additionally, DPO achieves a greater maximum win rate than the best-of-$N$ baseline. It is important to highlight that DPO’s $\beta$ hyperparameter was not substantially optimized, suggesting that its full capabilities may not be reflected here. We further observe that DPO’s performance is considerably less sensitive to sampling temperature compared to PPO, whose results can drop to the level of the original GPT-J model at higher temperatures. Preferred-FT offers limited improvement relative to the SFT baseline. In Section~\ref{sec:human-judgments}, we also conduct human evaluations, directly comparing DPO and PPO outputs: DPO samples at temperature 0.25 are selected over PPO samples at temperature 0 in 58\% of cases.

For single-turn dialogue, we assess the various approaches on a subset of the Anthropic HH dataset \citep{bai2022training} test split, focusing on instances involving one round of human-assistant exchange. For GPT-4 assessments, preferred completions from the test set are used as references to calculate the win rate across different techniques. Since a standard SFT model is not available for this benchmark, we initialize with a pre-trained Pythia-2.8B, employ Preferred-FT to develop a reference model on the selected completions (ensuring the generated completions match the model's distribution), and then proceed with DPO training. Our comparison includes the top completion out of 128 Preferred-FT generations (noting that the Best of $N$ baseline reaches saturation at $N=128$ for this evaluation; refer to Appendix Figure~\ref{fig:best-of-n}) and a two-shot prompt configuration for the Pythia-2.8B base model. We observe that DPO matches or outperforms the other approaches for the optimal temperature settings. We further test an RLHF model, trained with PPO on the Anthropic HH dataset \footnote{\url{https://huggingface.co/reciprocate/ppo_hh_pythia-6B}} provided by a reputable source \footnote{\url{https://github.com/CarperAI/trlx/tree/main/examples/hh}}, but we are unable to identify a prompt or sampling temperature that surpasses the performance of the unmodified Pythia-2.8B base model. Drawing from our TL;DR findings and noting that both strategies optimize an equivalent reward objective, we take Best of 128 as an approximate indicator of PPO-level results. In summary, DPO stands out as the sole computationally practical method that consistently betters the preferred completions on the Anthropic HH dataset, achieving performance on par with or superior to the much more resource-intensive Best of 128 baseline. Figure~\ref{fig:dialogue-main} further illustrates that DPO reaches its optimal results comparatively quickly.

\subsection{Generalization to a new input distribution}

\begin{wraptable}{r}{0.375\textwidth}
    \small
    \vspace{-10mm}
    \begin{tabular}{ccc}
        \toprule
        & \multicolumn{2}{c}{\textbf{Win rate against ground truth}} \\
        \cmidrule(lr){2-3}
        \textbf{Alg.} & Temp $0$ & Temp $0.25$ \\
        \midrule
        DPO & 0.36 & 0.31 \\
        PPO & 0.26 & 0.23 \\
        \bottomrule
    \end{tabular}
    \caption{GPT-4 win rates compared to ground truth summaries on out-of-distribution CNN/DailyMail articles.}
    \vspace{-3mm}
    \label{tab:ood}
\end{wraptable}

To further assess how PPO and DPO perform when exposed to distribution shifts, we apply the PPO and DPO policies, originally trained in our Reddit TL;DR summarization study, to a new domain: news articles from the test set of the CNN/DailyMail dataset \citep{nallapati-etal-2016-abstractive}. For this evaluation, we use the optimal sampling temperatures determined for TL;DR (0 and 0.25). The outcomes are shown in Table~\ref{tab:ood}. We calculated the GPT-4 win rate by comparing the generated summaries with the reference summaries from the datasets, employing the same GPT-4 (C) prompt as in the Reddit TL;DR evaluation, but substituting “forum post” with “news article”. On this out-of-distribution dataset, DPO still surpasses the PPO policy by a notable margin. These findings offer preliminary evidence that DPO policies can generalize at least as well as PPO policies, despite DPO not leveraging the extra unlabeled Reddit TL;DR prompts that PPO utilizes.

\subsection{Cross-checking GPT-4 evaluations with human assessments}
\label{sec:human-judgments}
We carry out a human evaluation to assess the trustworthiness of GPT-4's judgments. This is done by using outcomes from the TL;DR summarization study and applying two distinct prompts to GPT-4. The \textbf{GPT-4 (S)} (simple) prompt only asks which summary best captures the key points of the original post. In contrast, the \textbf{GPT-4 (C)} (concise) prompt asks for the summary that is both better and more concise; we include this prompt because, with the \textbf{GPT-4 (S)} prompt, GPT-4 shows a stronger preference for longer, more redundant summaries than humans. Full prompt details can be found in Appendix~\ref{app:prompts}. Three evaluation settings are considered: the top-performing (DPO, temp. 0.25), the lowest-performing (PPO, temp. 1.0), and a 
\begin{wraptable}{r}{0.47\textwidth}
    \centering
    \small
    \vspace{-1.5mm}
    \begin{tabular}{lccc}
    \toprule
        & \textbf{DPO} & \textbf{SFT} & \textbf{PPO-1} \\
        \cmidrule(lr){2-4}
        N respondents & 272 & 122 & 199 \\
        \midrule
        GPT-4 (S) win \% & 47 & 27 & 13 \\
        GPT-4 (C) win \% & 54 & 32 & 12 \\
        Human win \% & 58 & 43 & 17 \\
        \midrule
        GPT-4 (S)-H agree & 70 & 77 & 86 \\
        GPT-4 (C)-H agree & 67 & 79 & 85 \\
        H-H agree & 65 & - & 87 \\
        \bottomrule
    \end{tabular}
    \vspace{-1mm}
    \caption{Comparison of human and GPT-4 win rates and per-example agreement on TL;DR summarization samples. \textbf{Agreement between humans and GPT-4 is on par with human-human agreement.} Each comparison involves a summary from the method named versus a summary from PPO sampled greedily.}
    \vspace{-5mm}
    \label{tab:human_results}
\end{wraptable}method with intermediate performance (SFT, temp. 0.25), to cover a broad range of sample qualities. All these are evaluated against the PPO method at its optimal temperature setting. The results show that, for both prompts, GPT-4's agreement with human raters is comparable to human-human agreement. This indicates that GPT-4 serves as a reasonable substitute for human evaluation (since the pool of human raters is limited, multiple human annotations are only collected for the DPO and PPO-1 settings). Notably, the \textbf{GPT-4 (C)} prompt typically produces win rates that align more closely with human preferences, so we use this prompt for the primary findings reported in Section~\ref{sec:dpo-real-datasets}. Further information about the human evaluation, including the interface used by raters and a list of volunteers, is available in Appendix~\ref{app:human-study}.

\section{Discussion}
Learning from preferences represents an effective and scalable approach for developing capable and aligned language models. In this work, we presented DPO, a straightforward training method that allows language models to learn from preferences without the need for reinforcement learning. Instead of adapting the preference learning task to fit conventional RL frameworks and employing standard RL techniques, DPO establishes a correspondence between language model policies and reward functions. This mapping makes it possible to train language models to align with human preferences \textit{directly} using a basic cross-entropy loss, eliminating the need for reinforcement learning and without compromising generality. Remarkably, DPO matches or outperforms existing RLHF techniques, including those that rely on PPO, with minimal hyperparameter adjustment; as a result, DPO significantly lowers the barriers for training language models based on human feedback.

\textbf{Limitations \& Future Work.} Our findings prompt several avenues for further investigation. One open question is how well the DPO policy generalizes to out-of-distribution data, especially in comparison with approaches that learn from explicit reward signals. Preliminary evidence indicates that DPO policies exhibit similar generalization capabilities as those based on PPO, but a more extensive analysis is required. Another aspect worth exploring is whether leveraging self-labeling with the DPO policy can facilitate effective use of unlabeled prompts. Additionally, it remains unclear how reward over-optimization might appear within the direct preference optimization framework, and whether the modest performance drop observed in Figure~\ref{fig:dialogue-main}-right is a manifestation of this phenomenon. While our current experiments consider models with up to 6B parameters, investigating the scalability of DPO to much larger, state-of-the-art models represents a promising research direction. On the evaluation side, we observe that GPT-4 win rates are sensitive to the phrasing of the prompt; future research may examine optimal methods for eliciting reliable assessments from automated evaluators. Lastly, DPO holds potential for various use cases beyond aligning language models with human preferences, such as training generative models across different modalities.

\end{document}