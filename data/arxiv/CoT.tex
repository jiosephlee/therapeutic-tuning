
\documentclass[]{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} %
\usepackage{multirow}
\usepackage{multicol}
\usepackage{soul}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{float}
\usepackage[T1]{fontenc}
\usepackage{xcolor}         %

\definecolor{bblue}{HTML}{4F81BD}
\definecolor{rred}{HTML}{C0504D}
\definecolor{ggreen}{HTML}{9BBB59}
\definecolor{ppurple}{HTML}{9F4C7C}
\definecolor{darkGreen}{rgb}{0.2,0.5,0.2}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\usepackage[colorlinks,citecolor=mydarkblue,urlcolor=mydarkblue,linkcolor=mydarkblue]{hyperref}

\usepackage{pgfplots}
\usetikzlibrary{pgfplots.groupplots}
\pgfplotsset{compat=1.3}
\usepackage{tikz}
\usetikzlibrary{patterns}

\usepackage{hyperref}


\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage[final]{neurips_2022}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{amssymb}%
\usepackage{pifont}%
\usepackage{makecell}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\usepackage[capitalize,noabbrev]{cleveref}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage{todonotes}
\makeatletter
\newcommand*\iftodonotes{\if@todonotes@disabled\expandafter\@secondoftwo\else\expandafter\@firstoftwo\fi} 
\makeatother
\newcommand{\noindentaftertodo}{\iftodonotes{\noindent}{}}
\newcommand{\fixme}[2][]{\todo[color=yellow,size=\scriptsize,fancyline,caption={},#1]{#2}} %
\newcommand{\note}[4][]{\todo[author=#2,color=#3,size=\scriptsize,fancyline,caption={},#1]{#4}} %

\definecolor{myYellow}{rgb}{0.9,0.9,1}
\sethlcolor{myYellow}

\definecolor{battleshipgrey}{rgb}{0.3, 0.3, 0.3}
\definecolor{brilliantrose}{rgb}{1.0, 0.33, 0.64}
\definecolor{americanrose}{rgb}{1.0, 0.01, 0.24}
\definecolor{jweigreen}{rgb}{0,0.45,0.24}
\definecolor{bluegray}{rgb}{0.1, 0.1, 0.4}
\definecolor{ao(english)}{rgb}{0.0, 0.5, 0.0}
\definecolor{blanchedalmond}{rgb}{1.0, 0.92, 0.8}
\definecolor{atomictangerine}{rgb}{1.0, 0.6, 0.4}
\definecolor{chocolate(web)}{rgb}{0.82, 0.41, 0.12}
\definecolor{bananayellow}{rgb}{1.0, 0.88, 0.21}
\definecolor{goldenbrown}{rgb}{0.6, 0.4, 0.08}
\definecolor{aliceblue}{rgb}{0.94, 0.97, 1.0}
\definecolor{beige}{rgb}{0.96, 0.96, 0.86}
\definecolor{babyblue}{rgb}{0.54, 0.81, 0.94}
\definecolor{camel}{rgb}{0.76, 0.6, 0.42}
\definecolor{cinnamon}{rgb}{0.82, 0.41, 0.12}
\newcommand{\battleshipgrey}[1]{{\color{battleshipgrey}{#1}}}
\newcommand{\americanrose}[1]{{\color{americanrose}{#1}}}
\newcommand{\jweigreen}[1]{{\color{jweigreen}{#1}}}
\newcommand{\darkgreen}[1]{{\color{ao(english)}{#1}}}
\newcommand{\aliceblue}[1]{{\color{aliceblue}{#1}}}
\newcommand{\beige}[1]{{\color{beige}{#1}}}
\newcommand{\babyblue}[1]{{\color{babyblue}{#1}}}
\newcommand{\camel}[1]{{\color{camel}{#1}}}
\newcommand{\cinnamon}[1]{{\color{cinnamon}{#1}}}
\newcommand{\orange}[1]{#1}


\newcommand{\camera}[1]{{\color{orange}{#1}}}
\newcommand{\Fixme}[2][]{\fixme[inline,#1]{#2}\noindentaftertodo}
\newcommand{\Notewho}[3][]{\notewho[inline,#1]{#2}{#3}\noindentaftertodo}
\newcommand{\jason}[2][]{\note[#1]{jason}{violet!20}{#2}}
\newcommand{\Jason}[2][]{\jason[inline,#1]{#2}\noindentaftertodo \vspace{-5mm}}
\newcommand{\denny}[2][]{\note[#1]{denny}{orange!40}{#2}}
\newcommand{\Denny}[2][]{\denny[inline,#1]{#2}\noindentaftertodo \vspace{-5mm}}
\newcommand{\checkme}[1]{\textcolor{violet}{#1}}

\newcommand{\cellcheck}[0]{\makecell[c]{\checkmark}}
\newcommand{\cellx}[0]{\makecell[c]{\xmark}}
\newcommand{\lamda}[0]{LaMDA}
\newcommand{\palm}[0]{PaLM}

\newcommand{\cell}[2]{#1{ \scriptsize $\pm$#2}}



\title{Chain-of-Thought Prompting Elicits Reasoning \\ in Large Language Models}

\author{%
  \vspace{2.5mm}
  Jason Wei \hspace{6mm} Xuezhi Wang \hspace{6mm} Dale Schuurmans \hspace{6mm} Maarten Bosma \\
  \vspace{3mm}
  \textbf{Brian Ichter \hspace{5mm} Fei Xia \hspace{5mm} Ed H. Chi \hspace{5mm} Quoc V. Le \hspace{5mm} Denny Zhou} \\
  Google Research, Brain Team \\
  \vspace{3mm}
  \texttt{\{jasonwei,dennyzhou\}@google.com} \\
}

\begin{document}

\maketitle

\begin{abstract}

We explore how generating a \textit{chain of thought}---a series of intermediate reasoning steps---significantly improves the ability of large language models to perform complex reasoning.
In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called \textit{chain-of-thought prompting}, where a few chain of thought demonstrations are provided as exemplars in prompting.

Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks.
The empirical gains can be striking.
For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.

\end{abstract}


\begin{figure}[H]
\centering
\includegraphics[width=0.96\linewidth]{main_fables/new-pull-figure-landscape.pdf}
\caption{
Chain-of-thought prompting enables large language models to tackle complex arithmetic, commonsense, and symbolic reasoning tasks.
Chain-of-thought reasoning processes are highlighted.
}
\label{fig:pull-figure}
\end{figure} 

\newpage
\input{main_fables/pull-bar-chart}

\section{Introduction}
The NLP landscape has recently been revolutionized by language models \citep[][\textit{inter alia}]{peters-etal-2018-deep,devlin-etal-2019-bert,brown2020language}.
Scaling up the size of language models has been shown to confer a range of benefits, such as improved performance and sample efficiency  \citep[\textit{inter alia}]{kaplan2020scaling,brown2020language}.
However, scaling up model size alone has not proved sufficient for achieving high performance on challenging tasks such as arithmetic, commonsense, and symbolic reasoning \citep{rae2021scaling}.




This work explores how the reasoning ability of large language models can be unlocked by a simple method motivated by two ideas. %
First, techniques for arithmetic reasoning can benefit from generating natural language rationales that lead to the final answer.
Prior work has given models the ability to generate natural language intermediate steps by training from scratch \citep{ling-etal-2017-program} or finetuning a pretrained model \citep{cobbe2021training}, in addition to neuro-symbolic methods that use formal languages instead of natural language \citep{roy-roth-2015-solving, chiang-chen-2019-semantically,amini-etal-2019-mathqa, chen2019neural}.
Second, large language models offer the exciting prospect of in-context few-shot learning via \emph{prompting}.
That is, instead of finetuning a separate language model checkpoint for each new task, one can simply ``prompt'' the model with a few input--output exemplars demonstrating the task.
Remarkably, this has been successful for a range of simple question-answering tasks \citep{brown2020language}.

Both of the above ideas, however, have key limitations. For rationale-augmented training and finetuning methods, it is costly to create a large set of high quality rationales, which is much more complicated than simple input--output pairs used in normal machine learning. 
For the traditional few-shot prompting method used in \citet{brown2020language}, it works poorly on tasks that require reasoning abilities, and often does not improve substantially with increasing language model scale \citep{rae2021scaling}. 
In this paper, we combine the strengths of these two ideas in a way that avoids their limitations. Specifically, we explore the ability of language models to perform few-shot prompting for reasoning tasks, given a prompt that consists of triples: $\langle$input, \emph{chain of thought}, output$\rangle$.
A \emph{chain of thought} is a series of intermediate natural language reasoning steps that lead to the final output, and we refer to this approach as \textit{chain-of-thought prompting}. An example prompt is shown in \cref{fig:pull-figure}. %


We present empirical evaluations on arithmetic, commonsense, and symbolic reasoning benchmarks, showing that chain-of-thought prompting outperforms standard prompting, sometimes to a striking degree.
\cref{fig:pull-bar-chart} illustrates one such result---on the GSM8K benchmark of math word problems \citep{cobbe2021training}, chain-of-thought prompting with \palm{} 540B outperforms standard prompting by a large margin and achieves new state-of-the-art performance.
A prompting only approach is important because it does not require a large training dataset and because a single model checkpoint can perform many tasks without loss of generality.
This work underscores how large language models can learn via a few examples with natural language data about the task (c.f. automatically learning the patterns underlying inputs and outputs via a large training dataset).

\section{Chain-of-Thought Prompting}
Consider one's own thought process when solving a complicated reasoning task such as a multi-step math word problem. 
It is typical to decompose the problem into intermediate steps and solve each before giving the final answer: \textit{``After Jane gives 2 flowers to her mom she has 10 $\ldots$ then after she gives 3 to her dad she will have 7 $\ldots$ so the answer is 7.''}
The goal of this paper is to endow language models with the ability to generate a similar \textit{chain of thought}---a coherent series of intermediate reasoning steps that lead to the final answer for a problem.
We will show that sufficiently large language models can generate chains of thought if demonstrations of chain-of-thought reasoning are provided in the exemplars for few-shot prompting.

\cref{fig:pull-figure} shows an example of a model producing a chain of thought to solve a math word problem that it would have otherwise gotten incorrect.
The chain of thought in this case resembles a solution and can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it mimics a step-by-step thought process for arriving at the answer (and also, solutions/explanations typically come \textit{after} the final answer \citep[][\textit{inter alia}]{narang2020wt5,wiegreffe2021reframing,lampinen2022can}).

Chain-of-thought prompting has several attractive properties as an approach for facilitating reasoning in language models.
\begin{enumerate}[topsep=1pt,itemsep=0ex]%
    \item First, chain of thought, in principle, allows models to decompose multi-step problems into intermediate steps, which means that additional computation can be allocated to problems that require more reasoning steps.
    \item Second, a chain of thought provides an interpretable window into the behavior of the model, suggesting how it might have arrived at a particular answer and providing opportunities to debug where the reasoning path went wrong (although fully characterizing a model's computations that support an answer remains an open question).
    \item Third, chain-of-thought reasoning can be used for tasks such as math word problems, commonsense reasoning, and symbolic manipulation, and is potentially applicable (at least in principle) to any task that humans can solve via language.
    \item Finally, chain-of-thought reasoning can be readily elicited in sufficiently large off-the-shelf language models simply by including examples of chain of thought sequences into the exemplars of few-shot prompting.
\end{enumerate}

In empirical experiments, we will observe the utility of chain-of-thought prompting for arithmetic reasoning (\cref{sec:arithmetic-reasoning}), commonsense reasoning (\cref{sec:commonsense-reasoning}), and symbolic reasoning (\cref{sec:symbolic-reasoning}).


\section{Arithmetic Reasoning}\label{sec:arithmetic-reasoning}
We begin by considering math word problems of the form in \cref{fig:pull-figure}, which measure the arithmetic reasoning ability of language models.
Though simple for humans, arithmetic reasoning is a task where language models often struggle \citep[][\textit{inter alia}]{hendrycks2021measuring,patel-etal-2021-nlp}.
Strikingly, chain-of-thought prompting when used with the 540B parameter language model performs comparably with task-specific finetuned models on several tasks, even achieving new state of the art on the challenging GSM8K benchmark \citep{cobbe2021training}.



\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{main_fables/cot-examples.pdf}
\caption{
Examples of $\langle$input, chain of thought, output$\rangle$ triples for arithmetic, commonsense, and symbolic reasoning benchmarks. Chains of thought are highlighted.
Full prompts in \cref{sec:appendix-full-prompts}.
}
\vspace{-5mm}~
\label{fig:dataset-examples}
\end{figure}

\subsection{Experimental Setup}

We explore chain-of-thought prompting for various language models on multiple benchmarks.

\textbf{Benchmarks.}
We consider the following five math word problem benchmarks:
\textbf{(1)} the \textbf{GSM8K} benchmark of math word problems \citep{cobbe2021training}, 
\textbf{(2)} the \textbf{SVAMP} dataset of math word problems with varying structures \citep{patel-etal-2021-nlp},
\textbf{(3)} the \textbf{ASDiv} dataset of diverse math word problems \citep{miao-etal-2020-diverse},
\textbf{(4)} the \textbf{AQuA} dataset of algebraic word problems, and
\textbf{(5)} the \textbf{MAWPS} benchmark \citep{koncel-kedziorski-etal-2016-mawps}.
Example problems are given in Appendix \cref{tab:math-datasets}.

\textbf{Standard prompting.}
For the baseline, we consider standard few-shot prompting, popularized by \citet{brown2020language}, in which a language model is given in-context exemplars of input--output pairs before outputting a prediction for a test-time example. 
Exemplars are formatted as questions and answers. 
The model gives the answer directly, as shown in \cref{fig:pull-figure} (left).

\textbf{Chain-of-thought prompting.}
Our proposed approach is to augment each exemplar in few-shot prompting with a chain of thought for an associated answer, as illustrated in \cref{fig:pull-figure} (right). 
As most of the datasets only have an evaluation split, we manually composed a set of eight few-shot exemplars with chains of thought for prompting---\cref{fig:pull-figure} (right) shows one chain of thought exemplar, and the full set of exemplars is given in Appendix \cref{tab:appendix-mwp-prompts}. 
(These particular exemplars did not undergo prompt engineering; robustness is studied in \cref{subsec:robustness} and \cref{subsec:faq-prompt-engineering}.)
To investigate whether chain-of-thought prompting in this form can successfully elicit successful reasoning across a range of math word problems, we used this single set of eight chain of thought exemplars for all benchmarks except AQuA, which is multiple choice instead of free response.
For AQuA, we used four exemplars and solutions from the training set, as given in Appendix \cref{tab:appendix-aqua-prompt}.

\textbf{Language models.}
We evaluate five large language models.
The first is \textbf{GPT-3} \citep{brown2020language}, for which we use text-ada-001, text-babbage-001, text-curie-001, and text-davinci-002, which presumably correspond to InstructGPT models of 350M, 1.3B, 6.7B, and 175B parameters \citep{ouyang2022training}.%
The second is \textbf{\lamda{}} \citep{thoppilan2022lamda}, which has models of 422M, 2B, 8B, 68B, and 137B parameters. %
The third is \textbf{\palm{}}, which has models of 8B, 62B, and 540B parameters.
The fourth is \textbf{UL2 20B} \citep{tay2022unifying}, and the fifth is \textbf{Codex} \citep[][code-davinci-002 in the OpenAI API]{chen2021evaluating}.
We sample from the models via greedy decoding (though follow-up work shows chain-of-thought prompting can be improved by taking the majority final answer over many sampled generations \citep{wang2022self}).
For \lamda{}, we report averaged results over five random seeds, where each seed had a different randomly shuffled order of exemplars.
As \lamda{} experiments did not show large variance among different seeds, to save compute we report results for a single exemplar order for all other models.

\subsection{Results}
The strongest results of chain-of-thought prompting are summarized in \cref{fig:main-math}, with all experimental outputs for each model collection, model size, and benchmark shown in \cref{tab:all-lm-math} in the Appendix.
There are three key takeaways.
First, \cref{fig:main-math} shows that chain-of-thought prompting is an emergent ability of model scale \citep{wei2022emergent}. 
That is, chain-of-thought prompting does not positively impact performance for small models, and only yields performance gains when used with models of $\sim$100B parameters.
We qualitatively found that models of smaller scale produced fluent but illogical chains of thought, leading to lower performance than standard prompting.

\input{main_fables/main-math}
Second, chain-of-thought prompting has larger performance gains for more-complicated problems. 
For instance, for GSM8K (the dataset with the lowest baseline performance), performance more than doubled for the largest GPT and \palm{} models. 
On the other hand, for SingleOp, the easiest subset of MAWPS which only requires a single step to solve, performance improvements were either negative or very small (see Appendix \cref{tab:mawps-subsets}).

Third, chain-of-thought prompting via GPT-3 175B and \palm{} 540B compares favorably to prior state of the art, which typically finetunes a task-specific model on a labeled training dataset. 
\cref{fig:main-math} shows how \palm{} 540B uses chain-of-thought prompting to achieve new state of the art on GSM8K, SVAMP, and MAWPS (though note that standard prompting already passed the prior best for SVAMP).
On the other two datasets, AQuA and ASDiv, \palm{} with chain-of-thought prompting reaches within 2\% of the state of the art (Appendix \cref{tab:all-lm-math}).

To better understand why chain-of-thought prompting works,
we manually examined model-generated chains of thought by \lamda{} 137B for GSM8K.
Of 50 random examples where the model returned the correct final answer, all of the generated chains of thought were also logically and mathematically correct except two that coincidentally arrived at the correct answer (see \cref{subsec:correct-chain-of-thought}, and \cref{tab:appendix-gsm8k-correct-examples} for examples of correct model-generated chains of thought).
We also randomly examined 50 random samples for which the model gave the wrong answer.
The summary of this analysis is that 46\% of the chains of thought were almost correct, barring minor mistakes (calculator error, symbol mapping error, or one reasoning step missing), and that the other 54\% of the chains of thought had major errors in semantic understanding or coherence (see \cref{subsec:incorrect-chain-of-thought-analysis}). 
To provide a small insight into why scaling improves chain-of-thought reasoning ability, we performed a similar analysis of errors made by \palm{} 62B and whether those errors were fixed by scaling to \palm{} 540B.
The summary is that scaling \palm{} to 540B fixes a large portion of one-step missing and semantic understanding errors in the 62B model (see \cref{subsec:why-scale-helps}).


\subsection{Ablation Study}\label{subsec:math-ablation}

The observed benefits of using chain-of-thought prompting raises the natural question of whether the same performance improvements can be conferred via other types of prompting.
\cref{fig:bar_ablation} shows an ablation study with three variations of chain of thought described below.

\textbf{Equation only.} One reason for why chain-of-thought prompting might help is that it produces the mathematical equation to be evaluated, and so we test a variation where the model is prompted to output only a mathematical equation before giving the answer. 
\cref{fig:bar_ablation} shows that equation only prompting does not help much for GSM8K, which implies that the semantics of the questions in GSM8K are too challenging to directly translate into an equation without the natural language reasoning steps in chain of thought.
For datasets of one-step or two-step problems, however, we find that equation only prompting does improve performance, since the equation can be easily derived from the question (see Appendix \cref{tab:ablations-arithmetic}). 

\input{main_fables/ablation-bar}
\textbf{Variable compute only.}
Another intuition is that chain of thought allows the model to spend more computation (i.e., intermediate tokens) on harder problems. 
To isolate the effect of variable computation from chain-of-thought reasoning, we test a configuration where the model is prompted to output a only sequence of dots ($\ldots$) equal to the number of characters in the equation needed to solve the problem.
This variant performs about the same as the baseline, which suggests that variable computation by itself is not the reason for the success of chain-of-thought prompting, and that there appears to be utility from expressing intermediate steps via natural language.

\textbf{Chain of thought after answer.} 
Another potential benefit of chain-of-thought prompting could simply be that such prompts allow the model to better access relevant knowledge acquired during pretraining.
Therefore, we test an alternative configuration where the chain of thought prompt is only given after the answer, isolating whether the model actually depends on the produced chain of thought to give the final answer.
This variant performs about the same as the baseline, which suggests that the sequential reasoning embodied in the chain of thought is useful for reasons beyond just activating knowledge.

\input{main_fables/main-robustness}
\subsection{Robustness of Chain of Thought}\label{subsec:robustness}

Sensitivity to exemplars is a key consideration of prompting approaches---for instance, varying the permutation of few-shot exemplars can cause the accuracy of GPT-3 on SST-2 to range from near chance (54.3\%) to near state of the art (93.4\%) \citep{zhao2021calibrate}.
In this final subsection, we evaluate robustness to chains of thought written by different annotators.
In addition to the results above, which used chains of thought written by an Annotator A, two other co-authors of this paper (Annotators B and C) independently wrote chains of thought for the same few-shot exemplars (shown in \cref{sec:appendix-alternate-annotators}).
Annotator A also wrote another chain of thought that was more concise than the original, following the style of solutions given in \citet{cobbe2021training}.\footnote{For instance, whereas original chain of thought uses several short sentences (\textit{``'There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29.''}), the concise chain of thought would read \textit{``5 * 4 = 20 new computers were added. So there are 9 + 20 = 29 new computers in the server room now''}.}

\cref{fig:bar-robustness-analysis} shows these results for \lamda{} 137B on GSM8K and MAWPS (ablation results for other datasets are given in Appendix \cref{tab:ablations-arithmetic} / \cref{tab:ablations-commonsense-symbolic}).
Although there is variance among different chain of thought annotations, as would be expected when using exemplar-based prompting \citep{le-scao-rush-2021-many,reynolds2021prompt,zhao2021calibrate}, all sets of chain of thought prompts outperform the standard baseline by a large margin. %
This result implies that successful use of chain of thought does not depend on a particular linguistic style.

To confirm that successful chain-of-thought prompting works for other sets of exemplars, we also run experiments with three sets of eight exemplars randomly sampled from the GSM8K training set, an independent source (examples in this dataset already included reasoning steps like a chain of thought).\footnote{We sample examples $\leq 60$ tokens to fit into our input context window, and also limit the examples to $\leq 2$ steps to solve for a fair comparison with the eight exemplars that we composed.}
\cref{fig:bar-robustness-analysis} shows that these prompts performed comparably with our manually written exemplars, also substantially outperforming standard prompting.

In addition to robustness to annotators, independently-written chains of thought, different exemplars, and various language models, we also find that chain-of-thought prompting for arithmetic reasoning is robust to different exemplar orders and varying numbers of exemplars (see \cref{subsec:faq-prompt-engineering}).

\section{Commonsense Reasoning}\label{sec:commonsense-reasoning}

Although chain of thought is particularly suitable for math word problems, the language-based nature of chain of thought actually makes it applicable to a broad class of commonsense reasoning problems, which involve reasoning about physical and human interactions under the presumption of general background knowledge.
Commonsense reasoning is key for interacting with the world and is still beyond the reach of current natural language understanding systems \citep{talmor2022commonsenseqa}.

\textbf{Benchmarks.}
We consider five datasets covering a diverse range of commonsense reasoning types.
The popular \textbf{CSQA} \citep{talmor-etal-2019-commonsenseqa} asks commonsense questions about the world involving complex semantics that often require prior knowledge.
\textbf{StrategyQA} \citep{geva-etal-2021-aristotle} requires models to infer a multi-hop strategy to answer questions.
We choose two specialized evaluation sets from the BIG-bench effort \citep{bigbench}: \textbf{Date} Understanding, which involves inferring a date from a given context, and \textbf{Sports} Understanding, which involves determining whether a sentence relating to sports is plausible or implausible.
Finally, the \textbf{SayCan} dataset \citep{ahn2022can} involves mapping a natural language instruction to a sequence of robot actions from a discrete set.
\cref{fig:dataset-examples} shows examples with chain of thought annotations for all datasets.

\textbf{Prompts.}
We follow the same experimental setup as the prior section. 
For CSQA and StrategyQA, we randomly selected examples from the training set and manually composed chains of thought for them to use as few-shot exemplars.
The two BIG-bench tasks do not have training sets, so we selected the first ten examples as exemplars in the evaluation set as few-shot exemplars and report numbers on the rest of the evaluation set.
For SayCan, we use six examples from the training set used in \citet{ahn2022can} and also manually composed chains of thought.

\textbf{Results.}
\cref{fig:commonsense-results} highlights these results for \palm{} (full results for \lamda{}, GPT-3, and different model scales are shown in \cref{tab:all-lm-commonsense}).
For all tasks, scaling up model size improved the performance of standard prompting; chain-of-thought prompting led to further gains, with improvements appearing to be largest for \palm{} 540B.
With chain-of-thought prompting, \palm{} 540B achieved strong performance relative to baselines, outperforming the prior state of the art on StrategyQA (75.6\% vs 69.4\%) and outperforming an unaided sports enthusiast on sports understanding (95.4\% vs 84\%).
These results demonstrate that chain-of-thought prompting can also improve performance on tasks requiring a range of commonsense reasoning abilities (though note that gain was minimal on CSQA).

\input{main_fables/main-commonsense}


\input{main_fables/main-symbolic}
\section{Symbolic Reasoning}\label{sec:symbolic-reasoning}
Our final experimental evaluation considers symbolic reasoning, which is simple for humans but potentially challenging for language models.
We show that chain-of-thought prompting not only enables language models to perform symbolic reasoning tasks that are challenging in the standard prompting setting, but also facilitates length generalization to inference-time inputs longer than those seen in the few-shot exemplars.

\paragraph{Tasks.}
We use the following two toy tasks.
\begin{itemize}[leftmargin=*,topsep=0pt]
    \itemsep0em 
    \item \textbf{Last letter concatenation.}
    This task asks the model to concatenate the last letters of words in a name (e.g., \textit{``Amy Brown''} $\rightarrow$ \textit{``yn''}). 
    It is a more challenging version of first letter concatenation, which language models can already perform without chain of thought.\footnote{We tested 10 common names using GPT-3 \texttt{davinci} and it got all but one correct.}
    We generate full names by randomly concatenating names from the top one-thousand first and last names from name census data ({\small{\url{https://namecensus.com/}}}).
    \item \textbf{Coin flip.}
    This task asks the model to answer whether a coin is still heads up after people either flip or don't flip the coin (e.g., \textit{``A coin is heads up. Phoebe flips the coin. Osvaldo does not flip the coin. Is the coin still heads up?''} $\rightarrow$ \textit{``no''}).
\end{itemize}

As the construction of these symbolic reasoning tasks is well-defined, for each task we consider an \textit{in-domain} test set for which examples had the same number of steps as the training/few-shot exemplars, as well as an \textit{out-of-domain} (OOD) test set, for which evaluation examples had more steps than those in the exemplars. 
For last letter concatenation, the model only sees exemplars of names with two words, and then performs last letter concatenation on names with 3 and 4 words.\footnote{For names of length longer than 2 words, we concatenate multiple first and last names together.}
We do the same for the number of potential flips in the coin flip task. 
Our experimental setup uses the same methods and models as in the prior two sections.
We again manually compose chains of thought for the few-shot exemplars for each task, which are given in \cref{fig:dataset-examples}.

\paragraph{Results.}
The results of these in-domain and OOD evaluations are shown in \cref{fig:symbolic-main} for \palm{}, with results for \lamda{} shown in Appendix \cref{tab:all-lm-symbolic}.
With \palm{} 540B, chain-of-thought prompting leads to almost 100\% solve rates (note that standard prompting already solves coin flip with \palm{} 540, though not for \lamda{} 137B).
Note that these in-domain evaluations are ``toy tasks'' in the sense that perfect solution structures are already provided by the chains of thought in the few-shot exemplars; all the model has to do is repeat the same steps with the new symbols in the test-time example.
And yet, small models still fail---the ability to perform abstract manipulations on unseen symbols for these three tasks only arises at the scale of 100B model parameters.

As for the OOD evaluations, standard prompting fails for both tasks.
With chain-of-thought prompting, language models achieve upward scaling curves (though performance is lower than in the in-domain setting).
Hence, chain-of-thought prompting facilitates length generalization beyond seen chains of thought for language models of sufficient scale.


\section{Discussion}\label{sec:discussion}

We have explored chain-of-thought prompting as a simple mechanism for eliciting multi-step reasoning behavior in large language models.
We first saw that chain-of-thought prompting improves performance by a large margin on arithmetic reasoning, yielding improvements that are much stronger than ablations and robust to different annotators, exemplars, and language models (\cref{sec:arithmetic-reasoning}).
Next, experiments on commonsense reasoning underscored how the linguistic nature of chain-of-thought reasoning makes it generally applicable (\cref{sec:commonsense-reasoning}).
Finally, we showed that for symbolic reasoning, chain-of-thought prompting facilitates OOD generalization to longer sequence lengths (\cref{sec:symbolic-reasoning}).
In all experiments, chain-of-thought reasoning is elicited simply by prompting an off-the-shelf language model. 
No language models were finetuned in the process of writing this paper.

The emergence of chain-of-thought reasoning as a result of model scale has been a prevailing theme \citep{wei2022emergent}.
For many reasoning tasks where standard prompting has a flat scaling curve, chain-of-thought prompting leads to dramatically increasing scaling curves.
Chain-of-thought prompting appears to expand the set of tasks that large language models can perform successfully---in other words, our work underscores that standard prompting only provides a lower bound on the capabilities of large language models.
This observation likely raises more questions than it answers---for instance, how much more can we expect reasoning ability to improve with a further increase in model scale?
What other prompting methods might expand the range of tasks that language models can solve?

As for limitations, we first qualify that although chain of thought emulates the thought processes of human reasoners, this does not answer whether the neural network is actually ``reasoning,'' which we leave as an open question.
Second, although the cost of manually augmenting exemplars with chains of thought is minimal in the few-shot setting, such annotation costs could be prohibitive for finetuning (though this could potentially be surmounted with synthetic data generation, or zero-shot generalization).
\orange{Third, there is no guarantee of correct reasoning paths, which can lead to both correct and incorrect answers; improving factual generations of language models is an open direction for future work \citep[][\textit{inter alia}]{rashkin2021measuring,ye2022unreliability,wiegreffe2021reframing}.}
Finally, the emergence of chain-of-thought reasoning only at large model scales makes it costly to serve in real-world applications; further research could explore how to induce reasoning in smaller models.

\section{Related Work}
This work is inspired by many research areas, which we detail in an extended related work section (\cref{sec:extended-related-work}). Here we describe two directions and associated papers that are perhaps most relevant.

The first relevant direction is using intermediate steps to solve reasoning problems. \cite{ling-etal-2017-program} pioneer the idea of using natural language rationales to solve math word problems through a series of intermediate steps. Their work is a remarkable contrast to the literature using formal languages to reason \citep{roy-2016-reasoning, chiang-chen-2019-semantically, amini-etal-2019-mathqa, chen2019neural}.  \citet{cobbe2021training} extend \citet{ling-etal-2017-program} by creating a larger dataset and using it to finetune a pretrained language model rather than training a model from scratch. In the domain of program synthesis, \cite{nye2021show} leverage language models to predict the final outputs of Python programs via first line-to-line predicting the intermediate computational results, and show that their step-by-step prediction method performs better than directly predicting the final outputs.  

Naturally, this paper also relates closely to the large body of recent work on prompting.
Since the popularization of few-shot prompting as given by \citet{brown2020language}, several general approaches have improved the prompting ability of models, such as automatically learning prompts \citep{lester-etal-2021-power} or giving models instructions describing a task \citep{wei2021finetuned,sanh2021multitask,ouyang2022training}.
Whereas these approaches improve or augment the input part of the prompt (e.g., instructions that are prepended to inputs), our work takes the orthogonal direction of augmenting the outputs of language models with a chain of thought.

\section{Conclusions}
We have explored chain-of-thought prompting as a simple and broadly applicable method for enhancing reasoning in language models. 
Through experiments on arithmetic, symbolic, and commonsense reasoning, we find that chain-of-thought reasoning is an emergent property of model scale that allows sufficiently large language models to perform reasoning tasks that otherwise have flat scaling curves.
Broadening the range of reasoning tasks that language models can perform will hopefully inspire further work on language-based approaches to reasoning. 

\clearpage


\section*{Acknowledgements}
We thank Jacob Devlin, Claire Cui, Andrew Dai, and Ellie Pavlick for providing feedback on the paper.
We thank Jacob Austin, Yuhuai Wu, Henryk Michalewski, Aitor Lewkowycz, Charles Sutton, and Aakanksha Chowdhery for helpful discussions. 
We thank Sid Maxwell for notifying us about a mistake in the manual error analysis in the original manuscript.

\bibliography{example_paper}
\bibliographystyle{acl_natbib}


\clearpage
\section*{Checklist}

\begin{enumerate}
\item For all authors...
\begin{enumerate}
  \item Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope?
    \answerYes{}
  \item Did you describe the limitations of your work? 
    \answerYes{See \cref{sec:discussion} and \cref{subsec:faq-prompt-engineering}.}
  \item Did you discuss any potential negative societal impacts of your work?
    \answerYes{We don't expect negative societal impacts as a direct result of the contributions in our paper. One consideration, however, is that generated chain of thought is not always factual, which is noted as a limitation in \cref{subsec:correct-chain-of-thought} (and note that we do not suggest using such chains of thought in a factual manner or in any real-world scenario).}
  \item Have you read the ethics review guidelines and ensured that your paper conforms to them?
    \answerYes{}
\end{enumerate}


\item If you are including theoretical results...
\begin{enumerate}
  \item Did you state the full set of assumptions of all theoretical results?
    \answerNA{}
        \item Did you include complete proofs of all theoretical results?
    \answerNA{}
\end{enumerate}


\item If you ran experiments...
\begin{enumerate}
  \item Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)?
    \answerYes{We included inputs, outputs, and targets for \lamda{} and GPT-3 in the supplementary material. Although we use proprietary models, we GPT-3 results are fully reproducible. Reproducibility is further discussed in \cref{subsec:reproducibility}.}
  \item Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)?
    \answerYes{Data splits were specified, N/A for hyperparams.}
        \item Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)?
    \answerYes{Standard deviation for multiple seeds using \lamda{} 137B, where each seed is a different random order of exemplars, is given in \cref{tab:ablations-arithmetic} and \cref{tab:ablations-commonsense-symbolic}.}
        \item Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)?
    \answerYes{Type of resources are described in \cref{subsec:computational-resources}, though we did not estimate the total amount of compute.}
\end{enumerate}


\item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
\begin{enumerate}
  \item If your work uses existing assets, did you cite the creators?
    \answerYes{We used two models that we anonymized based on the recommendation of the NeurIPS chairs. These models will be cited in the camera-ready version of the paper.}
  \item Did you mention the license of the assets?
    \answerYes{See \cref{subsec:licenses}.}
  \item Did you include any new assets either in the supplemental material or as a URL?
    \answerYes{The coinflip and last letter concatenation datasets are the only new assets, and they are given in the Supplementary Materials.}
  \item Did you discuss whether and how consent was obtained from people whose data you're using/curating?
    \answerNA{No human data collected.}
  \item Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content?
    \answerNA{No human data collected.}
\end{enumerate}


\item If you used crowdsourcing or conducted research with human subjects...
\begin{enumerate}
  \item Did you include the full text of instructions given to participants and screenshots, if applicable?
    \answerNA{}
  \item Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable?
    \answerNA{}
  \item Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?
    \answerNA{}
\end{enumerate}


\end{enumerate}


\newpage
\clearpage
\appendix





\clearpage
\section{Frequently Asked Questions}\label{sec:faq}

\subsection{Why does increasing model scale improve chain-of-thought prompting?}\label{subsec:why-scale-helps}

The finding that successful chain-of-thought reasoning predictably emerges only at certain model scales is intriguing.
Scaling up language models has been shown to confer benefits such as improved performance and sample efficiency \citep{kaplan2020scaling}, but chain-of-thought reasoning is emergent in the sense that its success cannot be predicted only by extrapolating the performance of small scale models, as chain of thought actually hurts performance for most models smaller than 10B parameters.

The question of why model scale improves chain-of-thought prompting is certainly multi-faceted, and we made a preliminary attempt to shed insight into it via error analysis. 
This small analysis involved manually reading 45 errors made by \palm{} 62B and categorizing them into semantic understanding (20 errors), one step missing (18 errors), and other errors (7 errors). 
The ``other category'' included hallucinations, repetitive outputs, and symbol mapping errors.
This categorization is a coarse one borrowed from the initial error analysis done on \lamda{} in \cref{subsec:incorrect-chain-of-thought-analysis}, for which categories were conceived based on what improvements were needed to make the chain of thought correct.

As shown in \cref{fig:palm-error-analysis}, scaling \palm{} to 540B parameters fixed a substantial portion of errors in all three categories.
Examples of semantic understanding and one-step missing errors that were fixed by scaling \palm{} to 540B are given in \cref{fig:palm-errors-fixed}.
This result appears consistent with a hypothesis that language models acquire a range of semantic understanding and logical reasoning skills as a function of model scale (though note that model scale is often conflated with other factors, such as amount of training compute). 

\begin{figure}[H]
\centering
\includegraphics[width=0.8\linewidth]{fables/palm-error-analysis.pdf}
\caption{
Error analysis of 45 problems that \palm{} 62B got incorrect.
These errors were categorized that semantic understanding, one step missing, and other. The other category includes hallucinations, repetitive outputs, and symbol mapping errors.
Scaling \palm{} to 540B fixed a substantial portion of errors in all categories.
}
\label{fig:palm-error-analysis}
\end{figure} 

There are also three notable points regarding why small language models fail.
The first observation is that small language models fail at even relatively easy symbol mapping tasks. 
As demonstrated in \cref{sec:symbolic-reasoning}, for even symbolic reasoning tasks that only require generalization to new examples using the same chain of thought logical structure that was given in the few-shot exemplars, small language models still failed.
The second observation is that small language models seem to have inherently weaker arithmetic abilities, as shown by \cite{brown2020language}, the ability to do simple arithmetic operations (without semantic understanding) requires sufficient model scale.
Finally, we noticed qualitatively that small language models often did not generate a final answer that could be parsed, due to either repetitions or logic that never arrived at a final answer.

In summary, the success of chain-of-thought reasoning as a result of model scale is a complicated phenomena that likely involves a variety of emergent abilities (semantic understanding, symbol mapping, staying on topic, arithmetic ability, faithfulness, etc).
Future work could more thoroughly investigate what properties of pretraining data, model architecture, and optimization objective causally enable such reasoning capabilities.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{fables/palm-errors-fixed.pdf}
\caption{
Examples of semantic understanding and one-step missing errors that were fixed by scaling \palm{} from 62B to 540B. 
}
\label{fig:palm-errors-fixed}
\end{figure} 

\subsection{What is the role of prompt engineering?}\label{subsec:faq-prompt-engineering}

One of the key considerations of prompting is sensitivity to the exact prompt. 
There is no shortage of work showing that prompts affect language models in unexpected ways \citep{min2022rethinking}.
The general way that we created chain of thought annotations was by taking eight exemplars from the training set and decomposing the reasoning process into multiple steps leading to the final answer. 
Examples of chain of thought annotations are provided in \cref{fig:dataset-examples}, with full prompts given in \cref{sec:appendix-full-prompts}.
To analyze how sensitive chain of thought is to prompt engineering, we performed robustness experiments with respect to various factors.

\begin{itemize}[leftmargin=*]
    \item \textbf{Different annotators.} We first analyze robustness to three different annotators (\cref{subsec:robustness} and \cref{fig:bar-robustness-analysis}). Although there is notable variance in performance (which we will discuss later), chain of thought performed better than the baseline by a large margin for all three annotators on eight datasets in arithmetic, commonsense, and symbolic reasoning (\cref{tab:ablations-arithmetic} and \cref{tab:ablations-commonsense-symbolic}). \orange{Similar to the annotation process in \citet{cobbe2021training}, annotators were not given specific instructions about how to write the chain of thought annotations other than to simply write the step-by-step reasoning process that led to the final answer. Thus, the annotations were written in each annotator's own linguistic ``chain of thought'' writing style.}
    \item \textbf{Annotators without machine learning background.} The GSM8K dataset \citep{cobbe2021training} conveniently provides a training set with reasoning chains written by crowd compute workers, which enables us to investigate whether chain of thought still works with reasoning chains from an independent source without a background in machine learning. So we randomly sampled three sets of eight exemplars with chains of thought from GSM8K. These chain of thought annotations also outperformed the baseline by a large margin for all four arithmetic datasets (\cref{tab:ablations-arithmetic}), indicating that chain of thought is not dependent on a particular set of annotators.
    \item \textbf{Different exemplars.} The different GSM8K exemplars experiment above (\cref{tab:ablations-arithmetic}) also shows that chain-of-thought prompting works for different sets of exemplars. Notably, we test every set of exemplars on all four arithmetic datasets (instead of picking exemplars from the training set for each dataset), which suggests that the exemplars do not necessarily have to come from the same dataset distribution as the test examples.
    \item \textbf{Different order of exemplars.} Prior work has shown that in some cases (e.g., classification) even the order of prompts matter---varying the permutation of few-shot exemplars can cause the accuracy of GPT-3 on SST-2 to range from near chance (54.3\%) to near SOTA (93.4\%) \citep{zhao2021calibrate}. We show the standard deviation of performance from different exemplars in \cref{tab:ablations-arithmetic} and \cref{tab:ablations-commonsense-symbolic}. Standard deviations with respect to prompt order are relatively minimal in almost all cases. The one exception is the coin flip task, for which exemplar orders have high standard deviation, likely for the reason cited in \citet{zhao2021calibrate}---for classification, many exemplars of the same category in a row biases the model outputs).
    \item \textbf{Different number of exemplars.} We also found that gains from chain-of-thought prompting generally still held when there was a varying number of few-shot exemplars. This is shown for five datasets in \cref{fig:n-shot-ablation} (we did not have the compute to run this for all datasets). We also found in preliminary experiments that further increasing the number of exemplars in standard prompting did not lead to significant gains (e.g., increasing from 8 to 16 exemplars did not improve the performance of standard prompting enough to catch up with chain-of-thought prompting).
    \item \textbf{Different language models.} Another interesting question is whether certain prompts that work better for one model work better for other large language models. We find that with the same prompts, chain-of-thought prompting improves performance across all three models (\lamda{}, GPT-3, and \palm{}) for all datasets except CSQA and StrategyQA for GPT-3 (\cref{tab:flagship-table}, \cref{tab:all-lm-commonsense}, \cref{tab:all-lm-symbolic}). The fact that gains from chain of thought did not transfer perfectly among models is a limitation; further work could investigate why how different pre-training datasets and model architectures affect the performance gain from chain-of-thought prompting.
\end{itemize}

\textbf{Prompt engineering still matters, though.} Although the results are relatively robust to the prompt for arithmetic reasoning, we want to be clear that prompt engineering still does matter, and can improve performance significantly in many cases.
Though most chain of thought annotations outperform standard prompting, there is large variation in many cases.
For instance, for the coin flip task, the performance varied from 99.6\% for Annotator A to 71.4\% for Annotator C, though both were above standard prompting = 50.0\% (see \cref{tab:ablations-commonsense-symbolic}).
There are even tasks where prompt engineering is a requirement for good performance. 
In preliminary experiments, we tried using chain of thought to enable language models to reverse the order of a list of 5 items.
While two co-authors were not able to write chain of thought prompts that solved the task despite their best attempts, a third co-author was able to write a chain of thought that perfectly solved the task.

How to generate chain of thought annotations in a robust fashion could be an interesting direction for future work.
For instance, an idea here could be to use a large language model to automatically generate chains of thought via prompting (and potentially optimize this over a validation set).


\subsection{Will chain-of-thought prompting improve performance for my task of interest?}

While chain-of-thought prompting is in principle applicable for any text-to-text task, it is more helpful for some tasks than others.
Based on the experiments in this paper, our intuition is that chain of thought helps the most when three conditions are met: (1) the task is challenging and requires multi-step reasoning, (2) a large language model is used, and (3) the scaling curve is relatively flat.
Conversely, the benefits are smaller when one or more of these conditions are not met.

These intuitions are perhaps supported by the arithmetic reasoning results.
The performance gain from chain-of-thought prompting is largest for \palm{} 540B on GSM8K (challenging multi-step problems, flat scaling curve), which meets these conditions.
The performance gain is small for the subsets of MAWPS that only require one or two steps (SingleOP, SingleEq, and AddSub), for which \palm{} 540B already achieves performance of 90\% or higher (and it is also generally true that there is less headroom for improvement when performance is already strong).

\orange{Although in this paper we focused on multi-step reasoning tasks (arithmetic, commonsense, and symbolic), chain-of-thought prompting can potentially be applied to any task for which humans use a ``chain of thought'' to solve (at least in principle). 
We leave the empirical evaluation of chain-of-thought prompting on such diverse tasks (e.g., machine translation, etc.) to future work.}



\subsection{Why is prompting with the equation only not enough for some arithmetic reasoning datasets?}

Prompting with the equation only as an intermediate step does help on many datasets, especially when the datasets only require a few reasoning steps (SVAMP, ASDiv, MAWPS). For GSM8K, however, using the equation only did not improve performance substantially. Based on qualitative analysis, we believe that these questions are too semantically challenging for the model to directly translate them into a math equation. Consider this example from \lamda{} 137B:

\begin{quote}
{\small{\textbf{\textsc{Question:}} Mike plays ping pong for 40 minutes. In the first 20 minutes, he scores 4 points. In the second 20 minutes, he scores 25\% more points.  How many total points did he score?}}

{\small{\textbf{\textsc{\color{blue}{Equation only (wrong answer):}}} (4 + 20 * 0.25) = 6. The answer is 6.}}

{\small{\textbf{\textsc{\color{darkGreen}{Chain of thought (correct):}}} Mike played ping pong for 40 minutes. In the first 20 minutes, he scored 4 points. In the second 20 minutes, he scored 25\% more points. So he scored 25\% more in the second 20 minutes. 4 x 1.25 = 5. So he scored 5 points in the second 20 minutes. So he scored 9 points in total. The answer is 9.}}
\end{quote}

It is hard for the model to directly translate all of the semantics into a single equation, but chain of thought allows it to better reason about each part of the question via intermediate steps in natural language. 


\clearpage
\section{All Experimental Results}\label{sec:all-results}

This section contains tables for experimental results for varying models and model sizes, on all benchmarks, for standard prompting vs.\ chain-of-thought prompting.

For the arithmetic reasoning benchmarks, some chains of thought (along with the equations produced) were correct, except the model performed an arithmetic operation incorrectly. 
A similar observation was made in \citet{cobbe2021training}.
Hence, we can further add a Python program as an external calculator (using the Python \texttt{\small eval} function) to all the equations in the generated chain of thought. 
When there are multiple equations in a chain of thought, we propagate the external calculator results from one equation to the following equations via string matching.
As shown in \cref{tab:flagship-table}, we see that adding a calculator significantly boosts performance of chain-of-thought prompting on most tasks.

\input{fables/llm-math}

\input{fables/all-lm-tables}

\clearpage
\input{fables/appendix-ablation}


\clearpage
\section{Extended Related Work}\label{sec:extended-related-work}

Chain-of-thought prompting is a general approach that is inspired by several prior directions: prompting, natural language explanations, program synthesis/execution, numeric and logical reasoning, and intermediate language steps.

\subsection{Prompting}

The recent success of large-scale language models has led to growing interest in improving their capability to perform tasks via prompting (\citet{brown2020language}, and see \citet{liu2021pre} for a survey).
This paper falls in the category of general prompting approaches, whereby input prompts are optimized to allow a single large language model to better perform a variety of tasks \citep[][\textit{inter alia}]{li-liang-2021-prefix,lester-etal-2021-power,reif2021recipe}.

One recent line of work aims to improve the ability of language models to perform a task by providing instructions that describe the task \citep{raffel2020exploring,wei2021finetuned,ouyang2022training,sanh2021multitask,wang2022benchmarking}.
This line of work is related because it also augments input--output pairs with meta-data. But whereas an instruction augments the input to a task (instructions are typically prepended to the inputs), chain-of-thought prompting augments the outputs of language models.
Another related direction is sequentially combining the outputs of language models; human--computer interaction (HCI) work \citep{wu2022promptchainer,wu2022ai} has shown that combining sequential generations of language models improves task outcomes in a 20-person user study.

\subsection{Natural language explanations}

Another closely related direction uses natural language explanations (NLEs), often with the goal of improving model interpretability \citep[][\textit{inter alia}]{zhou2020towards,wiegreffe2021teach}.
That line of work typically focuses on natural language inference \citep{camburu2018snli,yordanov2021few,bostrom-etal-2021-flexible}, and produces explanations either simultaneously to or after the final prediction \citep{narang2020wt5,majumder2021rationale,wiegreffe-etal-2021-measuring,wiegreffe2021reframing}.
By contrast, the chain of thought processing considered in this paper occurs \textit{before} the final answer.
And while NLE aims mostly to improve neural network interpretability \citep{rajagopal2021selfexplain}, the goal of chain-of-thought prompting is to allow models to decompose multi-hop reasoning tasks into multiple steps---interpretability is just a side effect.
\citet{marasovic2021few} show that prompt-based finetuning with NLE improves NLI and classification performance, though they largely focus on evaluating explanation plausibility.
In comparison, our work focuses on a range of arithmetic, commonsense, and symbolic tasks that require multi-hop reasoning.

\subsection{Program synthesis and execution}

Using intermediate reasoning steps has a long history in program synthesis and execution \citep[][\textit{inter alia}]{zaremba2014learning}.
Recent work along in this direction has included a number of architectural innovations \citep{cai2017making,dong2019neural,yan2020neural}, as well as the use of large language models \citep{chen2021evaluating,austin2021program}.
The program execution work closest to ours is perhaps \citet{nye2021show}, which show that large language models can perform up to 10-digit addition, evaluate polynomials, and execute python programs.
Whereas generating a program and then executing it can be viewed as a type of reasoning, our work generalizes such domain-specific primitives to natural language, which is open-domain and relevant to any text-to-text NLP task in principle.

\subsection{Numeric and logical reasoning}

Numeric and logical reasoning has been a long-studied task in machine learning and natural language processing \citep[][\textit{inter alia}]{lev-etal-2004-solving}.
Recent work has also aimed to inject numeric reasoning abilities in language models in various ways, such as augmenting BERT with a predefined set of executable operations \citep{andor-etal-2019-giving}, including a graph neural network \citep{ran-etal-2019-numnet}, and using specialized training procedures \citep{piekos-etal-2021-measuring}.
Another line of work aims to enable language models to perform logical or formal reasoning, often by verablizing the rules in natural language 
formal rules using language \citep{clark2020transformers,saeed-etal-2021-rulebert,liang-etal-2021-explainable}.
Perhaps the most-related work here is \citet{recchia2021teaching}, which shows that finetuning enables longhand module operations, which has previously been difficult for performers.
Whereas work in this direction is often task-specific and uses finetuning, we show that chain-of-thought prompting works for a broad range of tasks without any finetuning.

\subsection{Intermediate language steps}

Extensive prior work has shown the benefits of endowing neural networks with the ability to produce intermediate steps via training or finetuning confers various benefits in a range of scenarios.
As examples, it has been shown that natural language intermediate steps can
improve performance \citep{zaidan-etal-2007-using,yao2021refining,hase2021can,gu2021dream}, 
improve robustness \citep{chen2022can},
speed up training \citep{hancock-etal-2018-training},
mitigate bias \citep{dua-etal-2020-benefits}, 
and even help in image and reinforcement learning settings \citep{andreas-etal-2018-learning}.
To endow models with the ability to produce intermediate steps, prior work typically finetunes models on either manually annotated training datasets \citep[][\textit{inter alia}]{camburu2018snli,rajani-etal-2019-explain} or generates synthetic datasets \citep{talmor2020leap,zelikman2022star}.
Compared with these training or finetuning methods, our work shows that various natural language reasoning abilities can be elicited in off-the-shelf language models of sufficient scale simply via prompting.
This prompting setup is important because it allows for intermediate step reasoning without a large number of labeled annotations, and because a single model can perform a range of reasoning tasks without any gradient updates.

\section{Appendix: Additional Analysis}

\subsection{Correct Chain of Thought Analysis}\label{subsec:correct-chain-of-thought}
As mentioned in the main text, we analyze 50 chains of thought from \lamda{} 137B that led to correct answers in the GSM8K dataset.
Of these 50, only one arrived at the correct answer through incorrect reasoning (shown in \cref{tab:appendix-gsm8k-correct-analysis}: ``correct by chance'').
The other 49 had correct logic and math, with examples shown in \cref{tab:appendix-gsm8k-correct-examples}.
Five had minor imperfections while maintaining coherent and understandable logic:
\begin{itemize}[leftmargin=*,topsep=0pt]
    \itemsep0em 
    \item One had underspecified statements (shown in \cref{tab:appendix-gsm8k-correct-analysis}: ``correct but underspecified statements''). 
    \item One made an unrelated but correct statement (shown in \cref{tab:appendix-gsm8k-correct-analysis}: ``correct but unrelated statement'').
    \item Two had the correct logic but omitted specific steps in the equation, though it was still understandable (shown in \cref{tab:appendix-gsm8k-correct-analysis}: ``correct but imperfect equation'').
    \item One had the correct math but inverted the semantics (shown in \cref{tab:appendix-gsm8k-correct-analysis}: ``correct but inverted semantics'')
\end{itemize}

\input{fables/appendix-gsm8k-correct-examples}

Although we find that chain-of-thought reasoning is mostly correct for math questions where the final answer was correct, this is likely because it is hard to arrive at the correct final answer by coincidence for free response questions.
For multiple choice or binary classification problems, it was much more likely that models could arrive at the correct answer via an incorrect reasoning path (e.g., all the commmonsense reasoning datasets we evaluate). 
This is a limitation, and future work should perform an analysis of the factuality of such chains of thought.

\input{fables/appendix-gsm8k-correct-analysis}

\clearpage
\subsection{Incorrect Chain of Thought Analysis}\label{subsec:incorrect-chain-of-thought-analysis}
We also manually analyze 50 randomly sampled outputs of the model that were incorrect on GSM8K for \lamda{} 137B. 
There are many ways that a chain of thought can be incorrect, making the design of error categorization non-trivial.
We decided to categorize errors into what changes are needed to make the chain of thought correct, with the goal of elucidating how the model can be improved in the future.

We found that many chains of thought can be made correct with one of the following three classes of modification.

\begin{itemize}[leftmargin=*,topsep=0pt]
    \itemsep0em 
    \item \textbf{Calculator error only.} We found that 8\% of the chains of thought were completely correct except for a calculator error---in other words, applying an external calculator to equations, as done in \citet{cobbe2021training}, would make the chain of thought correct. An example of this type of error is shown in \cref{tab:appendix-gsm8k-incorrect-examples}: ``calculator error only''. Indeed, the solve rate of chain-of-thought prompting on for \lamda{} 137B GSM8K went up from 14.3\% to 17.3\% when we added a Python program as an external calculator, as shown in \cref{tab:all-lm-math}. 
    Also, 34\% of the examples contained calculator errors in addition to other types of errors. However, we perform the rest of the error categorization independently of calculator errors.
    \item \textbf{Symbol mapping error.} We next found that 16\% percent of the chains of thought were correct except for what we call symbol mapping errors. We define a symbol mapping error as when the chain of thought is correct except for the number symbols, and it could be made totally correct by modifying only the equations and not the words. 
    As one might argue that they could simply place the correct final equation in any chain of thought, we constrain this category to chains of thought where the chain of thought can be modified to be a completely correct reasoning process (not just final answer). An example of this error category is shown in \cref{tab:appendix-gsm8k-incorrect-examples}: ``symbol mapping error''.
    \item \textbf{One step missing error.} Our next category of error is chains of thought which were correct except that they were missing a single step. In other words, these chains of thoughts could be rewritten to be correct by adding in an additional reasoning step that was missed by the model. An example of this error category is shown in \cref{tab:appendix-gsm8k-incorrect-examples}: ``one step missing error''. We found that 22\% percent of the errors fell into this category.
\end{itemize}

\input{fables/appendix-gsm8k-incorrect-examples}

We found that the remaining chains of thought (27 of 50; 54\%) would require substantial edits to make into a correct chain of thought.
Almost all cases here involved some error in semantic understanding (see \cref{tab:appendix-gsm8k-incorrect-examples-cont}: ``semantic understanding error''), and 8 of the 27 also had incoherent chain of thoughts, meaning that some statements in the generated chain of thought did not follow from prior ones or violated basic world knowledge (see \cref{tab:appendix-gsm8k-incorrect-examples-cont}: ``incoherent chain of thought error'').

\input{fables/appendix-gsm8k-incorrect-examples-cont}

\orange{Overall, there are no guarantees that the reasoning processes generated by large language models are coherent or factually correct, as underscored by the recent work evaluating the factuality of language model generations and explanations \citep{maynez-etal-2020-faithfulness,rashkin2021measuring,ye2022unreliability,marasovic2021few,wiegreffe2021reframing}. Incorrect reasoning processes can lead to both incorrect final answers as well as accidentally correct final answers (with accidentally correct final answers being more likely for tasks such as binary classification as opposed to free response). Improving the factuality of language model generations with respect to context and world knowledge is an important direction open problems in language model research and could also be expected to potentially improve multi-step reasoning abilities of language models. One potential method for improving the quality of decoding could involve generating multiple reasoning paths and scoring each of them with a verifier, though this requires training the verifier \citep{cobbe2021training,shen-etal-2021-generate-rank,thoppilan2022lamda}.}

\subsection{Additional Robustness Analysis}\label{subsec:additional-robustness}
As the experiments in the main paper use a fixed number of few-shot exemplars (8; as constrained by the input length of 1024 tokens), we verify that the chain-of-thought prompting is robust to various numbers of few-shot exemplars. 
We run experiments for \lamda{} 137B, comparing chain-of-thought prompting with standard prompting for the five datasets where standard prompting had a mostly flat scaling curve (the largest model did not achieve high performance). 
As shown in \cref{fig:n-shot-ablation}, the improvement of chain-of-thought prompting over standard prompting remains robust to varying the number of few-shot exemplars in the prompt.

\input{fables/appendix-n-shot}
\input{fables/math-datasets}

\clearpage
\section{Additional Details}


\section*{Version Control}
\textbf{V5 $\rightarrow$ V6}. Fixed minor typo in Figure 3.  

\textbf{V4 $\rightarrow$ V5}. Added Codex and UL2 results. Small changes to writing and style of paper.

\textbf{V3 $\rightarrow$ V4}. Fixed typo in Figure 3 and added a couple citations.

\textbf{V2 $\rightarrow$ V3}. Added GPT-3 results. Added SVAMP and AQuA eval datasets for math. Added SayCan eval for commonsense. Added Extended Related Work section (\cref{sec:extended-related-work}). Added ablations for Commonsense and Symbolic Reasoning (\cref{tab:ablations-commonsense-symbolic}). Added FAQ section (\cref{sec:faq}). Added raw results in \cref{sec:all-results}.

\textbf{V1 $\rightarrow$ V2}. Added \palm{} results (V1 only had \lamda{}).

\subsection{Reproducibility Statement}\label{subsec:reproducibility}
As our results make use of two sets of large language models that is not publicly available, we take the following actions to facilitate reproducibility. 
First, we provide the exact input prompts for all tasks in \cref{tab:appendix-mwp-prompts}--\cref{tab:appendix-sports-understanding-prompt} in \cref{sec:appendix-full-prompts} (and emphasize that we do not perform any finetuning and only apply prompting to off-the-shelf language models). 
Second, we conduct experiments using the publicly available GPT-3 API for four model scales text-ada-001, text-babbage-001, text-curie-001, text-davinci-002).
Finally, we make exact inputs, targets, and predictions for \lamda{} 137B for each task available as a zip file in the supplementary material.


\subsection{Computational Resources}\label{subsec:computational-resources}
For all three language models we evaluated, we did prompting-based inference only. No finetuning was done for this paper. For inference on \lamda{} 137B we use TPU v3 (8x8 configuration, 64 chips / 128 cores), and for inference on \palm{} 540B we use TPU v4 (4x4x12 configuration, 192 chips / 384 cores). GPT-3 experiments were done using the public API.\footnote{\url{https://beta.openai.com/docs/api-reference/making-requests}}


\subsection{Dataset Details and Licenses}\label{subsec:licenses}
We list the details and licenses for all arithmetic and commonsense datasets used in this paper. The symbolic reasoning datasets were created synthetically, as described in \cref{sec:commonsense-reasoning}.

\paragraph{Arithmetic reasoning}
\begin{itemize}
    \item Math Word Problem Repository \citep{koncel-kedziorski-etal-2016-mawps}:
    AddSub \citep{hosseini-etal-2014-learning}:  \url{https://www.cs.washington.edu/nlp/arithmetic}; 
    MultiArith \citep{roy-roth-2015-solving}, license: CC BY 4.0.
    \item ASDiv \citep{miao-etal-2020-diverse}: \url{https://github.com/chaochun/nlu-asdiv-dataset}.
    \item AQuA \citep{ling-etal-2017-program}: \url{https://github.com/deepmind/AQuA}, license: \url{https://github.com/deepmind/AQuA/blob/master/LICENSE}.
    \item GSM8K \citep{cobbe2021training}: \url{https://github.com/openai/grade-school-math}, MIT license: \url{https://github.com/openai/grade-school-math/blob/master/LICENSE}.
    \item SVAMP \citep{patel-etal-2021-nlp}: \url{https://github.com/arkilpatel/SVAMP}, MIT license: \url{https://github.com/arkilpatel/SVAMP/blob/main/LICENSE}.
\end{itemize}

\paragraph{Commonsense reasoning}
\begin{itemize}
\item CSQA \citep{talmor-etal-2019-commonsenseqa}: \url{https://www.tau-nlp.org/commonsenseqa}, \url{https://github.com/jonathanherzig/commonsenseqa}.
\item StrategyQA \citep{geva-etal-2021-aristotle}: we use the open-domain setting (question-only set) from  \cite{bigbench}: \url{https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/strategyqa}.
The original dataset is from \url{https://github.com/eladsegal/strategyqa}, MIT license: \url{https://github.com/eladsegal/strategyqa/blob/main/LICENSE}.
\item Date understanding and sports understanding from BIG-Bench \citep{bigbench}: Apache License v.2: \url{https://github.com/google/BIG-bench/blob/main/LICENSE}.
\item SayCan \citep{ahn2022can}: SayCan dataset can be accessed at \url{https://say-can.github.io/} under CC BY 4.0 license.

\end{itemize}


\clearpage
\section{Appendix: Input/Output Examples}\label{sec:appendix-input-output-examples}
\input{fables/appendix-letter-concat-examples}
\input{fables/appendix-coinflip-examples}
\input{fables/appendix-commonsenseqa-examples}
\input{fables/appendix-strategyqa-examples}
\input{fables/appendix-date-understanding-examples}
\input{fables/appendix-sports-understanding-examples}
\input{fables/appendix-saycan-examples}

\clearpage
\section{Appendix: Full Prompts}\label{sec:appendix-full-prompts}
\input{fables/appendix-mwp-prompt}
\input{fables/appendix-aqua-prompt}
\input{fables/appendix-letter-concat-prompt}
\input{fables/appendix-coinflip-prompt}
\input{fables/appendix-commonsenseqa-prompt}
\input{fables/appendix-strategyqa-prompt}
\input{fables/appendix-date-understanding-prompt}
\input{fables/appendix-sports-understanding-prompt}
\input{fables/appendix-saycan-prompt}


\clearpage
\section{Appendix: Alternate Annotators for MWP}\label{sec:appendix-alternate-annotators}
\input{fables/appendix-mwp-prompt-alt}

\newpage
\input{fables/appendix-mwp-prompt-alt-b}

\end{document}
