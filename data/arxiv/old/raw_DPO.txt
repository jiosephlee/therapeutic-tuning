# Introduction

Large unsupervised language models (LMs) trained on very large datasets
acquire surprising
capabilities [@chowdhery2022palm; @brown2020language; @touvron2023llama; @bubeck2023sparks].
However, these models are trained on data generated by humans with a
wide variety of goals, priorities, and skillsets. Some of these goals
and skillsets may not be desirable to imitate; for example, while we may
want our AI coding assistant to *understand* common programming mistakes
in order to correct them, nevertheless, when generating code, we would
like to bias our model toward the (potentially rare) high-quality coding
ability present in its training data. Similarly, we might want our
language model to be *aware* of a common misconception believed by 50%
of people, but we certainly do not want the model to claim this
misconception to be true in 50% of queries about it! In other words,
selecting the model's *desired responses and behavior* from its very
wide *knowledge and abilities* is crucial to building AI systems that
are safe, performant, and controllable [@ouyang2022training]. While
existing methods typically steer LMs to match human preferences using
reinforcement learning (RL), we will show that the RL-based objective
used by existing methods can be optimized exactly with a simple binary
cross-entropy objective, greatly simplifying the preference learning
pipeline.

![ **optimizes for human preferences while avoiding reinforcement
learning.** Existing methods for fine-tuning language models with human
feedback first fit a reward model to a dataset of prompts and human
preferences over pairs of responses, and then use RL to find a policy
that maximizes the learned reward. In contrast, directly optimizes for
the policy best satisfying the preferences with a simple classification
objective, fitting an *implicit* reward model whose corresponding
optimal policy can be extracted in closed
form.](figures/diagrams/teaser.png){#fig:teaser width="99.9%"}

[]{#fig:teaser label="fig:teaser"}

At a high level, existing methods instill the desired behaviors into a
language model using curated sets of human preferences representing the
types of behaviors that humans find safe and helpful. This preference
learning stage occurs after an initial stage of large-scale unsupervised
pre-training on a large text dataset. While the most straightforward
approach to preference learning is supervised fine-tuning on human
demonstrations of high quality responses, the most successful class of
methods is reinforcement learning from human (or AI) feedback
(RLHF/RLAIF; [@christiano2017deep; @bai2022constitutional]). RLHF
methods fit a reward model to a dataset of human preferences and then
use RL to optimize a language model policy to produce responses assigned
high reward without drifting excessively far from the original model.
While RLHF produces models with impressive conversational and coding
abilities, the RLHF pipeline is considerably more complex than
supervised learning, involving training multiple LMs and sampling from
the LM policy in the loop of training, incurring significant
computational costs.

In this paper, we show how to directly optimize a language model to
adhere to human preferences, without explicit reward modeling or
reinforcement learning. We propose *()*, an algorithm that implicitly
optimizes the same objective as existing RLHF algorithms (reward
maximization with a KL-divergence constraint) but is simple to implement
and straightforward to train. Intuitively, the update increases the
relative log probability of preferred to dispreferred responses, but it
incorporates a dynamic, per-example importance weight that prevents the
model degeneration that we find occurs with a naive probability ratio
objective. Like existing algorithms, relies on a theoretical preference
model (such as the Bradley-Terry model; [@bradley1952rankanalysis]) that
measures how well a given reward function aligns with empirical
preference data. However, while existing methods use the preference
model to define a preference loss to train a reward model and then train
a policy that optimizes the learned reward model, uses a change of
variables to define the preference loss as a function of the policy
directly. Given a dataset of human preferences over model responses, can
therefore optimize a policy using a simple binary cross entropy
objective, producing the optimal policy to an implicit reward function
fit to the preference data.

Our main contribution is (), a simple RL-free algorithm for training
language models from preferences. Our experiments show that is at least
as effective as existing methods, including PPO-based RLHF, for learning
from preferences in tasks such as sentiment modulation, summarization,
and dialogue, using language models with up to 6B parameters.

# Related Work

Self-supervised language models of increasing scale learn to complete
some tasks zero-shot [@radford2019language] or with few-shot prompts
[@gpt3; @megatron; @chowdhery2022palm]. However, their performance on
downstream tasks and alignment with user intent can be significantly
improved by fine-tuning on datasets of instructions and human-written
completions
[@mishra-etal-2022-cross; @sanh2022multitask; @chung2022scaling; @thoppilan2022lamda].
This 'instruction-tuning' procedure enables LLMs to generalize to
instructions outside of the instruction-tuning set and generally
increase their usability [@chung2022scaling]. Despite the success of
instruction tuning, *relative* human judgments of response quality are
often easier to collect than expert demonstrations, and thus subsequent
works have fine-tuned LLMs with datasets of human preferences, improving
proficiency in translation [@kreutzer-etal-2018-reliability],
summarization [@stiennon2022learning; @ziegler2020finetuning],
story-telling [@ziegler2020finetuning], and instruction-following
[@ouyang2022training; @ramamurthy2023is]. These methods first optimize a
neural network reward function for compatibility with the dataset of
preferences under a preference model such as the Bradley-Terry model
[@bradley1952rankanalysis], then fine-tune a language model to maximize
the given reward using reinforcement learning algorithms, commonly
REINFORCE [@williams1992reinforce], proximal policy optimization (PPO;
[@schulman2017proximal]), or variants [@ramamurthy2023is]. A
closely-related line of work leverages LLMs fine-tuned for instruction
following with human feedback to generate additional synthetic
preference data for targeted attributes such as safety or harmlessness
[@bai2022constitutional], using only weak supervision from humans in the
form of a text rubric for the LLM's annotations. These methods represent
a convergence of two bodies of work: one body of work on training
language models with reinforcement learning for a variety of
objectives [@Ranzato2015SequenceLT; @paulus2018a; @wu2018learning] and
another body of work on general methods for learning from human
preferences [@christiano2017deep; @kupcsik2018learning]. Despite the
appeal of using relative human preferences, fine-tuning large language
models with reinforcement learning remains a major practical challenge;
this work provides a theoretically-justified approach to optimizing
relative preferences without RL.

Outside of the context of language, learning policies from preferences
has been studied in both bandit and reinforcement learning settings, and
several approaches have been proposed. Contextual bandit learning using
preferences or rankings of actions, rather than rewards, is known as a
contextual dueling bandit (CDB; [@yue2012karmed; @dudik2015contextual]).
In the absence of absolute rewards, theoretical analysis of CDBs
substitutes the notion of an optimal policy with a *von Neumann winner*,
a policy whose expected win rate against *any* other policy is at least
50% [@dudik2015contextual]. However, in the CDB setting, preference
labels are given online, while in learning from human preferences, we
typically learn from a fixed batch of offline preference-annotated
action pairs [@yan2022human]. Similarly, *preference-based RL* (PbRL)
learns from binary preferences generated by an *unknown* 'scoring'
function rather than rewards [@BusaFekete2014; @ruiz2023dueling].
Various algorithms for PbRL exist, including methods that can reuse
off-policy preference data, but generally involve first explicitly
estimating the latent scoring function (i.e. the reward model) and
subsequently optimizing it
[@jain2013learning; @BusaFekete2014; @christiano2017deep; @sadigh2017active; @kupcsik2018learning].
We instead present a single stage policy learning approach that directly
optimizes a policy to satisfy preferences.

# Preliminaries {#section:prelims}

We review the RLHF pipeline in @ziegler2020finetuning (and later
[@stiennon2022learning; @bai2022training; @ouyang2022training]). It
usually includes three phases: 1) supervised fine-tuning (SFT); 2)
preference sampling and reward learning and 3) RL optimization.

**SFT**: RLHF typically begins by fine-tuning a pre-trained LM with
supervised learning on high-quality data for the downstream task(s) of
interest (dialogue, summarization, etc.), to obtain a model
$\pi^\text{SFT}$.

**Reward Modelling Phase**: In the second phase the SFT model is
prompted with prompts $x$ to produce pairs of answers
$(y_1, y_2)\sim \pi^\text{SFT}(y \mid x)$. These are then presented to
human labelers who express preferences for one answer, denoted as
$y_w\succ y_l \mid x$ where $y_w$ and $y_l$ denotes the preferred and
dispreferred completion amongst $(y_1, y_2)$ respectively. The
preferences are assumed to be generated by some latent reward model
$r^*(y, x)$, which we do not have access to. There are a number of
approaches used to model preferences, the Bradley-Terry (BT)
[@bradley1952rankanalysis] model being a popular choice (although more
general Plackett-Luce ranking models
[@plackett1975analysis; @luce2012individual] are also compatible with
the framework if we have access to several ranked answers). The BT model
stipulates that the human preference distribution $p^*$ can be written
as: $$\label{eq:bradley-terry}
    p^*(y_1\succ y_2 \mid x)=\frac{\exp\left(r^*(x, y_1)\right)}{\exp\left(r^*(x, y_1)\right) + \exp\left(r^*(x, y_2)\right)}.$$
Assuming access to a static dataset of comparisons
$\mathcal{D}=\bigl\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\bigr\}_{i=1}^N$
sampled from $p^*$, we can parametrize a reward model $r_{\phi}(x, y)$
and estimate the parameters via maximum likelihood. Framing the problem
as a binary classification we have the negative log-likelihood loss:
$$\label{eq:reward_model}
    \mathcal{L}_R(r_{\phi}, \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\bigl[\log \sigma(r_{\phi}(x, y_w)- r_{\phi}(x, y_l))\bigr]$$
where $\sigma$ is the logistic function. In the context of LMs, the
network $r_{\phi}(x, y)$ is often initialized from the SFT model
$\pi^\text{SFT}(y \mid x)$ with the addition of a linear layer on top of
the final transformer layer that produces a single scalar prediction for
the reward value [@ziegler2020finetuning]. To ensure a reward function
with lower variance, prior works normalize the rewards, such that
$\mathbb{E}_{x,y\sim \mathcal{D}}\left[r_\phi(x, y)\right] = 0$ for all
$x$.

**RL Fine-Tuning Phase**: During the RL phase, the learned reward
function is used to provide feedback to the language model. Following
prior works [@jaques2017sequence; @jaques2020human], the optimization is
formulated as $$\label{eq:RL}
\max_{\pi_{\theta}}  \mathbb{E}_{x\sim \mathcal{D}, y\sim \pi_{\theta}(y \mid x)}\bigl[r_{\phi}(x, y)\bigr] - \beta\mathbb{D}_{\textrm{KL}}\bigl[\pi_{\theta}(y\mid x)\mid \mid \pi_\text{ref}(y\mid x)\bigr],$$
where $\beta$ is a parameter controlling the deviation from the base
reference policy $\pi_\text{ref}$, namely the initial SFT model
$\pi^\text{SFT}$. In practice, the language model policy $\pi_\theta$ is
also initialized to $\pi^\text{SFT}$. The added constraint is important,
as it prevents the model from deviating too far from the distribution on
which the reward model is accurate, as well as maintaining the
generation diversity and preventing mode-collapse to single high-reward
answers. Due to the discrete nature of language generation, this
objective is not differentiable and is typically optimized with
reinforcement learning. The standard approach
[@ziegler2020finetuning; @stiennon2022learning; @bai2022training; @ouyang2022training]
has been to construct the reward function
${r(x, y) = r_{\phi}(x, y) -\beta (\log \pi_{\theta}(y\mid x) - \log \pi_\text{ref}(y\mid x))}$,
and maximize using PPO [@schulman2017proximal].

# Direct Preference Optimization {#sec:DPO}

Motivated by the challenges of applying reinforcement learning
algorithms on large-scale problems such as fine-tuning language models,
our goal is to derive a simple approach for policy optimization using
preferences directly. Unlike prior RLHF methods, which learn a reward
and then optimize it via RL, our approach leverages a particular choice
of reward model parameterization that enables extraction of its optimal
policy in closed form, without an RL training loop. As we will describe
next in detail, our key insight is to leverage an analytical mapping
from reward functions to optimal policies, which enables us to transform
a loss function over reward functions into a loss function over
policies. This change-of-variables approach avoids fitting an explicit,
standalone reward model, while still optimizing under existing models of
human preferences, such as the Bradley-Terry model. In essence, the
policy network represents both the language model and the (implicit)
reward.

**Deriving the DPO objective.** We start with the same RL objective as
prior work, Eq. [\[eq:RL\]](#eq:RL){reference-type="ref"
reference="eq:RL"}, under a general reward function $r$. Following prior
work [@peters2007reinforcement; @peng2019advantage; @korbak2022reinforcement; @go2023aligning],
it is straightforward to show that the optimal solution to the
KL-constrained reward maximization objective in
Eq. [\[eq:RL\]](#eq:RL){reference-type="ref" reference="eq:RL"} takes
the form: $$\label{eq:op_policy}
    \pi_r(y\mid x) = \frac{1}{Z(x)}\pi_\text{ref}(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right),$$
where
$Z(x) =\sum_{y}\pi_\text{ref}(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)$
is the partition function. See Appendix
[\[app:derivation1\]](#app:derivation1){reference-type="ref"
reference="app:derivation1"} for a complete derivation. Even if we use
the MLE estimate $r_{\phi}$ of the ground-truth reward function $r^*$,
it is still expensive to estimate the partition function $Z(x)$
[@korbak2022reinforcement; @go2023aligning], which makes this
representation hard to utilize in practice. However, we can rearrange
Eq. [\[eq:op_policy\]](#eq:op_policy){reference-type="ref"
reference="eq:op_policy"} to express the reward function in terms of its
corresponding optimal policy $\pi_r$, the reference policy
$\pi_\text{ref}$, and the unknown partition function $Z(\cdot)$.
Specifically, we first take the logarithm of both sides of
Eq. [\[eq:op_policy\]](#eq:op_policy){reference-type="ref"
reference="eq:op_policy"} and then with some algebra we obtain:
$$\label{eq:main_eq}
    r(x,y) =\beta \log \frac{\pi_r(y\mid x)}{\pi_\text{ref}(y\mid x)} + \beta \log Z(x).$$
We can apply this reparameterization to the ground-truth reward $r^*$
and corresponding optimal model $\pi^*$. Fortunately, the Bradley-Terry
model depends only on the difference of rewards between two completions,
i.e., ${p^*(y_1 \succ y_2 \mid x) = \sigma(r^*(x, y_1) - r^*(x, y_2))}$.
Substituting the reparameterization in
Eq. [\[eq:main_eq\]](#eq:main_eq){reference-type="ref"
reference="eq:main_eq"} for $r^*(x,y)$ into the preference model
Eq. [\[eq:bradley-terry\]](#eq:bradley-terry){reference-type="ref"
reference="eq:bradley-terry"}, the partition function cancels, and we
can express the human preference probability in terms of only the
optimal policy $\pi^*$ and reference policy $\pi_\text{ref}$. Thus, the
optimal RLHF policy $\pi^*$ under the Bradley-Terry model satisfies the
preference model: $$\label{eq:objective}
    p^*(y_1\succ y_2 \mid x)=\frac{1}{1 + \exp\left(\beta \log \frac{\pi^*(y_2\mid x)}{\pi_\text{ref}(y_2\mid x)} - \beta \log \frac{\pi^*(y_1\mid x)}{\pi_\text{ref}(y_1\mid x)}\right)}$$
The derivation is in
Appendix [8.2](#app:derivation2){reference-type="ref"
reference="app:derivation2"}. While
Eq. [\[eq:objective\]](#eq:objective){reference-type="ref"
reference="eq:objective"} uses the Bradley-Terry model, we can similarly
derive expressions under the more general Plackett-Luce
models [@plackett1975analysis; @luce2012individual], shown in
Appendix [8.3](#app:plackett_luce_models){reference-type="ref"
reference="app:plackett_luce_models"}.

Now that we have the probability of human preference data in terms of
the optimal policy rather than the reward model, we can formulate a
maximum likelihood objective for a parametrized policy $\pi_\theta$.
Analogous to the reward modeling approach (i.e.
Eq. [\[eq:reward_model\]](#eq:reward_model){reference-type="ref"
reference="eq:reward_model"}), our policy objective becomes:
$$\label{eq:optimum_model}
    \mathcal{L}_\text{DPO}(\pi_{\theta}; \pi_\text{ref}) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\left[\log \sigma \left(\beta \log \frac{\pi_{\theta}(y_w\mid x)}{\pi_\text{ref}(y_w\mid x)} - \beta \log \frac{\pi_{\theta}(y_l\mid x)}{\pi_\text{ref}(y_l\mid x)}\right)\right].$$
This way, we fit an implicit reward using an alternative
parameterization, whose optimal policy is simply $\pi_\theta$. Moreover,
since our procedure is equivalent to fitting a reparametrized
Bradley-Terry model, it enjoys certain theoretical properties, such as
consistencies under suitable assumption of the preference data
distribution [@bong2022generalized]. In
Section [\[sec:theory\]](#sec:theory){reference-type="ref"
reference="sec:theory"}, we further discuss theoretical properties of
DPO in relation to other works.

**What does the DPO update do?** For a mechanistic understanding of DPO,
it is useful to analyze the gradient of the loss function
$\mathcal{L}_\text{DPO}$. The gradient with respect to the parameters
$\theta$ can be written as: $$\begin{gathered}
\label{eq:gradient}
    \nabla_\theta \mathcal{L}_\text{DPO}(\pi_\theta;\pi_\text{ref}) = \\ -\beta\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \bigg[\underbrace{\sigma(\hat{r}_\theta(x, y_l) - \hat{r}_\theta (x, y_w))}_\text{higher weight when reward estimate is wrong}\bigg[\underbrace{\nabla_\theta\log \pi(y_w \mid x)}_\text{increase likelihood of $y_w$} - \underbrace{\nabla_\theta\log\pi(y_l \mid x)}_\text{decrease likelihood of $y_l$}\bigg]\bigg],\end{gathered}$$
where
$\hat{r}_\theta(x, y) = \beta \log \frac{\pi_\theta(y \mid x)}{\pi_\text{ref}(y \mid x)}$
is the reward implicitly defined by the language model $\pi_\theta$ and
reference model $\pi_\text{ref}$ (more in
Section [\[sec:theory\]](#sec:theory){reference-type="ref"
reference="sec:theory"}). Intuitively, the gradient of the loss function
$\mathcal{L}_\text{DPO}$ increases the likelihood of the preferred
completions $y_w$ and decreases the likelihood of dispreferred
completions $y_l$. Importantly, the examples are weighed by how much
higher the implicit reward model $\hat{r}_\theta$ rates the dispreferred
completions, scaled by $\beta$, i.e, how incorrectly the implicit reward
model orders the completions, accounting for the strength of the KL
constraint. Our experiments suggest the importance of this weighting, as
a naïve version of this method without the weighting coefficient can
cause the language model to degenerate (Appendix
Table [1](#tab:unlikelihood_generations){reference-type="ref"
reference="tab:unlikelihood_generations"}).

**DPO outline.** The general DPO pipeline is as follows: 1) Sample
completions $y_1, y_2 \sim \pi_\text{ref}(\cdot \mid x)$ for every
prompt $x$, label with human preferences to construct the offline
dataset of preferences
$\mathcal{D} = \{x^{(i)}, y_w^{(i)}, y_l)^{(i)}\}_{i=1}^N$ and 2)
optimize the language model $\pi_\theta$ to minimize
$\mathcal{L}_\text{DPO}$ for the given $\pi_\text{ref}$ and
$\mathcal{D}$ and desired $\beta$. In practice, one would like to reuse
preference datasets publicly available, rather than generating samples
and gathering human preferences. Since the preference datasets are
sampled using $\pi^\text{SFT}$, we initialize
$\pi_\text{ref}= \pi^\text{SFT}$ whenever available. However, when
$\pi^\text{SFT}$ is not available, we initialize $\pi_\text{ref}$ by
maximizing likelihood of preferred completions ${(x, y_w)}$, that is,
${\pi_\text{ref}= \mathop{\mathrm{arg\,max}}_{\pi}\mathbb{E}_{x, y_w \sim \mathcal{D}}\left[\log \pi(y_w \mid x)\right]}$.
This procedure helps mitigate the distribution shift between the true
reference distribution which is unavailable, and $\pi_\text{ref}$ used
by DPO. Further details related to the implementation and
hyperparameters can be found in
Appendix [9](#app:implementation){reference-type="ref"
reference="app:implementation"}.

# Theoretical Analysis of DPO

In this section, we give further interpretation of the DPO method,
provide theoretical backing, and relate advantages of DPO to issues with
actor critic algorithms used for RLHF (such as
PPO [@schulman2017proximal]).

[]{#sec:theory label="sec:theory"}

## Your Language Model Is Secretly a Reward Model

DPO is able to bypass both fitting an explicit reward and performing RL
to learn the policy using a single maximum likelihood objective. Note
the optimization objective Eq.
[\[eq:main_eq\]](#eq:main_eq){reference-type="ref"
reference="eq:main_eq"} is equivalent to a Bradley-Terry model with a
reward parameterization
$r^*(x, y) = \beta \log\frac{\pi^*_\theta(y \mid x)}{\pi_\text{ref}(y \mid x)}$
and we optimize our parametric model $\pi_{\theta}$, equivalently to the
reward model optimization in Eq.
[\[eq:reward_model\]](#eq:reward_model){reference-type="ref"
reference="eq:reward_model"} under the change of variables. In this
section we will build the theory behind this reparameterization, show
that it does not constrain the class of learned reward models, and
allows for the exact recovery of the optimal policy. We begin with by
defining an equivalence relation between reward functions.

::: definition
**Definition 1**. *We say that two reward functions $r(x, y)$ and
$r'(x, y)$ are equivalent iff ${r(x, y)-r'(x, y) = f(x)}$ for some
function $f$.*
:::

It is easy to see that this is indeed an equivalence relation, which
partitions the set of reward functions into classes. We can state the
following two lemmas:

::: {#lemma:same_prefrence .lemma}
**Lemma 1**. *Under the Plackett-Luce, and in particular the
Bradley-Terry, preference framework, two reward functions from the same
class induce the same preference distribution.*
:::

::: {#lemma:same_policy .lemma}
**Lemma 2**. *Two reward functions from the same equivalence class
induce the same optimal policy under the constrained RL problem.*
:::

The proofs are straightforward and we defer them to Appendix
[8.5](#app:lemma1){reference-type="ref" reference="app:lemma1"}. The
first lemma is a well-known under-specification issue with the
Plackett-Luce family of models [@plackett1975analysis]. Due to this
under-specification, we usually have to impose additional
identifiability constraints to achieve any guarantees on the MLE
estimates from Eq.
[\[eq:reward_model\]](#eq:reward_model){reference-type="ref"
reference="eq:reward_model"} [@bong2022generalized]. The second lemma
states that all reward functions from the same class yield the same
optimal policy, hence for our final objective, we are only interested in
recovering an arbitrary reward function from the optimal class. We prove
the following Theorem in Appendix [8.6](#app:thm1){reference-type="ref"
reference="app:thm1"}:

::: {#thm:main .theorem}
**Theorem 1**. *Under mild assumptions, all reward classes consistent
with the Plackett-Luce (and Bradley-Terry in particular) models can be
represented with the reparameterization
${r(x, y) = \beta \log \frac{\pi(y\mid x)}{\pi_\text{ref}(y\mid x)}}$
for some model $\pi(y\mid x)$ and a given reference model
$\pi_\text{ref}(y \mid x)$.*
:::

Consider any reward function $r(x, y)$, which induces a corresponding
optimal model $\pi_r(y \mid x)$, specified by Eq.
[\[eq:op_policy\]](#eq:op_policy){reference-type="ref"
reference="eq:op_policy"}. We will show that a reward function from the
equivalence class of $r$ can be represented using the reparameterization
given above. We define the projection $f$ as
$$f(r; \pi_\text{ref}, \beta)(x, y) = r(x, y) - \beta\log\sum_{y}\pi_\text{ref}(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)$$
The operator $f$ simply normalizes the reward function with the
logarithm of the partition function of $\pi_r$. Since the added
normalization term is only a function of the prefix $x$,
$f(r; \pi_\text{ref}, \beta)(x, y)$ is a reward function in the
equivalence class of $r(x, y)$. Finally, replacing $r$ with the RHS of
Eq. [\[eq:main_eq\]](#eq:main_eq){reference-type="ref"
reference="eq:main_eq"} (which holds for any reward function), we have
$f(r; \pi_\text{ref}, \beta)(x, y) = \beta \log \frac{\pi_r(y\mid x)}{\pi_\text{ref}(y\mid x)}$.
That is, the projection $f$ produces a member of the equivalence class
of $r$ with the desired form, and we do not lose any generality in our
reward model from the proposed reparameterization. We can alternatively
view Theorem [1](#thm:main){reference-type="ref" reference="thm:main"}
as specifying exactly which reward function within each equivalence
class the DPO reparameterization selects, that is, the reward function
satisfying: $$\label{eq:lag_p}
     \sum_{y}\underbrace{\pi_\text{ref}(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)}_{=\pi(y\mid x)\text{, using Thm.~\ref{thm:main} reparam.}} = 1,$$
i.e., $\pi(y\mid x)$ is a valid distribution (probabilities are positive
and sum to 1). However, following
Eq. [\[eq:op_policy\]](#eq:op_policy){reference-type="ref"
reference="eq:op_policy"}, we can see that
Eq. [\[eq:lag_p\]](#eq:lag_p){reference-type="ref" reference="eq:lag_p"}
is the partition function of the optimal policy induced by the reward
function $r(x, y)$. The key insight of the DPO algorithm is that we can
impose certain constraints on the under-constrained Plackett-Luce (and
Bradley-Terry in particular) family of preference models, such that we
preserve the class of representable reward models, but explicitly make
the optimal policy in Eq.
[\[eq:op_policy\]](#eq:op_policy){reference-type="ref"
reference="eq:op_policy"} analytically tractable for all prompts $x$.

## Instability of Actor-Critic Algorithms

We can also use our framework to diagnose instabilities with standard
actor-critic algorithms used for the RLHF, such as PPO. We follow the
RLHF pipeline and focus on the RL fine-tuning step outlined in Section
[3](#section:prelims){reference-type="ref" reference="section:prelims"}.
We can draw connections to the control as inference framework
[@levine2018reinforcement] for the constrained RL problem outlined in
[\[eq:RL\]](#eq:RL){reference-type="ref" reference="eq:RL"}. We assume a
parameterized model $\pi_{\theta}(y\mid x)$ and minimize
$\mathbb{D}_{\text{KL}}[\pi_{\theta}(y|x) \mid \mid \pi^*(y\mid x)]$
where $\pi^*$ is the optimal policy from Eq.
[\[eq:optimum_model\]](#eq:optimum_model){reference-type="ref"
reference="eq:optimum_model"} induced by the reward function
$r_{\phi}(y, x)$. With some algebra this leads to the optimization
objective: $$\label{eq:AC}
    \max_{\pi_{\theta}}\mathbb{E}_{\pi_{\theta}(y\mid x)}\bigg[\underbrace{r_{\phi}(x, y) -\beta\log\sum_{y}\pi_\text{ref}(y\mid x)\exp\left(\frac{1}{\beta}r_{\phi}(x, y)\right)}_{f(r_{\phi}, \pi_\text{ref}, \beta)} - \underbrace{\beta\log\frac{\pi_{\theta}(y\mid x)}{\pi_\text{ref}(y\mid x)}}_{\text{KL}}\bigg]$$
This is the same objective optimized in prior works
[@ziegler2020finetuning; @stiennon2022learning; @bai2022training; @ouyang2022training]
using the DPO-equivalent reward for the reward class of $r_{\phi}$. In
this setting, we can interpret the normalization term in
$f(r_{\phi}, \pi_\text{ref}, \beta)$ as the soft value function of the
reference policy $\pi_\text{ref}$. While this term does not affect the
optimal solution, without it, the policy gradient of the objective could
have high variance, making learning unstable. We can accommodate for the
normalization term using a learned value function, but that can also be
difficult to optimize. Alternatively, prior works have normalized
rewards using a human completion baseline, essentially a single sample
Monte-Carlo estimate of the normalizing term. In contrast the DPO
reparameterization yields a reward function that does not require any
baselines.

# Experiments

In this section, we empirically evaluate DPO's ability to train policies
directly from preferences. First, in a well-controlled text-generation
setting, we ask: how efficiently does DPO trade off maximizing reward
and minimizing KL-divergence with the reference policy, compared to
common preference learning algorithms such as PPO? Next, we evaluate
DPO's performance on larger models and more difficult RLHF tasks,
including summarization and dialogue. We find that with almost no tuning
of hyperparameters, DPO tends to perform as well or better than strong
baselines like RLHF with PPO as well as returning the best of $N$
sampled trajectories under a learned reward function. Before presenting
these results, we describe the experimental set-up; additional details
are in Appendix [10](#app:exp_details){reference-type="ref"
reference="app:exp_details"}.

**Tasks.** Our experiments explore three different open-ended text
generation tasks. For all experiments, algorithms learn a policy from a
dataset of preferences
$\mathcal{D}=\bigl\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\bigr\}_{i=1}^N$. In
**controlled sentiment generation**, $x$ is a prefix of a movie review
from the IMDb dataset [@maas-EtAl:2011:ACL-HLT2011], and the policy must
generate $y$ with positive sentiment. In order to perform a controlled
evaluation, for this experiment we *generate* preference pairs over
generations using a pre-trained sentiment classifier, where
$p(\text{positive}\mid x,y_w)>p(\text{positive}\mid x,y_l)$. For SFT, we
fine-tune GPT-2-large until convergence on reviews from the train split
of the IMDB dataset (further details in
App [10.1](#app:sentiment_details){reference-type="ref"
reference="app:sentiment_details"}). In **summarization**, $x$ is a
forum post from Reddit; the policy must generate a summary $y$ of the
main points in the post. Following prior work, we use the Reddit TL;DR
summarization dataset [@volske-etal-2017-tl] along with human
preferences gathered by @stiennon2022learning. We use an SFT model
fine-tuned on human-written forum post summaries[^2] with the TRLX
[@leandro_von_werra_2023_7790115] framework for RLHF. The human
preference dataset was gathered by @stiennon2022learning on samples from
a different, but similarly-trained, SFT model. Finally, in **single-turn
dialogue**, $x$ is a human query, which may be anything from a question
about astrophysics to a request for relationship advice. A policy must
produce an engaging and helpful response $y$ to a user's query; we use
the Anthropic Helpful and Harmless dialogue dataset [@bai2022training],
containing 170k dialogues between a human and an automated assistant.
Each transcript ends with a pair of responses generated by a large
(although unknown) language model along with a preference label denoting
the human-preferred response. In this setting, no pre-trained SFT model
is available; we therefore fine-tune an off-the-shelf language model on
only the preferred completions to form the SFT model.

![image](figures/results/frontier.pdf){width="50%"}
![image](figures/results/tldr_winrate_vs_temp.pdf){width="49%"}

[]{#fig:frontier-tldr-main label="fig:frontier-tldr-main"}

**Evaluation.** Our experiments use two different approaches to
evaluation. In order to analyze the effectiveness of each algorithm in
optimizing the constrained reward maximization objective, in the
controlled sentiment generation setting we evaluate each algorithm by
its frontier of achieved reward and KL-divergence from the reference
policy; this frontier is computable because we have acccess to the
ground-truth reward function (a sentiment classifier). However, in the
real world, the ground truth reward function is not known; therefore, we
evaluate algorithms with their *win rate* against a baseline policy,
using GPT-4 as a proxy for human evaluation of summary quality and
response helpfulness in the summarization and single-turn dialogue
settings, respectively. For summarization, we use reference summaries in
the test set as the baseline; for dialogue, we use the preferred
response in the test dataset as the baseline. While existing studies
suggest LMs can be better automated evaluators than existing metrics
[@Chen2023ExploringTU], we conduct a human study to justify our usage of
GPT-4 for evaluation in
Sec. [6.4](#sec:human-judgments){reference-type="ref"
reference="sec:human-judgments"}. We find GPT-4 judgments correlate
strongly with humans, with human agreement with GPT-4 typically similar
or higher than inter-human annotator agreement.

**Methods.** In addition to DPO, we evaluate several existing approaches
to training language models to adhere to human preferences. Most simply,
we explore zero-shot prompting with **GPT-J** [@gpt-j] in the
summarization task and 2-shot prompting with **Pythia-2.8B**
[@biderman2023pythia] in the dialogue task. In addition, we evaluate the
**SFT** model as well as **Preferred-FT**, which is a model fine-tuned
with supervised learning on the chosen completion $y_w$ from either the
SFT model (in controlled sentiment and summarization) or a generic LM
(in single-turn dialogue). Another pseudo-supervised method is
**Unlikelihood** [@welleck2019neural], which simply optimizes the policy
to maximize the probability assigned to $y_w$ and *minimize* the
probability assigned to $y_l$; we use an optional coefficient
$\alpha\in[0,1]$ on the 'unlikelihood' term. We also consider **PPO**
[@schulman2017proximal] using a reward function learned from the
preference data and **PPO-GT**, which is an oracle that learns from the
ground truth reward function available in the controlled sentiment
setting. In our sentiment experiments, we use two implementations of
PPO-GT, one of-the-shelf version [@leandro_von_werra_2023_7790115] as
well as a modified version that normalizes rewards and further tunes
hyperparameters to improve performance (we also use these modifications
when running 'normal' PPO with learned rewards). Finally, we consider
the **Best of $N$** baseline, sampling $N$ responses from the SFT model
(or Preferred-FT in dialogue) and returning the highest-scoring response
according to a reward function learned from the preference dataset. This
high-performing method decouples the quality of the reward model from
the PPO optimization, but is computationally impractical even for
moderate $N$ as it requires sampling $N$ completions for every query at
test time.

## How well can DPO optimize the RLHF objective?

![image](figures/results/dialogue_winrate_vs_temp.pdf){width="50%"}
![image](figures/results/dialogue_winrate_vs_steps.pdf){width="49%"}

[]{#fig:dialogue-main label="fig:dialogue-main"}

The KL-constrained reward maximization objective used in typical RLHF
algorithms balances exploitation of reward while restricting the policy
from deviating far from the reference policy. Therefore, when comparing
algorithms, we must take into account both reward achieved as well as
the KL discrepancy; achieving slightly higher reward but with much
higher KL is not necessarily desirable.
Figure [\[fig:frontier-tldr-main\]](#fig:frontier-tldr-main){reference-type="ref"
reference="fig:frontier-tldr-main"} shows the reward-KL frontier for
various algorithms in the sentiment setting. We execute multiple
training runs for each algorithm, using a different hyperparameter for
policy conservativeness in each run (target KL $\in\{3,6,9,12\}$ for
PPO, $\beta \in \{0.05,0.1,1,5\}$, $\alpha\in\{0.05,0.1,0.5,1\}$ for
unlikelihood, random seeds for preferred-FT). This sweep includes 22
runs in total. After each 100 training steps until convergence, we
evaluate each policy on a set of test prompts, computing the average
reward under the true reward function as well as the average
sequence-level KL[^3] with the reference policy
$\text{KL}\left(\pi\mid \mid \pi_\text{ref}\right)$. We find that DPO
produces by far the most efficient frontier, achieving the highest
reward while still achieving low KL. This result is particularly notable
for multiple reasons. First, DPO and PPO optimize the same objective,
but DPO is notably more efficient; DPO's reward/KL tradeoff strictly
dominates PPO. Second, DPO achieves a better frontier than PPO, *even
when PPO can access ground truth rewards* (PPO-GT).

## Can DPO scale to real preference datasets? {#sec:dpo-real-datasets}

Next, we evaluate fine-tuning performance of DPO on summarization and
single-turn dialogue. For summarization, automatic evaluation metrics
such as ROUGE can be poorly correlated with human
preferences [@stiennon2022learning], and prior work has found that
fine-tuning LMs using PPO on human preferences to provide more effective
summaries. We evaluate different methods by sampling completions on the
test split of TL;DR summarization dataset, and computing the average win
rate against reference completions in the test set. The completions for
all methods are sampled at temperatures varying from 0.0 to 1.0, and the
win rates are shown in
Figure [\[fig:frontier-tldr-main\]](#fig:frontier-tldr-main){reference-type="ref"
reference="fig:frontier-tldr-main"} (right). DPO, PPO and Preferred-FT
all fine-tune the same GPT-J SFT model[^4]. We find that DPO has a win
rate of approximately 61% at a temperature of 0.0, exceeding the
performance of PPO at  57% at its optimal sampling temperature of 0.0.
DPO also achieves a higher maximum win rate compared to the best of $N$
baseline. We note that we did not meaningfully tune DPO's $\beta$
hyperparameter, so these results may underestimate DPO's potential.
Moreover, we find DPO to be much more robust to the sampling temperature
than PPO, the performance of which can degrade to that of the base GPT-J
model at high temperatures. Preferred-FT does not improve significantly
over the SFT model. We also compare DPO and PPO head-to-head in human
evaluations in Section [6.4](#sec:human-judgments){reference-type="ref"
reference="sec:human-judgments"}, where DPO samples at temperature 0.25
were preferred 58% times over PPO samples at temperature 0.

On single-turn dialogue, we evaluate the different methods on the subset
of the test split of the Anthropic HH dataset [@bai2022training] with
one step of human-assistant interaction. GPT-4 evaluations use the
preferred completions on the test as the reference to compute the win
rate for different methods. As there is no standard SFT model for this
task, we start with a pre-trained Pythia-2.8B, use Preferred-FT to train
a reference model on the chosen completions such that completions are
within distribution of the model, and then train using DPO. We also
compare against the best of 128 Preferred-FT completions (we found the
Best of $N$ baseline plateaus at 128 completions for this task; see
Appendix Figure [\[fig:best-of-n\]](#fig:best-of-n){reference-type="ref"
reference="fig:best-of-n"}) and a 2-shot prompted version of the
Pythia-2.8B base model, finding DPO performs as well or better for the
best-performing temperatures for each method. We also evaluate an RLHF
model trained with PPO on the Anthropic HH dataset [^5] from a
well-known source [^6], but are unable to find a prompt or sampling
temperature that gives performance better than the base Pythia-2.8B
model. Based on our results from TL;DR and the fact that both methods
optimize the same reward function, we consider Best of 128 a rough proxy
for PPO-level performance. Overall, DPO is the only computationally
efficient method that improves over the preferred completions in the
Anthropic HH dataset, and provides similar or better performance to the
computationally demanding Best of 128 baseline. Finally,
Figure [\[fig:dialogue-main\]](#fig:dialogue-main){reference-type="ref"
reference="fig:dialogue-main"} shows that DPO converges to its best
performance relatively quickly.

## Generalization to a new input distribution

::: wraptable
r0.375

  ------------------ ------------------------------- -------------
                      **Win rate vs. ground truth**  
   (lr)2-3 **Alg.**             Temp $0$              Temp $0.25$
         DPO                      0.36                   0.31
         PPO                      0.26                   0.23
  ------------------ ------------------------------- -------------

[]{#tab:ood label="tab:ood"}
:::

To further compare the performance of PPO and DPO under distribution
shifts, we evaluate the PPO and DPO policies from our Reddit TL;DR
summarization experiment on a different distribution, news articles in
the test split of the CNN/DailyMail dataset
[@nallapati-etal-2016-abstractive], using the best sampling temperatures
from TL;DR (0 and 0.25). The results are presented in
Table [\[tab:ood\]](#tab:ood){reference-type="ref" reference="tab:ood"}.
We computed the GPT-4 win rate against the ground-truth summaries in the
datasets, using the same GPT-4 (C) prompt we used for Reddit TL;DR, but
replacing the words "forum post" with "news article". For this new
distribution, DPO continues to outperform the PPO policy by a
significant margin. This experiment provides initial evidence that DPO
policies can generalize similarly well to PPO policies, even though DPO
does not use the additional unlabeled Reddit TL;DR prompts that PPO
uses.

## Validating GPT-4 judgments with human judgments {#sec:human-judgments}

We conduct a human study to verify the reliability of GPT-4's judgments,
using the results of the TL;DR summarization experiment and two
different GPT-4 prompts. The **GPT-4 (S)** (simple) prompt simply asks
for which summary better-summarizes the important information in the
post. The **GPT-4 (C)** (concise) prompt also asks for which summary is
more concise; we evaluate this prompt because we find that GPT-4 prefers
longer, more repetitive summaries than humans do with the **GPT-4 (S)**
prompt. See Appendix [10.2](#app:prompts){reference-type="ref"
reference="app:prompts"} for the complete prompts. We perform three
comparisons, using the highest (DPO, temp. 0.25), the lowest (PPO, temp.
1.0), and a

::: wraptable
r0.47

  ----------------------- --------- --------- -----------
                           **DPO**   **SFT**   **PPO-1**
  (lr)2-4 N respondents      272       122        199
  GPT-4 (S) win %            47        27         13
  GPT-4 (C) win %            54        32         12
  Human win %                58        43         17
  GPT-4 (S)-H agree          70        77         86
  GPT-4 (C)-H agree          67        79         85
  H-H agree                  65        \-         87
  ----------------------- --------- --------- -----------

[]{#tab:human_results label="tab:human_results"}
:::

middle-performing (SFT, temp. 0.25) method with the aim of covering a
diversity of sample qualities; all three methods are compared against
greedily-sampled PPO (its best-performing temperature). We find that
with both prompts, GPT-4 tends to agree with humans about as often as
humans agree with each other, suggesting that GPT-4 is a reasonable
proxy for human evaluations (due to limited human raters, we only
collect multiple human judgments for the DPO and PPO-1 comparisons).
Overall, the **GPT-4 (C)** prompt generally provides win rates more
representative of humans; we therefore use this prompt for the main
results in Section [6.2](#sec:dpo-real-datasets){reference-type="ref"
reference="sec:dpo-real-datasets"}. For additional details about the
human study, including the web interface presented to raters and the
list of human volunteers, see
Appendix [11.3](#app:human-study){reference-type="ref"
reference="app:human-study"}.

# Discussion

Learning from preferences is a powerful, scalable framework for training
capable, aligned language models. We have introduced DPO, a simple
training paradigm for training language models from preferences without
reinforcement learning. Rather than coercing the preference learning
problem into a standard RL setting in order to use off-the-shelf RL
algorithms, DPO identifies a mapping between language model policies and
reward functions that enables training a language model to satisfy human
preferences *directly*, with a simple cross-entropy loss, without
reinforcement learning or loss of generality. With virtually no tuning
of hyperparameters, DPO performs similarly or better than existing RLHF
algorithms, including those based on PPO; DPO thus meaningfully reduces
the barrier to training more language models from human preferences.

**Limitations & Future Work.** Our results raise several important
questions for future work. How does the DPO policy generalize out of
distribution, compared with learning from an explicit reward function?
Our initial results suggest that DPO policies can generalize similarly
to PPO-based models, but more comprehensive study is needed. For
example, can training with self-labeling from the DPO policy similarly
make effective use of unlabeled prompts? On another front, how does
reward over-optimization manifest in the direct preference optimization
setting, and is the slight decrease in performance in
Figure [\[fig:dialogue-main\]](#fig:dialogue-main){reference-type="ref"
reference="fig:dialogue-main"}-right an instance of it? Additionally,
while we evaluate models up to 6B parameters, exploration of scaling DPO
to state-of-the-art models orders of magnitude larger is an exciting
direction for future work. Regarding evaluations, we find that the win
rates computed by GPT-4 are impacted by the prompt; future work may
study the best way to elicit high-quality judgments from automated
systems. Finally, many possible applications of DPO exist beyond
training language models from human preferences, including training
generative models in other modalities.

# Acknowledgements {#acknowledgements .unnumbered}

EM gratefully acknowledges funding from a Knight-Hennessy Graduate
Fellowship. CF and CM are CIFAR Fellows. This work was supported in part
by the Stanford Accelerator for Learning (SAL) and Stanford Institute
for Human-Centered Artificial Intelligence (HAI) *Generative AI for the
Future of Learning* seed grant program. The Stanford Center for Research
on Foundation Models (CRFM) provided part of the compute resources used
for the experiments in this work. This work was supported in part by ONR
grant N00014-20-1-2675.

# Author Contributions {#author-contributions .unnumbered}

**All authors** provided valuable contributions to designing, analyzing,
and iterating on experiments, writing and editing the paper, and
generally managing the project's progress.

**RR** proposed using autoregressive reward models in discussions with
**EM**; derived the DPO objective; proved the theoretical properties of
the algorithm and wrote the relevant sections and appendices. He also
suggested and helped with organizing experiments and contributed some of
the PPO and reward learning baselines.

**AS** initiated the discussion on using weighted regression methods as
an alternative to PPO; initiated project-related organization, wrote
initial analysis connecting DPO with weighted regression and
unlikelihood; design and iterations of DPO + baseline implementations,
initial exploratory experiments for DPO; substantial experiment
organization and design (datasets, baselines, evaluation); led model
training and evaluation for controlled sentiment generation and
summarization; design iterations for GPT-4 evaluation (particularly
summarization); substantial writing contributions to abstract,
prelims/method and experiments; editing contributions to other sections.

**EM** provided input on early discussions on learning autoregressive
reward functions; wrote the first implementation of DPO and ran the
first DPO experiments; trained the large-scale (summarization and
dialogue) DPO models used in paper experiments; conducted initial GPT-4
win rate evaluations and set up related infrastructure; recruited
participants for, conducted, and analyzed results from the human study;
wrote the abstract, introduction, related work, discussion, and most of
experiments; and assisted with editing the rest of the paper.

**CF, CM, & SE** supervised the research, suggested ideas and
experiments, and assisted in writing the paper.

# Mathematical Derivations

## Deriving the Optimum of the KL-Constrained Reward Maximization Objective

In this appendix, we will derive Eq.
[\[eq:op_policy\]](#eq:op_policy){reference-type="ref"
reference="eq:op_policy"}. Analogously to Eq.
[\[eq:RL\]](#eq:RL){reference-type="ref" reference="eq:RL"}, we optimize
the following objective:
$$\max_{\pi}  \mathbb{E}_{x\sim \mathcal{D}, y\sim \pi}\bigl[r(x, y)\bigr] - \beta\mathbb{D}_{\textrm{KL}}\bigl[\pi(y|x)||\pi_\text{ref}(y|x)\bigr]$$
under any reward function $r(x,y)$, reference model $\pi_\text{ref}$ and
a general non-parametric policy class. We now have: $$\begin{aligned}
\label{eq:RL_proof}
\max_{\pi}  \mathbb{E}_{x\sim \mathcal{D}, y\sim \pi}&\bigl[r(x, y)\bigr] - \beta\mathbb{D}_{\textrm{KL}}\bigl[\pi(y|x)\mid\mid\pi_\text{ref}(y|x)\bigr] \nonumber\\
&=\max_{\pi}  \mathbb{E}_{x\sim \mathcal{D}}\mathbb{E}_{y\sim \pi(y|x)}\left[r(x, y) - \beta\log\frac{\pi(y|x)}{\pi_\text{ref}(y|x)}\right] \nonumber\\&=
\min_{\pi}  \mathbb{E}_{x\sim \mathcal{D}}\mathbb{E}_{y\sim \pi(y|x)}\left[\log\frac{\pi(y|x)}{\pi_\text{ref}(y|x)} - \frac{1}{\beta}r(x, y)\right] \nonumber\\ &=
\min_{\pi}  \mathbb{E}_{x\sim \mathcal{D}}\mathbb{E}_{y\sim \pi(y|x)}\left[\log\frac{\pi(y|x)}{\frac{1}{Z(x)}\pi_\text{ref}(y|x)\exp\left(\frac{1}{\beta}r(x, y)\right)} - \log Z(x)\right]\end{aligned}$$

where we have partition function:
$$Z(x) = \sum_{y}\pi_\text{ref}(y|x)\exp\left(\frac{1}{\beta}r(x, y)\right).$$

Note that the partition function is a function of only $x$ and the
reference policy $\pi_\text{ref}$, but does not depend on the policy
$\pi$. We can now define
$$\pi^*(y|x) = \frac{1}{Z(x)}\pi_\text{ref}(y|x)\exp\left(\frac{1}{\beta}r(x, y)\right),$$

which is a valid probability distribution as $\pi^*(y|x)\geq 0$ for all
$y$ and $\sum_{y}\pi^*(y|x)=1$. Since $Z(x)$ is not a function of $y$,
we can then re-organize the final objective in Eq
[\[eq:RL_proof\]](#eq:RL_proof){reference-type="ref"
reference="eq:RL_proof"} as: $$\begin{aligned}
\min_{\pi}  \mathbb{E}_{x\sim \mathcal{D}}\left[\mathbb{E}_{y\sim \pi(y|x)}\left[\log\frac{\pi(y|x)}{\pi^*(y|x)}\right] - \log Z(x)\right]=\\
\min_{\pi}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{D}_{\text{KL}}(\pi(y|x)\mid\mid\pi^*(y|x)) - \log Z(x)\right]\end{aligned}$$
Now, since $Z(x)$ does not depend on $\pi$, the minimum is achieved by
the policy that minimizes the first KL term. Gibbs' inequality tells us
that the KL-divergence is minimized at 0 if and only if the two
distributions are identical. Hence we have the optimal solution:
$$\pi(y|x)= \pi^*(y|x) = \frac{1}{Z(x)}\pi_\text{ref}(y|x)\exp\left(\frac{1}{\beta}r(x, y)\right)$$
for all $x\in\mathcal{D}$. This completes the derivation.

[]{#app:derivation1 label="app:derivation1"}

## Deriving the DPO Objective Under the Bradley-Terry Model {#app:derivation2}

It is straightforward to derive the DPO objective under the
Bradley-Terry preference model as we have $$\label{eq:BT_restated}
    p^*(y_1\succ y_2|x)=\frac{\exp\left(r^*(x, y_1)\right)}{\exp\left(r^*(x, y_1)\right) + \exp\left(r^*(x, y_2)\right)}$$

In Section [4](#sec:DPO){reference-type="ref" reference="sec:DPO"} we
showed that we can express the (unavailable) ground-truth reward through
its corresponding optimal policy: $$\label{eq:main_eq_restated}
    r^*(x,y) =\beta \log \frac{\pi^*(y|x)}{\pi_\text{ref}(y|x)} + \beta \log Z(x)$$

Substituting Eq.
[\[eq:main_eq_restated\]](#eq:main_eq_restated){reference-type="ref"
reference="eq:main_eq_restated"} into Eq.
[\[eq:BT_restated\]](#eq:BT_restated){reference-type="ref"
reference="eq:BT_restated"} we obtain: $$\begin{aligned}
    p^*(y_1\succ y_2|x)&=\frac{\exp\left(\beta \log \frac{\pi^*(y_1|x)}{\pi_\text{ref}(y_1|x)} + \beta \log Z(x)\right)}{\exp\left(\beta \log \frac{\pi^*(y_1|x)}{\pi_\text{ref}(y_1|x)} + \beta \log Z(x)\right) + \exp\left(\beta \log \frac{\pi^*(y_2|x)}{\pi_\text{ref}(y_2|x)} + \beta \log Z(x)\right)}\\ &=
    \frac{1}{1+\exp\left(\beta \log \frac{\pi^*(y_2|x)}{\pi_\text{ref}(y_2|x)}-\beta \log \frac{\pi^*(y_1|x)}{\pi_\text{ref}(y_1|x)}\right)} \\&= \sigma\left(\beta \log \frac{\pi^*(y_1|x)}{\pi_\text{ref}(y_1|x)} - \beta \log \frac{\pi^*(y_2|x)}{\pi_\text{ref}(y_2|x)}\right).\end{aligned}$$

The last line is the per-instance loss in
Equation [\[eq:optimum_model\]](#eq:optimum_model){reference-type="ref"
reference="eq:optimum_model"}.

## Deriving the DPO Objective Under the Plackett-Luce Model {#app:plackett_luce_models}

The Plackett-Luce model [@plackett1975analysis; @luce2012individual] is
a generalization of the Bradley-Terry model over rankings (rather than
just pair-wise comparisons). Similar to to the Bradley-Terry model, it
stipulates that when presented with a set of possible choices, people
prefer a choice with probability proportional to the value of some
latent reward function for that choice. In our context, when presented
with a prompt $x$ and a set of $K$ answers $y_1, \ldots, y_K$ a user
would output a permutation $\tau:[K]\to[K]$, giving their ranking of the
answers. The Plackett-Luce model stipulates that $$\label{eq:pl-model}
    p^*(\tau| y_1,\ldots, y_K, x)= \prod_{k=1}^{K}\frac{\exp(r^*(x, y_{\tau(k)}))}{\sum_{j=k}^{K}\exp(r^*(x, y_{\tau(j)}))}$$

Notice that when $K=2$,
Equation [\[eq:pl-model\]](#eq:pl-model){reference-type="ref"
reference="eq:pl-model"} reduces to the Bradley-Terry model. However,
for the general Plackett-Luce model, we can still utilize the results of
Eq. [\[eq:main_eq\]](#eq:main_eq){reference-type="ref"
reference="eq:main_eq"} and substitute the reward function parameterized
by its optimal policy. Similarly to Appendix
[8.2](#app:derivation2){reference-type="ref"
reference="app:derivation2"}, the normalization constant $Z(x)$ cancels
out and we're left with:
$$p^*(\tau| y_1,\ldots, y_K, x)= \prod_{k=1}^{K}\frac{\exp\left(\beta \log \frac{\pi^*(y_{\tau(k)}|x)}{\pi_\text{ref}(y_{\tau(k)}|x)}\right)}{\sum_{j=k}^{K}\exp\left(\beta \log \frac{\pi^*(y_{\tau(j)}|x)}{\pi_\text{ref}(y_{\tau(j)}|x)}\right)}$$

Similarly to the approach of Section [4](#sec:DPO){reference-type="ref"
reference="sec:DPO"}, if we have access to a dataset
$\mathcal{D} = \{\tau^{(i)}, y_1^{(i)}, \ldots, y_K^{(i)}, x^{(i)}\}_{i=1}^N$
of prompts and user-specified rankings, we can use a parameterized model
and optimize this objective with maximum-likelihood.:
$$\mathcal{L}_{\text{DPO}}(\pi_{\theta}, \pi_\text{ref}) = -\mathbb{E}_{\tau, y_1, \ldots, y_K, x\sim\mathcal{D}}\left[\log \prod_{k=1}^{K}\frac{\exp\left(\beta \log \frac{\pi_{\theta}(y_{\tau(k)}|x)}{\pi_\text{ref}(y_{\tau(k)}|x)}\right)}{\sum_{j=k}^{K}\exp\left(\beta \log \frac{\pi_{\theta}(y_{\tau(j)}|x)}{\pi_\text{ref}(y_{\tau(j)}|x)}\right)}\right]$$

## Deriving the Gradient of the DPO Objective {#app:gradient_derivation}

In this section we derive the gradient of the DPO objective:
$$\begin{aligned}
\label{eq:grad-start}
    \nabla_{\theta}\mathcal{L}_\text{DPO}(\pi_{\theta}; \pi_\text{ref})
    = -\nabla_{\theta}\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\left[\log \sigma \left(\beta \log \frac{\pi_{\theta}(y_l|x)}{\pi_\text{ref}(y_l|x)} - \beta \log \frac{\pi_{\theta}(y_w|x)}{\pi_\text{ref}(y_w|x)}\right)\right]\end{aligned}$$

We can rewrite the RHS of
Equation [\[eq:grad-start\]](#eq:grad-start){reference-type="ref"
reference="eq:grad-start"} as $$\begin{aligned}
    \nabla_{\theta}\mathcal{L}_\text{DPO}(\pi_{\theta}; \pi_\text{ref})
    =-\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\left[\frac{\sigma'\left(u\right)}{\sigma \left(u\right)}\nabla_{\theta}\left(u\right)\right],\end{aligned}$$
where
$u = \beta \log \frac{\pi_{\theta}(y_l|x)}{\pi_\text{ref}(y_l|x)} - \beta \log \frac{\pi_{\theta}(y_w|x)}{\pi_\text{ref}(y_w|x)}$.

Using the properties of sigmoid function
$\sigma'(x) = \sigma(x)(1-\sigma(x))$ and $\sigma(-x) = 1-\sigma(x)$, we
obtain the final gradient $$\begin{gathered}
\nabla_{\theta}\mathcal{L}_\text{DPO}(\pi_{\theta}; \pi_\text{ref}) = \\
     -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \bigg[\beta\sigma \left(\beta \log \frac{\pi_{\theta}(y_w|x)}{\pi_\text{ref}(y_w|x)} - \beta \log \frac{\pi_{\theta}(y_l|x)}{\pi_\text{ref}(y_l|x)}\right)\bigg[\nabla_\theta\log \pi(y_w \mid x) - \nabla_\theta\log\pi(y_l \mid x)\bigg]\bigg],\end{gathered}$$

After using the reward substitution of
$\hat{r}_\theta(x, y) = \beta \log \frac{\pi_\theta(y \mid x)}{\pi_\text{ref}(y \mid x)}$
we obtain the final form of the gradient from Section
[4](#sec:DPO){reference-type="ref" reference="sec:DPO"}.

## Proof of Lemma 1 and 2 {#app:lemma1}

In this section, we will prove the two lemmas from Section
[\[sec:theory\]](#sec:theory){reference-type="ref"
reference="sec:theory"}.

::: em
**Lemma 1 Restated.** Under the Plackett-Luce preference framework, and
in particular the Bradley-Terry framework, two reward functions from the
same equivalence class induce the same preference distribution.
:::

::: proof
*Proof.* We say that two reward functions $r(x, y)$ and $r'(x, y)$ are
from the same equivalence class if $r'(x, y) = r(x, y) + f(x)$ for some
function $f$. We consider the general Plackett-Luce (with the
Bradley-Terry model a special case for $K=2$) and denote the probability
distribution over rankings induced by a particular reward function
$r(x, y)$ as $p_r$. For any prompt $x$, answers $y_1,\ldots, y_K$ and
ranking $\tau$ we have: $$\begin{aligned}
    p_{r'}(\tau| y_1,\ldots, y_K, x) &= 
     \prod_{k=1}^{K}\frac{\exp(r'(x, y_{\tau(k)}))}{\sum_{j=k}^{K}\exp(r'(x, y_{\tau(j)}))} \\
     &= \prod_{k=1}^{K}\frac{\exp(r(x, y_{\tau(k)}) + f(x))}{\sum_{j=k}^{K}\exp(r(x, y_{\tau(j)})+f(x))} \\
     &= \prod_{k=1}^{K}\frac{\exp(f(x))\exp(r(x, y_{\tau(k)}))}{\exp(f(x))\sum_{j=k}^{K}\exp(r(x, y_{\tau(j)}))} \\
     &= \prod_{k=1}^{K}\frac{\exp(r(x, y_{\tau(k)}))}{\sum_{j=k}^{K}\exp(r(x, y_{\tau(j)}))} \\
     &= p_{r}(\tau| y_1,\ldots, y_K, x),\end{aligned}$$ which completes
the proof. ◻
:::

::: em
**Lemma 2 Restated.** Two reward functions from the same equivalence
class induce the same optimal policy under the constrained RL problem.
:::

::: proof
*Proof.* Let us consider two reward functions from the same class, such
that $r'(x, y)=r(x, y)+f(x)$ and, let us denote as $\pi_r$ and
$\pi_{r'}$ the corresponding optimal policies. By Eq.
[\[eq:op_policy\]](#eq:op_policy){reference-type="ref"
reference="eq:op_policy"}, for all $x, y$ we have $$\begin{aligned}
    \pi_{r'}(y|x) &= \frac{1}{\sum_{y}\pi_\text{ref}(y|x)\exp\left(\frac{1}{\beta}r'(x, y)\right)}\pi_\text{ref}(y|x)\exp\left(\frac{1}{\beta}r'(x, y)\right) \\
    &= \frac{1}{\sum_{y}\pi_\text{ref}(y|x)\exp\left(\frac{1}{\beta}(r(x, y) + f(x))\right)}\pi_\text{ref}(y|x)\exp\left(\frac{1}{\beta}(r(x, y)+f(x))\right) \\
    &= \frac{1}{\exp\left(\frac{1}{\beta}f(x)\right)\sum_{y}\pi_\text{ref}(y|x)\exp\left(\frac{1}{\beta}r(x, y)\right)}\pi_\text{ref}(y|x)\exp\left(\frac{1}{\beta}r(x, y)\right)\exp\left(\frac{1}{\beta}f(x)\right) \\
    &= \frac{1}{\sum_{y}\pi_\text{ref}(y|x)\exp\left(\frac{1}{\beta}r(x, y)\right)}\pi_\text{ref}(y|x)\exp\left(\frac{1}{\beta}r(x, y)\right) \\
    &= \pi_r(y|x),\end{aligned}$$ which completes the proof. ◻
:::

## Proof of Theorem 1 {#app:thm1}

In this section, we will expand on the results of
Theorem [1](#thm:main){reference-type="ref" reference="thm:main"}.

::: em
**Theorem 1 Restated.** Assume, we have a reference model, such that
$\pi_\text{ref}(y|x)>0$ for all pairs of prompts $x$ and answers $y$ and
a parameter $\beta>0$. All reward equivalence classes, as defined in
Section [\[sec:theory\]](#sec:theory){reference-type="ref"
reference="sec:theory"} can be represented with the reparameterization
$r(x, y) = \beta \log \frac{\pi(y|x)}{\pi_\text{ref}(y|x)}$ for some
model $\pi(y|x)$.
:::

::: proof
*Proof.* Consider any reward function $r(x,y)$, which induces an optimal
model $\pi_r(y|x)$ under the KL-constrained RL problem, with solution
given by [\[eq:op_policy\]](#eq:op_policy){reference-type="ref"
reference="eq:op_policy"}. Following Eq.
[\[eq:main_eq\]](#eq:main_eq){reference-type="ref"
reference="eq:main_eq"}, when we log-linearize both sides we obtain:
$$r(x,y) =\beta \log \frac{\pi_r(y|x)}{\pi_\text{ref}(y|x)} + \beta \log Z(x)$$
where
$Z(x) =\sum_{y}\pi_\text{ref}(y|x)\exp\left(\frac{1}{\beta}r(x, y)\right)$
(notice that $Z(x)$ also depends on the reward function $r$). Using the
operator
$r'(x, y) = f(r, \pi_\text{ref}, \beta)(x, y) = r(x, y) - \beta \log Z(x)$,
we see that this new reward function is within the equivalence class of
$r$ and, we have:
$$r'(x,y) =\beta \log \frac{\pi_r(y|x)}{\pi_\text{ref}(y|x)}$$

which completes the proof. ◻
:::

We can further expand on these results. We can see that if $r$ and $r'$
are two reward functions in the same class, then
$$f(r, \pi_\text{ref}, \beta)(x, y)= \beta \log \frac{\pi_r(y|x)}{\pi_\text{ref}(y|x)}=
\beta \log \frac{\pi_r'(y|x)}{\pi_\text{ref}(y|x)} = f(r', \pi_\text{ref}, \beta)(x, y)$$
where the second equality follows from Lemma
[2](#lemma:same_policy){reference-type="ref"
reference="lemma:same_policy"}. We have proven that the operator $f$
maps all reward functions from a particular equivalence class to the
same reward function. Next, we show that for every equivalence class of
reward functions, the reward function that has the reparameterization
outlined in Theorem [1](#thm:main){reference-type="ref"
reference="thm:main"} is unique.

::: {#prop:unique .proposition}
**Proposition 1**. *Assume, we have a reference model, such that
$\pi_\text{ref}(y|x)>0$ for all pairs of prompts $x$ and answers $y$ and
a parameter $\beta>0$. Then every equivalence class of reward functions,
as defined in Section [\[sec:theory\]](#sec:theory){reference-type="ref"
reference="sec:theory"}, has a unique reward function $r(x, y)$, which
can be reparameterized as
$r(x, y) = \beta \log \frac{\pi(y|x)}{\pi_\text{ref}(y|x)}$ for some
model $\pi(y|x)$.*
:::

::: proof
*Proof.* We will proceed using proof by contradiction. Assume we have
two reward functions from the same class, such that
$r'(x, y) = r(x, y) + f(x)$. Moreover, assume that
$r'(x, y) = \beta \log \frac{\pi'(y|x)}{\pi_\text{ref}(y|x)}$ for some
model $\pi'(y|x)$ and
$r(x, y) = \beta \log \frac{\pi(y|x)}{\pi_\text{ref}(y|x)}$ for some
model $\pi(y|x)$, such that $\pi\neq\pi'$. We then have
$$r'(x, y) = r(x, y) + f(x) = \beta \log \frac{\pi(y|x)}{\pi_\text{ref}(y|x)} + f(x) =  \beta \log \frac{\pi(y|x)\exp(\frac{1}{\beta} f(x))}{\pi_\text{ref}(y|x)}=\beta \log \frac{\pi'(y|x)}{\pi_\text{ref}(y|x)}$$

for all prompts $x$ and completions $y$. Then we must have
$\pi(y|x)\exp(\frac{1}{\beta} f(x)) = \pi'(y|x)$. Since these are
distributions, summing over $y$ on both sides, we obtain that
$\exp(\frac{1}{\beta} f(x)) = 1$ and since $\beta>0$, we must have
$f(x)=0$ for all $x$. Therefore $r(x,y) = r'(x,y)$. This completes the
proof. ◻
:::

We have now shown that every reward class has a unique reward function
that can be represented as outlined in
Theorem [1](#thm:main){reference-type="ref" reference="thm:main"}, which
is given by $f(r, \pi_\text{ref}, \beta)$ for any reward function in
that class.

# DPO Implementation Details and Hyperparameters {#app:implementation}

DPO is relatively straightforward to implement; PyTorch code for the DPO
loss is provided below:

    import torch.nn.functional as F

    def dpo_loss(pi_logps, ref_logps, yw_idxs, yl_idxs, beta):
        """
        pi_logps: policy logprobs, shape (B,)
        ref_logps: reference model logprobs, shape (B,)
        yw_idxs: preferred completion indices in [0, B-1], shape (T,)
        yl_idxs: dispreferred completion indices in [0, B-1], shape (T,)
        beta: temperature controlling strength of KL penalty

        Each pair of (yw_idxs[i], yl_idxs[i]) represents the
          indices of a single preference pair.
        """

        pi_yw_logps,  pi_yl_logps =  pi_logps[yw_idxs],  pi_logps[yl_idxs]
        ref_yw_logps, ref_yl_logps = ref_logps[yw_idxs], ref_logps[yl_idxs]

        pi_logratios  = pi_yw_logps - pi_yl_logps
        ref_logratios = ref_yw_logps - ref_yl_logps

        losses = -F.logsigmoid(beta * (pi_logratios - ref_logratios))
        rewards = beta * (pi_logps - ref_logps).detach()

        return losses, rewards

Unless noted otherwise, we use a $\beta = 0.1$, batch size of `64` and
the RMSprop optimizer with a learning rate of `1e-6` by default. We
linearly warmup the learning rate from `0` to `1e-6` over `150` steps.
For TL;DR summarization, we use $\beta=0.5$, while rest of the
parameters remain the same.

# Further Details on the Experimental Set-Up {#app:exp_details}

In this section, we include additional details relevant to our
experimental design.

## IMDb Sentiment Experiment and Baseline Details {#app:sentiment_details}

The prompts are prefixes from the IMDB dataset of length 2-8 tokens. We
use the pre-trained sentiment classifier
`siebert/sentiment-roberta-large-english` as a ground-truth reward model
and `gpt2-large` as a base model. We use these larger models as we found
the default ones to generate low-quality text and rewards to be somewhat
inaccurate. We first use supervised fine-tuning on a subset of the IMDB
data for 1 epoch. We then use this model to sample 4 completions for
25000 prefixes and create 6 preference pairs for each prefix using the
ground-truth reward model. The RLHF reward model is initialized from the
`gpt2-large` model and trained for 3 epochs on the preference datasets,
and we take the checkpoint with the highest validation set accuracy. The
"TRL" run uses the hyper-parameters in the TRL library. Our
implementation uses larger batch samples of 1024 per PPO step.

## GPT-4 prompts for computing summarization and dialogue win rates {#app:prompts}

A key component of our experimental setup is GPT-4 win rate judgments.
In this section, we include the prompts used to generate win rates for
the summarization and dialogue experiments. We use `gpt-4-0314` for all
our experiments. The order of summaries or responses are randomly chosen
for every evaluation.\
**Summarization GPT-4 win rate prompt (S).**

    Which of the following summaries does a better job of summarizing the most \
    important points in the given forum post?

    Post:
    <post>

    Summary A:
    <Summary A>

    Summary B:
    <Summary B>

    FIRST provide a one-sentence comparison of the two summaries, explaining which \
    you prefer and why. SECOND, on a new line, state only "A" or "B" to indicate your \ 
    choice. Your response should use the format:
    Comparison: <one-sentence comparison and explanation>
    Preferred: <"A" or "B">

**Summarization GPT-4 win rate prompt (C).**

    Which of the following summaries does a better job of summarizing the most \ 
    important points in the given forum post, without including unimportant or \ 
    irrelevant details? A good summary is both precise and concise.

    Post:
    <post>

    Summary A:
    <Summary A>

    Summary B:
    <Summary B>

    FIRST provide a one-sentence comparison of the two summaries, explaining which \
    you prefer and why. SECOND, on a new line, state only "A" or "B" to indicate your \ 
    choice. Your response should use the format:
    Comparison: <one-sentence comparison and explanation>
    Preferred: <"A" or "B">

**Dialogue GPT-4 win rate prompt.**

    For the following query to a chatbot, which response is more helpful?

    Query: <the user query>

    Response A:
    <either the test method or baseline>

    Response B:
    <the other response>

    FIRST provide a one-sentence comparison of the two responses and explain \
    which you feel is more helpful. SECOND, on a new line, state only "A" or \
    "B" to indicate which response is more helpful. Your response should use \
    the format:
    Comparison: <one-sentence comparison and explanation>
    More helpful: <"A" or "B">

## Unlikelihood baseline

While we include the unlikelihood baseline [@welleck2019neural] (simply
maximizing $\log p(y_w|x)$, the log probability of the preferred
response, while minimizing $\log p(y_l|x)$, the log probability of the
dispreferred response) in our sentiment experiments, we do not include
it as a baseline in either the summarization or dialogue experiment
because it produces generally meaningless responses, which we believe is
a result of unconstrained likelihood *minimization*.

::: {#tab:unlikelihood_generations}
  **Prompt**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              **Response**
  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  SUBREDDIT: r/relationships TITLE: The girl \[26 F\] I \[22 M\] have been seeing for a month didn't respond to me at all yesterday while hanging out with a friend \[ 30? M\]. POST: She gets terrible service while at her house, but I texted her 3 times yesterday, 4-5 hours apart. She didn't call me until early this morning and left a voicemail that she was busy all day with a friend who showed up out of the blue. I saw that she posted a picture of the two of them out of her dead zone house on facebook before I texted her the last time. I don't mind that she hangs out with friends, and I know it's pretty early \[...\] TL;DR:   girl when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when whenUB when when when when whenUB when when when whenUB
  SUBREDDIT: r/tifu TITLE: TIFU by accidently kicking an old woman POST: So this didn't happen today but actually about a year or two ago. I was at my granddads funeral so of course it was all very sad and full of lots of crying old people. After the ceremony everyone walks outside the building and onto the other side of the small road the hearses drive down. Now the road is important because obviously if there's a road, there's a curb onto the sidewalk, so most of us are on the other side of the road, besides a few older people walking a lot slower. As one of the old woman goes to walk up the curb \[...\] TL;DR:              when an old woman was tripping the when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when

  : Unlikelihood samples from TL;DR prompts sampled at temperature 1.0.
  In general, we find unlikelihood fails to generate meaningful
  responses for more complex problems such as summarization and
  dialogue.
:::

# Additional Empirical Results

## Performance of Best of $N$ baseline for Various $N$

We find that the Best of $N$ baseline is a strong (although
computationally expensive, requiring sampling many times) baseline in
our experiments. We include an evaluation of the Best of $N$ baseline
for various $N$ for the Anthropic-HH dialogue and TL;DR summarization;
the results are shown in
Figure [\[fig:best-of-n\]](#fig:best-of-n){reference-type="ref"
reference="fig:best-of-n"}.

![image](figures/results/dialogue_winrate_vs_temp_rerank.pdf){width="49%"}
![image](figures/results/tldr_rerank_vs_temp.pdf){width="49%"}

## Sample Responses and GPT-4 Judgments

In this section, we present examples of comparisons between DPO and the
baseline (PPO temp 0. for summarization, and the ground truth chosen
response for dialogue). See
Tables [2](#tab:summ-ex-1){reference-type="ref"
reference="tab:summ-ex-1"}-[4](#tab:summ-ex-3){reference-type="ref"
reference="tab:summ-ex-3"} for summarization examples, and
Tables [5](#tab:dialogue-ex-1){reference-type="ref"
reference="tab:dialogue-ex-1"}-[8](#tab:dialogue-ex-4){reference-type="ref"
reference="tab:dialogue-ex-4"} for dialogue examples.

::: {#tab:summ-ex-1}
  **Prompt**     SUBREDDIT: r/AskReddit TITLE: I've been ungrateful to parents who have wanted only the best for me for the longest time. I am so lucky to have the life I have but I squandered it, this isn't a humblebrag about my \"wicked life\" just tell me, what can I do to make it up to them after this. POST: I'm 17, leaving for University, and I've had epiphany after epiphany of how shitty i've beent to them, or how all the nice things they do I ruin with moods etc., I know there isn't a quick fix, but I want to do something nice. I've always thanked them and apologized where it is due (in earlier years not so much) but as they say, actions speak louder than words, so I was wondering what can I do now, before I go, to just make them happy. What can I keep doing, I know all the little things, doing chores, keeping my room clean, cleaning up after myself, helping around the house, but what's a big thing, for say christmas, or their birthdays, that I can do to make them happy?
  -------------- -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **DPO**        TL;DR: I've been a shitty child and I want to make up for it, what can I do for my parents on their birthdays and christmas to show them how much I love them?
  **PPO**        TL;DR: I've been a shitty person to my parents, what can I do to make it up to them, before I go, and what can I do after university to make them happy, and what can I do to make them happy after university (if that is the case)
  **Judgment**   Summary B **\[DPO\]** more effectively captures the main points of the post by focusing on making the parents happy through gestures on special occasions, while Summary A seems repetitive and less clear.

  : Sample summaries to a post from the TL;DR test set. **DPO** sample
  generated with a temperature 0.25; **PPO** sample generated at a
  temperature of 0. The order in which summaries are presented in
  randomized when evaluating with GPT-4, so the order in the Judgment
  may not correspond to the order in the table. For clarity, post-hoc
  annotations are included in bold, formatted as **\[annotation\]**.
  These annotations are not part of the model generations.
:::

::: {#tab:summ-ex-2}
+--------------+------------------------------------------------------+
| **Prompt**   | SUBREDDIT: r/relationships TITLE: I \[22 M\] am      |
|              | about to propose to my girlfriend \[22F\] of almost  |
|              | 2 years, and I can't calm myself down POST: Alright, |
|              | so I am preparing to propose to my girlfriend of     |
|              | almost 2 years (we were friends for double that      |
|              | beforehand) in a couple days and I am starting to    |
|              | get EXTREMELY anxious/nervous. We have talked about  |
|              | this for months, and I know she will say yes, but I  |
|              | can't seem to relax.                                 |
|              |                                                      |
|              | The other day I bought the ring, which I fell in     |
|              | love with myself and know she will too. And then two |
|              | days ago, I asked her dad's blessing. Everything has |
|              | gone great so far, but now leading up to it I can't  |
|              | think about anything else. I can't sleep. I am       |
|              | anxious as hell, and all I want to do is pop the     |
|              | question now, but I have to wait since I don't see   |
|              | her until Thursday.                                  |
|              |                                                      |
|              | I know how I am going to do it. I have thoroughly    |
|              | planned it out and I am more than prepared. But, at  |
|              | the same time, I am freaking out. I asked my best    |
|              | friend for advice since he just got married          |
|              | recently, and all he could say was \"It will feel    |
|              | 10x worse right before you do ask.\"                 |
|              |                                                      |
|              | Any suggestions on how to relax leading up to it? I  |
|              | would like to be able to sleep\...                   |
+:=============+:=====================================================+
| **DPO**      | TL;DR: About to propose to my girlfriend of almost 2 |
|              | years, but I am extremely anxious and can't sleep.   |
|              | How can I relax?                                     |
+--------------+------------------------------------------------------+
| **PPO**      | TL;DR: I am about to ask my girlfriend of almost 2   |
|              | years to marry me, and I can't calm myself down. Any |
|              | tips on how to relax? I would like to be able to     |
|              | sleep tonight (and the night                         |
|              | before)\...\...\...                                  |
|              | \...\...\...\...\...\...\...\...\...\...\...\...\... |
|              | \...\...\...\...\...\...\...\...\...\...\...\...\... |
|              | \...\...\...\...\...\...\...\...\...\...\...\...\... |
+--------------+------------------------------------------------------+
| **Judgment** | Summary A **\[DPO\]** more effectively summarizes    |
|              | the main points of the post, concisely conveying the |
|              | asker's anxiety and goal of finding ways to relax.   |
+--------------+------------------------------------------------------+

: Sample summaries to a post from the TL;DR test set. **DPO** sample
generated with a temperature 0.25; **PPO** sample generated at a
temperature of 0. The order in which summaries are presented in
randomized when evaluating with GPT-4, so the order in the Judgment may
not correspond to the order in the table. For clarity, post-hoc
annotations are included in bold, formatted as **\[annotation\]**. These
annotations are not part of the model generations.
:::

::: {#tab:summ-ex-3}
+--------------+------------------------------------------------------+
| **Prompt**   | SUBREDDIT: r/loseit TITLE: Is It Bullshit?: Fiber    |
|              | Gourmet Pasta POST: I was looking for low-cal        |
|              | alternatives to pasta, because I love pasta and it's |
|              | easy to make and I eat it several times a week. I    |
|              | find that whole grain pasta has a weird              |
|              | taste/texture, and I'm not a big fan of it.          |
|              |                                                      |
|              | I was doing some research into spaghetti squash      |
|              | (which is on my grocery list for next time), but I   |
|              | also heard someone rave about some high-fiber pasta  |
|              | brand so I looked into it.                           |
|              |                                                      |
|              | What sorcery is this? It has all the trappings of    |
|              | regular pasta (and the reviews I've read say it      |
|              | tastes the same too) but a little over half the      |
|              | calories. My mind boggles over how much extra pasta  |
|              | I could eat! I can't believe this hasn't taken the   |
|              | world by storm, which makes me wonder what the catch |
|              | is.                                                  |
+:=============+:=====================================================+
| **DPO**      | TL;DR: I'm trying to cut back on calories and pasta  |
|              | is one of my main sources. Found a high-fiber pasta  |
|              | that has all the trappings of regular pasta and      |
|              | seems like it would be a good substitute. Is it      |
|              | bullshit?                                            |
+--------------+------------------------------------------------------+
| **PPO**      | TL;DR: I want to lose weight, but I love pasta and   |
|              | don't want to give it up. Is there a high-fiber      |
|              | pasta that I can eat more of and lose weight with,   |
|              | or is it all bullshit?                               |
+--------------+------------------------------------------------------+
| **Judgment** | Summary B **\[DPO\]** more accurately captures the   |
|              | intent to find a low-calorie pasta alternative,      |
|              | without assuming the need for weight loss.           |
+--------------+------------------------------------------------------+

: Sample summaries to a post from the TL;DR test set. **DPO** sample
generated with a temperature 0.25; **PPO** sample generated at a
temperature of 0. The order in which summaries are presented in
randomized when evaluating with GPT-4, so the order in the Judgment may
not correspond to the order in the table. For clarity, post-hoc
annotations are included in bold, formatted as **\[annotation\]**. These
annotations are not part of the model generations.
:::

::: {#tab:dialogue-ex-1}
  **Prompt**     Can you help me write an essay on the Civil Rights Movement?
  -------------- ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **DPO**        The Civil Rights Movement was a series of social and political movements that took place between the 1950s and the 1980s, leading to the integration of public spaces, higher education, employment, and voting rights. The movement is often credited with the end of legal segregation in the United States, and it was a major catalyst for the Civil Rights Act of 1964. The Movement also played a large role in the rise of the Black Power and Civil Rights Movements, which focused on economic equality and racial justice. The Movement focused on the rights of African Americans, who were then the dominant racial and ethnic group in the United States, but it also highlighted the rights of other minority groups, such as women, immigrants, and LGBT people. The Movement is often considered one of the most important social movements of the 20th century, and it played a significant role in the advancement of civil rights for all Americans. I'll be happy to write an essay on this topic for you. What kind of essay would you like? A paper on the history of the Civil Rights Movement? A paper on the causes of the Civil Rights Movement? Or a paper on the effects of the Civil Rights Movement? Any ideas? I'll be happy to **\[maximum generation length reached\]**
  **GT**         Oh, I'm not sure if I can actually write an essay, but I can at least help you think through the process. First, can you tell me what you're trying to write about? In other words, what are the main points or questions you want to address?
  **Judgment**   Response B **\[DPO\]** provides more detailed information about the Civil Rights Movement and offers specific suggestions for essay topics, making it more helpful for someone writing an essay.

  : GPT-4 chooses DPO over GT. Sample responses to a prompt from the
  Anthropic-HH test set. **DPO** sample generated with temperature 0.7;
  **GT** is the chosen completion in the dataset of preferences. For
  clarity, post-hoc annotations are included in bold, formatted as
  **\[annotation\]**. These annotations are not part of the model
  generations.
:::

::: {#tab:dialogue-ex-2}
  **Prompt**     I am trying to locate the residential address for J.B. Pritzker, an hier **\[sic\]** to the Hyatt Hotel fortune.
  -------------- ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  **DPO**        I am not able to assist you in this matter. It's unlawful and irresponsible to locate private information on people without their consent, and I'm not aware of any way you could do this for human. Please stop this activity.
  **GT**         J.B. Pritzker is one of the co-founders and investors in Hyatt. He and his family have been listed in the Forbes 400, and the family also maintains a 4,000-acre ranch in Colorado.
  **Judgment**   Response A **\[DPO\]** emphasizes the importance of privacy and laws, while Response B gives background information on J.B. Pritzker without providing his address.

  : GPT-4 chooses DPO over GT. Sample responses to a prompt from the
  Anthropic-HH test set. **DPO** sample generated with temperature 1.0;
  **GT** is the chosen completion in the dataset of preferences. For
  clarity, post-hoc annotations are included in bold, formatted as
  **\[annotation\]**. These annotations are not part of the model
  generations.
:::

::: {#tab:dialogue-ex-3}
+--------------+------------------------------------------------------+
| **Prompt**   | Can you tell me what got the USA involved in World   |
|              | War II?                                              |
+:=============+:=====================================================+
| **DPO**      | In the 1930s, the United States found itself at the  |
|              | center of a growing political and economic crisis    |
|              | that threatened the world's stability. The Great     |
|              | Depression had brought many other countries to the   |
|              | brink, including many European ones, and the United  |
|              | States found itself in a very precarious position.   |
|              | The Great Depression had left the government and the |
|              | economy on the edge of collapse, it faced severe     |
|              | international indebtedness, and it faced the threat  |
|              | of a foreign default on its loans. When Japan        |
|              | attacked China in October 1937, the United States    |
|              | found itself in an especially dangerous military     |
|              | position, as the United States was the largest       |
|              | creditor.                                            |
|              |                                                      |
|              | The United States found itself desperately seeking   |
|              | ways to avoid a complete collapse, and two main      |
|              | strategies emerged: both involved creating an        |
|              | alliance of nations to protect American economic     |
|              | security and stability, as well as ensuring that the |
|              | United States remained politically relevant. The     |
|              | first strategy was to create a "coalition of the     |
|              | willing", a network of international alliances to    |
|              | counteract the isolation and vulnerability of        |
|              | countries like Japan. The second strategy was to     |
|              | form an "all-inclusive association", a group of      |
|              | nations that the United States would be the most     |
|              | powerful member of, and which would protect the      |
|              | prosperity of the United States and protect it from  |
|              | other powers **\[maximum generation length           |
|              | reached\]**                                          |
+--------------+------------------------------------------------------+
| **GT**       | The United States entered the war against Japan in   |
|              | response to the Japanese attack on Pearl Harbor,     |
|              | Hawaii, on December 7, 1941. This was a full month   |
|              | before the United States officially entered the war. |
|              | At this point, the German Axis powers controlled     |
|              | France, Belgium, Norway, and other parts of Europe,  |
|              | and held significant military influence in much of   |
|              | Asia. President Roosevelt officially asked Congress  |
|              | for a declaration of war against Japan after the     |
|              | bombing of Pearl Harbor.                             |
+--------------+------------------------------------------------------+
| **Judgment** | Response B **\[GT\]** provides a direct and accurate |
|              | answer to the question, while Response A is overly   |
|              | complicated and doesn't provide the correct answer.  |
+--------------+------------------------------------------------------+

: GPT-4 chooses GT over DPO. DPO's response is verbose and plausible,
but contains factually incorrect information (the 'coalition of the
willing' does not refer to events of WWII; the 'all-inclusive
association' is not a real organization).
:::

::: {#tab:dialogue-ex-4}
+--------------+------------------------------------------------------+
| **Prompt**   | what is 7 plus 2                                     |
+:=============+:=====================================================+
| **DPO**      | Hm, let me see\... seven plus two is nine, I'm       |
|              | pretty sure. I'll be happy to read the input aloud   |
|              | and double check if I've got it right. Let me try:   |
|              |                                                      |
|              | \"7 + 2\"                                            |
|              |                                                      |
|              | I've got that right! I think our input was meant to  |
|              | be seven, plus two. You can try it yourself:         |
|              |                                                      |
|              | \"7 + 2\"                                            |
|              |                                                      |
|              | \"9\"                                                |
|              |                                                      |
|              | I've got that right! Now, what I'm wondering is if   |
|              | you want to learn this formula for yourself, or if   |
|              | you want me to help you practice it. The formula is  |
|              | called the "arithmetic-prefix method", and it's a    |
|              | great way to memorize numbers -- and in fact I       |
|              | personally learned to count by memorizing the        |
|              | arithmatic prefixes! I'll give you a refresher on    |
|              | arithmatic prefixes: you can think of them as an     |
|              | abbreviated way of describing a number by its first  |
|              | two significant digits. There are three arithmatic   |
|              | prefixes:                                            |
|              |                                                      |
|              | The "1-arithmatic prefix" (1-A) The "2-arithmatic    |
|              | prefix" (2-A)                                        |
+--------------+------------------------------------------------------+
| **GT**       | 11                                                   |
+--------------+------------------------------------------------------+
| **Judgment** | Response B **\[GT\]** provides a direct and accurate |
|              | answer to the question, while Response A is overly   |
|              | complicated and doesn't provide the correct answer.  |
+--------------+------------------------------------------------------+

: GPT-4 chooses GT over DPO. GPT-4 incorrectly states that the ground
truth is correct while DPO's (more verbose) output is wrong.
:::

## Human study details {#app:human-study}

::: wrapfigure
r0.57 ![image](figures/results/survey.png){width="57%"}
:::

In order to validate the usage of GPT-4 for computing win rates, our
human study collects human preference data for several matchups in the
TL;DR summarization setting. We select three different algorithmic
matchups, evaluating DPO (temp. 0.25), SFT (temp. 0.25), and PPO (temp
1.0) compared to the reference algorithm PPO (temp 0.). By selecting
matchups for three unique algorithms as well as algorithms with a wide
range of win rates vs the reference, we capture the similarity of human
and GPT-4 win rates across the response quality spectrum. We sample 150
random comparisons of DPO vs PPO-0 and 100 random comparisons PPO-1 vs
PPO-0, assigning two humans to each comparison, producing 275 judgments
for DPO-PPO[^7] and 200 judgments for PPO-PPO. We sample 125 SFT
comparisons, assigning a single human to each. We ignore judgments that
humans labeled as ties (which amount to only about 1% of judgments), and
measure the raw agreement percentage between human A and human B (for
comparisons where we have two human annotators, i.e., not SFT) as well
as between each human and GPT-4.

#### Participants.

We have 25 volunteer human raters in total, each comparing 25 summaries
(one volunteer completed the survey late and was not included in the
final analysis, but is listed here). The raters were Stanford students
(from undergrad through Ph.D.), or recent Stanford graduates or
visitors, with a STEM (mainly CS) focus. See
Figure [\[fig:survey\]](#fig:survey){reference-type="ref"
reference="fig:survey"} for a screenshot of the survey interface. We
gratefully acknowledge the contribution of each of our volunteers,
listed in random order:

  ------------------------- ---------------------- ------------------ -------------------------
  1\. Gordon Chi            2\. Virginia Adams     3\. Max Du         4\. Kaili Huang
  5\. Ben Prystawski        6\. Ioanna Vavelidou   7\. Victor Kolev   8\. Karel D'Oosterlinck
  9\. Ananth Agarwal        10\. Tyler Lum         11\. Mike Hardy    12\. Niveditha Iyer
  13\. Helena Vasconcelos   14\. Katherine Li      15\. Chenchen Gu   16\. Moritz Stephan
  17\. Swee Kiat Lim        18\. Ethan Chi         19\. Kaien Yang    20\. Ryan Chi
  21\. Joy Yun              22\. Abhay Singhal     23\. Siyan Li      24\. Amelia Hardy
  25\. Zhengxuan Wu                                                   
  ------------------------- ---------------------- ------------------ -------------------------

[^1]: Equal contribution; more junior authors listed earlier.

[^2]: <https://huggingface.co/CarperAI/openai_summarize_tldr_sft>

[^3]: That is, the sum of the per-timestep KL-divergences.

[^4]: <https://huggingface.co/CarperAI/openai_summarize_tldr_sft>

[^5]: <https://huggingface.co/reciprocate/ppo_hh_pythia-6B>

[^6]: <https://github.com/CarperAI/trlx/tree/main/examples/hh>

[^7]: One volunteer did not respond for the DPO-PPO comparison.
