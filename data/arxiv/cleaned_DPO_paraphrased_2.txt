\title{Direct Preference Optimization: Your Language Model is Secretly a Reward Model}

\begin{abstract}
Although large-scale unsupervised language models (LMs) acquire extensive world knowledge and some reasoning abilities, exerting accurate control over their outputs remains challenging due to their purely unsupervised training.
Current techniques for improving such steerability involve collecting human judgments on the relative quality of model responses and then fine-tuning the original LM to follow these preferences, typically using reinforcement learning from human feedback (RLHF).
Yet, RLHF is a complicated and sometimes unstable process that first fits a reward model to reflect human preference data, then applies reinforcement learning to the large LM to maximize this reward signal, while constraining deviations from the base model.
In this work, we present a novel way to parameterize the reward model within RLHF, enabling us to derive the associated optimal policy in closed form and thereby reduce the standard RLHF objective to a straightforward classification loss.
We introduce the resulting method, termed \textit{Direct Preference Optimization} (DPO), which is robust, efficient, and requires minimal computation, removing the need for LM sampling during fine-tuning or extensive hyperparameter searches.
Empirical results demonstrate that DPO is capable of aligning LMs to human preferences as well as or better than prior approaches. In particular, DPO surpasses PPO-based RLHF in controlling generation sentiment, and equals or outperforms it in summary and single-turn dialogue quality, all while being significantly easier to apply and train.
\end{abstract}

\section{Introduction}
Large-scale unsupervised language models (LMs) trained on massive corpora often exhibit unexpected competencies~\citep{chowdhery2022palm, brown2020language, touvron2023llama,bubeck2023sparks}. Despite this, these models are exposed to human-produced data encompassing diverse objectives, values, and proficiencies. Not all of these human-driven intents and practices are suitable for the model to replicate. For instance, though we might want our AI coding assistant to \textit{recognize} frequent programming errors to assist in their correction, we would still prefer the model to emulate the (sometimes infrequent) instances of superior coding skills found in its training set when producing code. Similarly, it's useful for a language model to \textit{know} about a prevalent misconception held by half of the population, but we would not want the system to assert this falsehood as fact in half its responses! Thus, being able to distinguish and guide the model towards \emph{preferred outputs and actions} from the breadth of its \textit{knowledge and skills} is essential for constructing AI systems that are reliable, effective, and governable \citep{ouyang2022training}. Although current approaches commonly adjust LMs to align with human judgments via reinforcement learning (RL), we demonstrate that the RL-based learning objective typically adopted can be directly optimized through a straightforward binary cross-entropy loss, which substantially streamlines the preference optimization process.

\begin{figure}
    \centering
    \includegraphics[width=0.999\textwidth]{figures/diagrams/teaser.png}
    \caption{\textbf{DPO aligns with human preferences without relying on reinforcement learning.} Traditional approaches for adapting language models using human feedback typically involve training a reward model on data containing prompts and human-annotated preferences between response pairs, followed by employing RL to identify a policy that optimizes the learned reward. By contrast, DPO directly trains the policy to adhere to human preferences through a straightforward classification loss, effectively learning an \textit{implicit} reward model from which the optimal policy can be analytically derived.}
    \vspace{-2mm}
    \label{fig:teaser}
\end{figure}

Broadly, current approaches impart desired behaviors to language models by leveraging carefully selected human preference datasets that capture what people consider safe and useful. This preference learning phase follows a prior stage of extensive unsupervised pre-training on a massive text corpus. Although the most direct method for incorporating preferences is supervised fine-tuning using high-quality human-generated responses, the most effective techniques rely on reinforcement learning from human (or AI) feedback (RLHF/RLAIF; \citep{christiano2017deep,bai2022constitutional}). RLHF strategies involve training a reward model on human preference data and subsequently applying RL to adjust the language model’s policy, encouraging outputs that receive high reward scores while maintaining closeness to the original model. Despite RLHF yielding models with strong abilities in conversation and coding, its workflow is far more intricate than that of supervised learning, requiring the training of several language models and integrating model output sampling within the training process, which leads to substantial computational demands.

In this work, we demonstrate how to directly tune a language model to match human preferences, bypassing the need for explicit reward modeling or reinforcement learning. We introduce Direct Preference Optimization (DPO), an algorithm that implicitly optimizes the same objective as traditional RLHF methods—maximizing reward subject to a KL-divergence constraint—but offers greater simplicity in both implementation and training. Conceptually, DPO updates the model by boosting the relative log likelihood of preferred responses over less preferred ones, while applying a per-example, adaptive importance weight to avoid the model degradation seen with a naive probability ratio approach. Similar to previous techniques, DPO utilizes a theoretical preference framework (such as the Bradley-Terry model; \cite{bradley1952rankanalysis}) to assess how closely a reward function corresponds to observed preference data. However, in contrast to earlier methods that use the preference model to define a loss for training a reward model—followed by training a policy against that reward—DPO leverages a change of variables to directly express the preference loss in terms of the policy itself. With a collection of human preference annotations over model outputs, DPO can train a policy using a straightforward binary cross entropy loss, resulting in an optimal policy relative to an implicit reward function learned from the preference data.

The primary contribution of our work is Direct Preference Optimization (DPO), a straightforward algorithm that enables training language models from preferences without using reinforcement learning. Through our experiments, we demonstrate that DPO matches or surpasses the performance of current approaches—such as PPO-based RLHF—for preference-based learning tasks like sentiment adjustment, summarization, and conversational modeling, even when applied to language models with up to 6B parameters.

\section{Related Work}

Large-scale self-supervised language models are capable of tackling certain tasks in a zero-shot manner \citep{radford2019language} or with only a few example prompts \citep{gpt3,megatron,chowdhery2022palm}. Nevertheless, their results on downstream benchmarks and alignment with users’ goals can be substantially enhanced by further training on instruction datasets paired with human-generated completions \citep{mishra-etal-2022-cross,sanh2022multitask,chung2022scaling,thoppilan2022lamda}. This process, known as ‘instruction-tuning’, empowers LLMs to better generalize to novel instructions outside their original training distribution and typically improves overall usefulness \citep{chung2022scaling}. Although instruction tuning has yielded promising outcomes, collecting \textit{relative} human assessments of model output is often simpler than obtaining expert-provided demonstrations. As a result, later research has trained LLMs using human preference datasets, boosting performance in tasks such as translation \citep{kreutzer-etal-2018-reliability}, summarization \citep{stiennon2022learning,ziegler2020finetuning}, story generation \citep{ziegler2020finetuning}, and instruction-following \citep{ouyang2022training,ramamurthy2023is}. The standard approach involves first learning a neural network reward model that predicts preference compatibility using frameworks such as the Bradley-Terry model \citep{bradley1952rankanalysis}, and then fine-tuning the language model via reinforcement learning methods like REINFORCE \citep{williams1992reinforce}, proximal policy optimization (PPO; \cite{schulman2017proximal}), or related algorithms \citep{ramamurthy2023is}. Related efforts have harnessed instruction-tuned LLMs, refined through human feedback, to synthesize additional preference data for specific traits like harmlessness or safety \citep{bai2022constitutional}, relying primarily on weak human supervision through textual rubrics used by the LLM for scoring. These strategies combine research directions: one focuses on reinforcement learning approaches for optimizing language models for various objectives~\citep{Ranzato2015SequenceLT,paulus2018a,wu2018learning}, and another centers on general frameworks for incorporating human preferences into learning algorithms \citep{christiano2017deep,kupcsik2018learning}. Despite the advantages of using relative human preferences, applying reinforcement learning for fine-tuning large language models remains difficult in practice; in this work, we introduce a theoretically motivated method to optimize for relative preferences without relying on RL.

Beyond the realm of language, learning policies based on preferences has been explored in both bandit and reinforcement learning frameworks, leading to the development of multiple techniques. Contextual bandit learning where preferences or action rankings are used in place of reward signals is referred to as contextual dueling bandits (CDB; \cite{yue2012karmed,dudik2015contextual}). Without access to absolute rewards, theoretical work on CDBs replaces the standard idea of an optimal policy with that of a \textit{von Neumann winner}—a policy that has an expected win rate of at least 50\% when compared against \textit{any} alternative policy \citep{dudik2015contextual}. A key distinction is that, in the CDB framework, preference feedback is collected online, whereas learning from human preferences typically utilizes a predetermined, offline set of action pairs labeled with preferences \citep{yan2022human}. Analogously, \textit{preference-based RL} (PbRL) is driven by binary preferences derived from an \textit{unknown} scoring function rather than observed rewards \citep{BusaFekete2014,ruiz2023dueling}. A range of PbRL algorithms have been proposed—some capable of leveraging off-policy preference data—but these usually require an explicit intermediate step where the underlying scoring (reward) function is estimated before policy optimization \citep{jain2013learning,BusaFekete2014,christiano2017deep,sadigh2017active,kupcsik2018learning}. By contrast, we introduce a one-step policy learning method that aims to directly optimize the policy in accordance with the given preferences.

\section{Preliminaries}\label{section:prelims}

We summarize the RLHF process described by \citeauthor{ziegler2020finetuning} (see also \citep{stiennon2022learning, bai2022training, ouyang2022training}). This procedure typically consists of three main steps: (1) supervised fine-tuning (SFT), (2) collecting preferences and training a reward model, and (3) reinforcement learning-based optimization.

\textbf{SFT}: RLHF usually starts by applying supervised fine-tuning to a pre-trained language model using curated datasets tailored to the target tasks (such as summarization or dialogue). This process results in a model referred to as $\pisft$.

\textbf{Reward Modeling Stage}: During this stage, the SFT model is given prompts $x$, which are used to generate answer pairs $(y_1, y_2) \sim \pisft(y \mid x)$. These answer pairs are then shown to human evaluators, who indicate a preference for one answer, represented as $y_w \succ y_l \mid x$, where $y_w$ and $y_l$ denote the chosen and non-chosen responses from $(y_1, y_2)$, respectively. The observed preferences are assumed to arise from an underlying, unknown reward function $r^*(y, x)$. Several models have been developed to formalize these preferences, with the Bradley-Terry (BT) \cite{bradley1952rankanalysis} model being a widely adopted method (though more flexible models, such as Plackett-Luce \citep{plackett1975analysis, luce2012individual}, can also be employed when multiple ranked completions are available). In the BT framework, the human preference probability $p^*$ is defined as follows:
\begin{equation}\label{eq:bradley-terry}
    p^*(y_1\succ y_2 \mid x)=\frac{\exp\left(r^*(x, y_1)\right)}{\exp\left(r^*(x, y_1)\right) + \exp\left(r^*(x, y_2)\right)}.
\end{equation}
Given a fixed dataset of preference comparisons $\mathcal{D}=\bigl\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\bigr\}_{i=1}^N$ sampled from $p^*$, a parameterized reward model $r_{\phi}(x, y)$ can be trained by maximizing the likelihood. By interpreting this as a binary classification problem, the objective becomes the negative log-likelihood:
\begin{equation}\label{eq:reward_model}
    \mathcal{L}_R(r_{\phi}, \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\bigl[\log \sigma(r_{\phi}(x, y_w)- r_{\phi}(x, y_l))\bigr]
\end{equation}
where $\sigma$ denotes the logistic sigmoid function. Typically, in the context of language models, $r_{\phi}(x, y)$ is initialized using the SFT model $\pisft(y \mid x)$ and is augmented by a linear output layer on top of the final transformer layer, producing a single scalar as the reward prediction \cite{ziegler2020finetuning}. To minimize reward variance, prior approaches often standardize the rewards so that $\mathbb{E}_{x,y\sim \mathcal{D}}\left[r_\phi(x, y)\right] = 0$ for all $x$.

\textbf{RL Fine-Tuning Step}: In the RL fine-tuning stage, the language model receives feedback based on the previously trained reward function. In line with earlier studies~\citep{jaques2017sequence, jaques2020human}, the optimization objective is defined as
\begin{equation}\label{eq:RL}
\max_{\pi_{\theta}}  \mathbb{E}_{x\sim \mathcal{D}, y\sim \pi_{\theta}(y \mid x)}\bigl[r_{\phi}(x, y)\bigr] - \beta\mathbb{D}_{\textrm{KL}}\bigl[\pi_{\theta}(y\mid x)\mid \mid \piref(y\mid x)\bigr],
\end{equation}
where the parameter $\beta$ manages how much the learned policy $\pi_{\theta}$ is allowed to diverge from the reference policy $\piref$, which is the initial SFT model $\pisft$. In implementation, the policy $\pi_\theta$ typically starts from $\pisft$. This regularization term serves to keep the policy close to the region where the reward model provides reliable evaluations and also to preserve diversity in outputs, avoiding collapse to a single, highly-rewarded response. Since text generation involves discrete outputs, the objective above is non-differentiable, and so reinforcement learning methods are used for training. The conventional method~\citep{ziegler2020finetuning, stiennon2022learning, bai2022training, ouyang2022training} involves defining the reward as ${r(x, y) = r_{\phi}(x, y) -\beta (\log \pi_{\theta}(y\mid x) - \log \piref(y\mid x))}$, and then optimizing it with PPO~\cite{schulman2017proximal}.

\section{Direct Preference Optimization}\label{sec:DPO}

Inspired by the difficulties faced when using reinforcement learning algorithms on complex tasks like language model fine-tuning, we aim to propose a straightforward policy optimization method that uses preferences directly. In contrast to traditional RLHF techniques that first learn a reward function and subsequently maximize it with RL, our method utilizes a specific form of reward model parameterization that permits direct extraction of the optimal policy in closed form, eliminating the need for an RL training process. 

As we will explain in detail below, our main idea is to use an analytic relationship connecting reward functions to optimal policies, allowing us to translate a loss defined over rewards into a loss defined over policies. This change-of-variables strategy sidesteps the need to train an explicit, separate reward model, yet still optimizes within established frameworks for modeling human preferences, such as the Bradley-Terry model. Effectively, the policy network jointly represents the language model and the (implicit) reward function.

\textbf{Derivation of the DPO Objective.} We begin from the standard RL objective found in earlier studies, Eq.~\ref{eq:RL}, which is defined over a general reward function $r$. Building on prior work~\citep{peters2007reinforcement, peng2019advantage, korbak2022reinforcement, go2023aligning}, it is easy to verify that the solution to the KL-regularized reward maximization problem in Eq.~\ref{eq:RL} is given by:
\begin{equation}\label{eq:op_policy}
    \pi_r(y\mid x) = \frac{1}{Z(x)}\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right),
\end{equation}
where the normalizing constant $Z(x) =\sum_{y}\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)$ is called the partition function. A full derivation appears in Appendix \ref{app:derivation1}. Even if we substitute in an MLE estimate $r_{\phi}$ for the true reward $r^*$, computing $Z(x)$ remains computationally challenging~\citep{korbak2022reinforcement, go2023aligning}, making this expression impractical for real-world applications. Nevertheless, Eq.~\ref{eq:op_policy} can be rearranged to solve for the reward in terms of the optimal policy $\pi_r$, the reference policy $\piref$, and the partition function $Z(\cdot)$. By taking the logarithm of both sides of Eq.~\ref{eq:op_policy} and performing straightforward algebra, we arrive at:
\begin{equation}\label{eq:main_eq}
    r(x,y) = \beta \log \frac{\pi_r(y\mid x)}{\piref(y\mid x)} + \beta \log Z(x).
\end{equation}
This reparameterization can be used for the true reward $r^*$ and its associated optimal policy $\pi^*$. Importantly, the Bradley-Terry model depends only on reward differences between two outputs, namely, ${p^*(y_1 \succ y_2 \mid x) = \sigma(r^*(x, y_1) - r^*(x, y_2))}$. Plugging the above expression from Eq.~\ref{eq:main_eq} into the preference model Eq.~\ref{eq:bradley-terry}, the partition function terms cancel out, which means the human preference can be written solely in terms of $\pi^*$ and $\piref$. Therefore, the optimal RLHF policy $\pi^*$ for the Bradley-Terry framework obeys the preference model:
\begin{equation}\label{eq:objective}
    p^*(y_1\succ y_2 \mid x) = \frac{1}{1 + \exp\left(\beta \log \frac{\pi^*(y_2\mid x)}{\piref(y_2\mid x)} - \beta \log \frac{\pi^*(y_1\mid x)}{\piref(y_1\mid x)}\right)}
\end{equation}
The detailed derivation is available in Appendix~\ref{app:derivation2}. Although Eq.~\ref{eq:objective} is based on the Bradley-Terry model, analogous forms can be derived for more general Plackett-Luce models~\citep{plackett1975analysis, luce2012individual}, as described in Appendix~\ref{app:plackett_luce_models}.

With the probability of human preference now expressed in terms of the optimal policy rather than the reward function, we can establish a maximum likelihood objective for a parameterized policy $\pi_\theta$. Drawing a parallel to the reward modeling method (see Eq.~\ref{eq:reward_model}), the objective for our policy takes the form:
\begin{equation}\label{eq:optimum_model}
    \mathcal{L}_\text{DPO}(\pi_{\theta}; \piref) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\left[\log \sigma \left(\beta \log \frac{\pi_{\theta}(y_w\mid x)}{\piref(y_w\mid x)} - \beta \log \frac{\pi_{\theta}(y_l\mid x)}{\piref(y_l\mid x)}\right)\right].
\end{equation}
Through this approach, we learn an implicit reward via a different parameterization, where the optimal policy corresponds directly to $\pi_\theta$. Additionally, because this process is equivalent to fitting a reparameterized Bradley-Terry model, it inherits certain theoretical guarantees, such as consistency given appropriate assumptions about the preference data distribution \cite{bong2022generalized}. In Section~\ref{sec:theory}, we examine these theoretical characteristics of DPO and how they relate to other approaches.

\textbf{How does the DPO update function?} To gain a mechanistic intuition for DPO, it is helpful to examine the gradient of the DPO loss, $\mathcal{L}_\text{DPO}$. The gradient with respect to the model parameters $\theta$ is given by:
\begin{multline*}
    \nabla_\theta \mathcal{L}_\text{DPO}(\pi_\theta;\piref) = \\ -\beta\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \bigg[\underbrace{\sigma(\hat{r}_\theta(x, y_l) - \hat{r}_\theta (x, y_w))}_\text{places more weight when the reward prediction is inaccurate}\bigg[\underbrace{\nabla_\theta\log \pi(y_w \mid x)}_\text{boost likelihood of $y_w$} - \underbrace{\nabla_\theta\log\pi(y_l \mid x)}_\text{lower likelihood of $y_l$}\bigg]\bigg],
\end{multline*}
where the function $\hat{r}_\theta(x, y) = \beta \log \frac{\pi_\theta(y \mid x)}{\piref(y \mid x)}$ represents the implicit reward assigned by the language model $\pi_\theta$ relative to the reference model $\piref$ (see Section~\ref{sec:theory} for further discussion). In essence, the gradient of $\mathcal{L}_\text{DPO}$ increases the probability of preferred outputs $y_w$ while reducing the probability of non-preferred outputs $y_l$. Crucially, each training instance is weighted by the extent to which the implicit reward model $\hat{r}_\theta$ ranks the less-preferred completion above the preferred one, scaled by $\beta$—this reflects how badly the reward model misorders completions, modulated by the KL regularization term. Empirically, we find this weighting to be significant: omitting it—resulting in a naive implementation—often leads to degeneration in the language model's behavior (see Appendix Table~\ref{tab:unlikelihood_generations}).

\textbf{Overview of DPO.}  
The standard DPO workflow consists of the following steps: 1) For each prompt $x$, draw completions $y_1, y_2 \sim \piref(\cdot \mid x)$, annotate them based on human preference to form an offline preference dataset $\mathcal{D} = \{x^{(i)}, y_w^{(i)}, y_l^{(i)}\}_{i=1}^N$, and 2) train the language model $\pi_\theta$ by minimizing the loss $\mathcal{L}_\text{DPO}$, given the reference policy $\piref$, dataset $\mathcal{D}$, and a specified $\beta$.  
In real-world scenarios, practitioners prefer to utilize existing public preference datasets rather than creating new completions and manually collecting human preference data. Because such datasets typically use $\pisft$ for sampling, we set $\piref = \pisft$ whenever it is accessible. If $\pisft$ is not provided, we instead set $\piref$ by maximizing the likelihood of the preferred responses ${(x, y_w)}$—that is, we define ${\piref = \argmax_{\pi}\mathbb{E}_{x, y_w \sim \mathcal{D}}\left[\log \pi(y_w \mid x)\right]}$. This approach addresses the mismatch between the ideal (but unavailable) reference distribution and the $\piref$ employed in DPO. Additional information regarding implementation details and hyperparameter choices is available in Appendix~\ref{app:implementation}.

\section{Theoretical Examination of DPO}
Here, we offer a deeper analysis of the DPO approach, support it with theoretical justification, and discuss how the strengths of DPO address challenges faced by actor-critic methods in RLHF, including PPO~\cite{schulman2017proximal}.

\label{sec:theory}

\subsection{Your Language Model Is Secretly a Reward Model}

DPO manages to avoid both explicit reward modeling and reinforcement learning for policy training by utilizing a single maximum likelihood objective. Observe that the optimization objective in Eq. \ref{eq:main_eq} aligns with a Bradley-Terry framework, where the reward is given by $r^*(x, y) = \beta \log\frac{\pi^*_\theta(y \mid x)}{\piref(y \mid x)}$. Here, we update our parameterized model $\pi_{\theta}$, which is functionally equivalent to optimizing the reward model in Eq. \ref{eq:reward_model} through a suitable variable transformation. In this section, we will develop the theoretical basis for this reparameterization, demonstrate that it does not limit the family of reward models that can be learned, and prove that it permits exact retrieval of the optimal policy. We start by introducing an equivalence relation on reward functions.

\begin{definition}
Two reward functions $r(x, y)$ and $r'(x, y)$ are considered equivalent if and only if there exists a function $f$ such that ${r(x, y)-r'(x, y) = f(x)}$.
\end{definition}
It is straightforward to verify that this defines an equivalence relation, dividing the space of reward functions into distinct equivalence classes. The following two lemmas can thus be formulated:

\begin{lemma}\label{lemma:same_prefrence} Within the Plackett-Luce model, including the Bradley-Terry setting, any two reward functions belonging to the same category generate identical preference distributions.
\end{lemma}

\begin{lemma}\label{lemma:same_policy}
    Any two reward functions belonging to the same equivalence class produce the identical optimal policy under the constrained RL setting.
\end{lemma}
The proofs are direct and can be found in Appendix \ref{app:lemma1}. The first lemma highlights a classic identifiability issue associated with the Plackett-Luce model family \cite{plackett1975analysis}. Because of this ambiguity, additional identifiability requirements are often necessary to obtain meaningful guarantees for MLE solutions from Eq. \ref{eq:reward_model} \cite{bong2022generalized}. The second lemma asserts that all reward functions within the same equivalence class result in the same optimal policy. Thus, our main focus becomes the recovery of any representative reward function from the optimal equivalence class for our objective. The following Theorem is established in Appendix~\ref{app:thm1}:
\begin{theorem}\label{thm:main}
    Assuming mild conditions, every reward equivalence class compatible with the Plackett-Luce (including Bradley-Terry) models can be expressed using the reparameterization ${r(x, y) = \beta \log \frac{\pi(y\mid x)}{\piref(y\mid x)}}$ for some policy $\pi(y\mid x)$ and a chosen reference policy $\piref(y \mid x)$.
\end{theorem}
\begin{sproof}
    Let $r(x, y)$ denote any reward function, and let $\pi_r(y \mid x)$ be the induced optimal policy, given by Eq. \ref{eq:op_policy}. We aim to show that a reward function equivalent to $r$ can be written in the proposed reparameterized form. Define the mapping $f$ as  
\begin{equation}
    f(r; \piref, \beta)(x, y) = r(x, y) - \beta\log\sum_{y}\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)
\end{equation}
This operator $f$ effectively centers the reward function using the log partition function corresponding to $\pi_r$. As the normalization term only depends on the prefix $x$, $f(r; \piref, \beta)(x, y)$ remains within the equivalence class of $r(x, y)$. Substituting the right-hand side of Eq.~\ref{eq:main_eq} for $r$ (which is valid for any reward function), we get $f(r; \piref, \beta)(x, y) = \beta \log \frac{\pi_r(y\mid x)}{\piref(y\mid x)}$. Thus, the mapping $f$ produces an equivalent reward function in the specified form, so the suggested reparameterization does not sacrifice generality in the reward model.
\end{sproof}
Alternatively, Theorem~\ref{thm:main} can be interpreted as identifying exactly which representative within each reward equivalence class is chosen by the DPO reparameterization; specifically, the function that satisfies:
\begin{equation}\label{eq:lag_p}
     \sum_{y}\underbrace{\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)}_{=\pi(y\mid x)\text{, by Theorem~\ref{thm:main} reparameterization}} = 1,
\end{equation}
meaning $\pi(y\mid x)$ forms a proper probability distribution (non-negative and summing to 1).
Notably, by referencing Eq.~\ref{eq:op_policy}, Eq.~\ref{eq:lag_p} represents the partition function for the optimal policy defined by $r(x, y)$.
A central insight behind the DPO approach is that certain constraints can be imposed on the under-determined Plackett-Luce (and, in particular, Bradley-Terry) class of preference models in such a way that the space of reward models remains unchanged, but the optimal policy in Eq. \ref{eq:op_policy} becomes explicitly computable for all input prompts $x$.

\subsection{Instability of Actor-Critic Algorithms}
Our framework can also be leveraged to analyze instability issues in common actor-critic algorithms, such as PPO, employed for RLHF. We adhere to the RLHF workflow, with an emphasis on the RL fine-tuning process described in Section \ref{section:prelims}. This setup allows us to relate to the control-as-inference perspective \cite{levine2018reinforcement} for the constrained RL formulation presented in \ref{eq:RL}. Assuming a parameterized policy $\pi_{\theta}(y\mid x)$, we aim to minimize $\mathbb{D}_{\text{KL}}[\pi_{\theta}(y|x) \mid \mid \pi^*(y\mid x)]$, where $\pi^*$ denotes the optimal policy from Eq. \ref{eq:optimum_model}, derived from the reward function $r_{\phi}(y, x)$. After some algebraic manipulation, this results in the following optimization target:
\begin{equation}\label{eq:AC}
    \max_{\pi_{\theta}}\mathbb{E}_{\pi_{\theta}(y\mid x)}\left[\underbrace{r_{\phi}(x, y) -\beta\log\sum_{y}\piref(y\mid x)\exp\left(\frac{1}{\beta}r_{\phi}(x, y)\right)}_{f(r_{\phi}, \piref, \beta)} - \underbrace{\beta\log\frac{\pi_{\theta}(y\mid x)}{\piref(y\mid x)}}_{\text{KL}}\right]
\end{equation}
This objective matches those previously optimized in works such as \citep{ziegler2020finetuning, stiennon2022learning, bai2022training, ouyang2022training}, where the DPO-equivalent reward is used for the reward structure $r_{\phi}$. Here, the normalization term within $f(r_{\phi}, \piref, \beta)$ can be viewed as the soft value function associated with the reference policy $\piref$. Although this component does not influence the optimal policy, omitting it can cause the policy gradient to exhibit large variance, thus undermining stability in training. One can address this by learning a value function to estimate the normalization term, though this approach also poses optimization challenges. Alternatively, previous studies have normalized the reward with a human completion baseline, effectively treating it as a single-sample Monte Carlo approximation of the normalizer. In contrast, the DPO reparameterization introduces a reward structure that eliminates the need for such baselines.

\section{Experiments}
In this section, we assess DPO's effectiveness in training policies from preference data through empirical analysis. We begin with a controlled text-generation environment, examining how well DPO balances maximizing rewards and minimizing KL-divergence from the reference policy, compared to widely used preference-based algorithms such as PPO. Afterwards, we test DPO on larger-scale models and more challenging RLHF tasks, like summarization and dialogue. Our findings show that, with minimal hyperparameter tuning, DPO typically matches or surpasses strong baselines, including RLHF using PPO and selecting the best of $N$ generated trajectories based on a learned reward model. Prior to discussing these outcomes, we outline the experimental design; further information is provided in Appendix~\ref{app:exp_details}.

\textbf{Tasks.} Our study investigates three distinct open-ended text generation tasks. In each experiment, the methods learn a policy based on a preference dataset $\mathcal{D}=\bigl\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\bigr\}_{i=1}^N$. For \textbf{controlled sentiment generation}, $x$ represents a prompt taken from a movie review in the IMDb dataset \cite{maas-EtAl:2011:ACL-HLT2011}, and the policy’s objective is to generate a continuation $y$ with positive sentiment. To ensure evaluation is controlled, we construct preference pairs automatically using a pre-trained sentiment classifier, selecting $y_w$ and $y_l$ such that $p(\text{positive}\mid x,y_w)>p(\text{positive}\mid x,y_l)$. For the SFT baseline, GPT-2-large is fine-tuned to convergence on the training portion of the IMDB reviews (see App~\ref{app:sentiment_details} for more information). In the \textbf{summarization} task, $x$ comes from a Reddit forum post and the model is trained to output a summary $y$ that distills the core points of the post. Consistent with past research, we employ the Reddit TL;DR summarization dataset \citep{volske-etal-2017-tl} along with human preference data from \citeauthor{stiennon2022learning}. For SFT, we utilize a model fine-tuned on human-generated summaries of forum posts\footnote{\url{https://huggingface.co/CarperAI/openai_summarize_tldr_sft}} in conjunction with the TRLX framework \citep{leandro_von_werra_2023_7790115} for RLHF. Human preferences were collected on outputs from a model trained similarly, but distinct from, the SFT model we use. For \textbf{single-turn dialogue}, $x$ is a user prompt—ranging from technical questions to personal advice. The policy must generate a response $y$ that is both helpful and engaging. We rely on the Anthropic Helpful and Harmless dialogue dataset \citep{bai2022training}, which contains 170k human–assistant conversations; each concludes with two model-generated replies, one of which is marked as preferred by a human labeler. Since there is no existing SFT model for this dataset, we create one by fine-tuning a standard language model exclusively on preferred response examples.

\begin{figure}
    \centering
    \includegraphics[width=0.50\textwidth]{figures/results/frontier.pdf}
    \includegraphics[width=0.49\textwidth]{figures/results/tldr_winrate_vs_temp.pdf}
    \caption{\textbf{Left.} The trade-off curve of expected reward versus KL divergence from the reference policy. DPO achieves superior expected rewards at every level of KL, illustrating its optimization effectiveness. \textbf{Right.} TL;DR summarization win rates against human-generated summaries, as judged by GPT-4. DPO surpasses PPO's top performance on the summarization task and remains stable under varying sampling temperatures.}
    \vspace{-2mm}
    \label{fig:frontier-tldr-main}
\end{figure}

\textbf{Evaluation.} Our experimental evaluation employs two distinct methodologies. To assess each algorithm’s ability to optimize the constrained reward maximization objective, we use a controlled sentiment generation environment where we measure the set of achievable trade-offs between reward and KL-divergence from the reference policy—this trade-off curve is computable because we possess the exact reward function (the sentiment classifier). In contrast, in practical applications where the true reward function is unavailable, we assess algorithms based on their \textit{win rate} when compared to a baseline policy, using GPT-4 as an approximate human evaluator to judge summary quality and response helpfulness in summarization and single-turn dialogue scenarios, respectively. For summarization tasks, we treat the test set’s reference summaries as the baseline; in dialogue, the baseline is the preferred response from the test data. Although previous research indicates that language models can outperform existing metrics as automatic evaluators \citep{Chen2023ExploringTU}, we validate our use of GPT-4 through a human evaluation detailed in Sec.~\ref{sec:human-judgments}. Our findings show that GPT-4’s assessments are highly correlated with human judgments, with agreement between humans and GPT-4 often matching or exceeding the agreement levels among human annotators.

\textbf{Methods.} Beyond DPO, we assess various established methods for training language models to align with human preferences. For a straightforward baseline, we apply zero-shot prompting with \textbf{GPT-J} \citep{gpt-j} in the summarization scenario, and 2-shot prompting with \textbf{Pythia-2.8B} \citep{biderman2023pythia} for the dialogue scenario. Additionally, we test the \textbf{SFT} model and introduce \textbf{Preferred-FT}, a model fine-tuned through supervised learning using the preferred response $y_w$—either from SFT (in the sentiment control and summarization tasks) or a standard LM (in single-turn dialogue). Another pseudo-supervised approach we examine is \textbf{Unlikelihood}~\citep{welleck2019neural}, which trains the policy to increase the probability of $y_w$ and actively decrease the likelihood of $y_l$, using an adjustable coefficient $\alpha\in[0,1]$ for the unlikelihood penalty. We further consider \textbf{PPO} \citep{schulman2017proximal}, utilizing a reward model trained on preference annotations, and \textbf{PPO-GT}, which serves as an oracle by leveraging the ground truth reward function in the sentiment control setting. In our experiments focused on sentiment, we employ two PPO-GT variants: an off-the-shelf implementation \cite{leandro_von_werra_2023_7790115} and a custom version that incorporates reward normalization and hyperparameter adjustments for enhanced performance; these improvements are also applied to standard PPO with learned rewards. Lastly, we include the \textbf{Best of $N$} baseline, which involves generating $N$ outputs from the SFT model (or Preferred-FT for dialogue), then selecting the top response according to the preference-trained reward function. Although this approach often yields strong results by disentangling the reward model's effectiveness from PPO optimization, it is computationally costly, as it necessitates generating $N$ completions for each test query.

\subsection{How well can DPO optimize the RLHF objective?}

\begin{figure}
    \centering
    \includegraphics[width=0.50\textwidth]{figures/results/dialogue_winrate_vs_temp.pdf}
    \includegraphics[width=0.49\textwidth]{figures/results/dialogue_winrate_vs_steps.pdf}
    \caption{\textbf{Left.} Dialogue win rates for Anthropic-HH one-step interactions, as judged by GPT-4; among all methods, only DPO surpasses the performance of the reference summaries from the Anthropic-HH evaluation set. \textbf{Right.} Dialogue win rates across training steps for various sampling temperatures. DPO consistently outperforms the dataset reference labels, and this advantage remains relatively stable throughout training across different sampling temperatures.}
    \vspace{-2mm}
    \label{fig:dialogue-main}
\end{figure}

The KL-penalized reward maximization objective commonly employed in RLHF methods strikes a balance between maximizing reward and preventing the policy from straying too far from the reference policy. Consequently, when evaluating different algorithms, it is essential to consider both the attained reward and the corresponding KL divergence; obtaining marginally higher rewards at the cost of significantly larger KL is often undesirable. Figure~\ref{fig:frontier-tldr-main} illustrates the reward-KL tradeoff frontier across various algorithms for the sentiment task. For each algorithm, we conduct several training runs, varying the policy conservativeness hyperparameter in each trial (target KL $\in\{3,6,9,12\}$ for PPO, $\beta \in \{0.05,0.1,1,5\}$, $\alpha\in\{0.05,0.1,0.5,1\}$ for unlikelihood, and different random seeds for preferred-FT), resulting in 22 total runs. Every 100 steps during training, up to convergence, we evaluate the resulting policies on a test prompt set, calculating both the mean reward with respect to the true reward function and the mean sequence-level KL\footnote{Defined as the sum of KL-divergences across all timesteps.} with respect to the reference policy $\text{KL}\left(\pi\mid \mid \piref\right)$. Our findings show that DPO attains the most favorable frontier, consistently yielding the highest reward for a given KL, and maintaining low KL values. This outcome stands out for several reasons. Firstly, despite DPO and PPO targeting the same underlying objective, DPO is significantly more sample-efficient; DPO’s performance in the reward/KL space is strictly superior to PPO’s. Secondly, DPO outperforms PPO on the frontier, \emph{even when PPO is provided with ground truth rewards} (PPO-GT).

\subsection{Is DPO capable of scaling to real-world preference datasets?}
\label{sec:dpo-real-datasets}
We now assess the fine-tuning effectiveness of DPO on tasks such as summarization and single-turn dialogue. In the case of summarization, automated metrics like ROUGE often show weak alignment with human judgments~\citep{stiennon2022learning}. Earlier research demonstrated that fine-tuning language models with PPO using human feedback results in more effective summaries. Our evaluation considers various approaches by generating completions on the test portion of the TL;DR summarization dataset, then calculating the mean win rate when compared to reference completions from the test set. For every method, completions are sampled across a temperature range from 0.0 to 1.0, with the resulting win rates displayed in Figure~\ref{fig:frontier-tldr-main} (right). DPO, PPO, and Preferred-FT all use the same fine-tuned GPT-J SFT model\footnote{\url{https://huggingface.co/CarperAI/openai_summarize_tldr_sft}} for additional training. Our results indicate that DPO achieves about a 61\% win rate at a sampling temperature of 0.0, outperforming PPO, which attains roughly 57\% at its optimal temperature of 0.0. Furthermore, DPO reaches a higher maximum win rate than the best $N$ baseline. It should be mentioned that we did not perform substantial tuning of DPO's $\beta$ hyperparameter, implying these outcomes could underrepresent DPO’s true capabilities. Additionally, DPO exhibits significantly greater resilience to variations in sampling temperature compared to PPO, whose performance may deteriorate to that of the original GPT-J model at elevated temperatures. Preferred-FT, meanwhile, shows minimal gains over the initial SFT model. A direct comparison between DPO and PPO in human preference studies is presented in Section~\ref{sec:human-judgments}, demonstrating that DPO samples generated at a temperature of 0.25 were favored 58\% of the time over PPO samples at the same temperature.

For single-turn dialogue, we assess various approaches using the subset of the Anthropic HH dataset \citep{bai2022training} test split that involves one round of human-assistant exchange. For GPT-4 assessments, we utilize the preferred completions from the test set as references to determine the win rate of each approach. Since there is no standard SFT baseline for this task, our process begins with the pre-trained Pythia-2.8B. We then apply Preferred-FT to train a reference model on the selected completions to ensure outputs remain within the model’s distribution, followed by training with DPO. We also include comparisons with the best of 128 Preferred-FT completions (the Best of $N$ baseline plateaus at 128 completions on this benchmark; refer to Appendix Figure~\ref{fig:best-of-n}), and a 2-shot prompted variant of the Pythia-2.8B base model, observing that DPO matches or outperforms other methods at their optimal temperature settings. Additionally, we test an RLHF model, trained with PPO on the Anthropic HH dataset \footnote{\url{https://huggingface.co/reciprocate/ppo_hh_pythia-6B}} from a prominent repository \footnote{\url{https://github.com/CarperAI/trlx/tree/main/examples/hh}}, but do not find a prompt or sampling temperature that surpasses the base Pythia-2.8B model’s performance. Given our TL;DR findings and the fact that both PPO and Best of 128 optimize the same reward, we treat Best of 128 as an approximate indicator for PPO-level results. Overall, DPO stands out as the sole computationally efficient approach that exceeds the preferred completions in the Anthropic HH dataset, achieving performance on par with or better than the computationally intensive Best of 128 standard. Finally, Figure~\ref{fig:dialogue-main} illustrates that DPO rapidly converges to its peak performance.

\subsection{Generalization to a new input distribution}

\begin{wraptable}{r}{0.375\textwidth}
    \small
    \vspace{-10mm}
    \begin{tabular}{ccc}
        \toprule
        & \multicolumn{2}{c}{\textbf{Win rate against ground truth}} \\
        \cmidrule(lr){2-3}
        \textbf{Algorithm} & Temp $0$ & Temp $0.25$ \\
        \midrule
        DPO & 0.36 & 0.31 \\
        PPO & 0.26 & 0.23 \\
        \bottomrule
    \end{tabular}
    \caption{GPT-4 win rates compared to ground truth summaries on out-of-distribution CNN/DailyMail articles.}
    \vspace{-3mm}
    \label{tab:ood}
\end{wraptable}

To further assess the robustness of PPO and DPO under distributional shifts, we test the policies obtained from our Reddit TL;DR summarization study on a new domain: news articles from the CNN/DailyMail test split \citep{nallapati-etal-2016-abstractive}. We employ the optimal sampling temperatures identified for TL;DR (0 and 0.25). The comparative results are shown in Table~\ref{tab:ood}. We measured the win rate of GPT-4 against the reference summaries in these datasets, utilizing the same GPT-4 (C) prompt as in the Reddit TL;DR evaluation, except substituting ``forum post'' with ``news article.'' On this out-of-domain benchmark, DPO consistently surpasses the PPO policy by a notable margin. This finding serves as preliminary evidence that DPO policies can generalize at least as effectively as those trained with PPO, despite not leveraging the additional unlabeled Reddit TL;DR prompts available to PPO.

\subsection{Cross-validating GPT-4 assessments with human evaluations}
\label{sec:human-judgments}
We carry out a user study to assess how trustworthy GPT-4's assessments are, employing data from the TL;DR summarization task and two distinct prompting strategies for GPT-4. The \textbf{GPT-4 (S)} (simple) prompt asks which summary better encapsulates the key points of the original post. The \textbf{GPT-4 (C)} (concise) prompt, on the other hand, inquires not only about informativeness but also about which summary is more succinct; this prompt is included because we observed that, under the \textbf{GPT-4 (S)} prompt, GPT-4 favors longer, more redundant summaries compared to human preferences. The complete wording for both prompts can be found in Appendix~\ref{app:prompts}. We run three comparative tests: one using the highest-performing approach (DPO, temp. 0.25), one with the lowest-performing (PPO, temp. 1.0), and one with an intermediate method (SFT, temp. 0.25). This selection aims to represent a range of summary qualities. Each method is compared against PPO using its optimal temperature (greedy sampling). 

\begin{wraptable}{r}{0.47\textwidth}
    \centering
    \small
    \vspace{-1.5mm}
    \begin{tabular}{lccc}
    \toprule
        & \textbf{DPO} & \textbf{SFT} & \textbf{PPO-1} \\
        \cmidrule(lr){2-4}
        N respondents & 272 & 122 & 199 \\
        \midrule
        GPT-4 (S) win \% & 47 & 27 & 13 \\
        GPT-4 (C) win \% & 54 & 32 & 12 \\
        Human win \% & 58 & 43 & 17 \\
        \midrule
        GPT-4 (S)-H agree & 70 & 77 & 86 \\
        GPT-4 (C)-H agree & 67 & 79 & 85 \\
        H-H agree & 65 & - & 87 \\
        \bottomrule
    \end{tabular}
    \vspace{-1mm}
    \caption{Evaluation of win rates and per-example agreement for human judges and GPT-4 on TL;DR summarization tasks. \textbf{Agreement between humans and GPT-4 is comparable to the agreement observed between human raters.} Each test pits a summary produced by the specified method against a summary from PPO at temperature 0.}
    \vspace{-5mm}
    \label{tab:human_results}
\end{wraptable}

The data indicate that, for both prompt types, GPT-4's choices align with human preferences to a degree similar to that of human-human agreement, suggesting that GPT-4 can serve as a suitable surrogate for human judgment (note: due to the small number of human raters, multiple human evaluations were collected only for the DPO and PPO-1 comparisons). In general, the \textbf{GPT-4 (C)} prompt produces win rates that more closely mirror human outcomes; thus, we rely on this prompt for the main analyses reported in Section~\ref{sec:dpo-real-datasets}. For further information on the setup of the human evaluation, including details about the web interface and the roster of human participants, refer to Appendix~\ref{app:human-study}.

\section{Discussion}
Preference-based learning offers a robust and scalable approach for developing capable and aligned language models. In this work, we presented DPO, a straightforward method for training language models using preferences without the need for reinforcement learning. Instead of forcing the preference learning challenge into a conventional RL framework to leverage standard RL techniques, DPO establishes a direct relationship between language model policies and reward functions. This mapping allows for training models to adhere to human preferences directly via a simple cross-entropy loss, sidestepping the use of reinforcement learning and maintaining generality. DPO matches or surpasses the performance of current RLHF algorithms, such as those utilizing PPO, and requires minimal hyperparameter adjustment. As a result, DPO significantly lowers the barriers to leveraging human preferences for language model training.

\textbf{Limitations \& Future Work.} Our findings open up a number of key avenues for further investigation. One question is how well the DPO policy handles out-of-distribution generalization compared to methods that learn from a defined reward function. Preliminary evidence indicates DPO policies generalize on par with models trained using PPO, yet a broader evaluation is necessary. For instance, does self-labeling with the DPO policy enable effective learning from prompts that lack labels? Another area for exploration concerns how reward over-optimization presents itself within direct preference optimization, and whether the modest performance drop seen in Figure~\ref{fig:dialogue-main}-right can be attributed to this phenomenon. Although our experiments are limited to models with up to 6B parameters, future work could examine the feasibility and behavior of scaling DPO to much larger, cutting-edge models. In terms of evaluation, our observations show that GPT-4’s win rates are sensitive to the phrasing of the prompt, suggesting the need for further research into techniques for obtaining high-quality assessments from automated evaluators. Finally, DPO has the potential for a wide range of uses beyond aligning language models with human preferences, such as training generative models across different data modalities.

\end{document}