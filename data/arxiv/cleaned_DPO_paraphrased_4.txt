\title{Direct Preference Optimization: Your Language Model is Secretly a Reward Model}

\begin{abstract}
Although large-scale unsupervised language models (LMs) acquire extensive world knowledge and certain reasoning abilities, precisely guiding their outputs remains challenging due to the nature of their unsupervised training. Common approaches to improving steerability involve collecting human judgments on the relative quality of model outputs and then fine-tuning the LM to conform to these preferences, typically using reinforcement learning from human feedback (RLHF). However, RLHF involves a multi-step and often unstable process: first, a reward model is trained to mirror human choices, and then the original unsupervised LM is fine-tuned through reinforcement learning to optimize the predicted reward while remaining close to its initial behavior. In this work, we present a novel way of parameterizing the reward model within RLHF, which enables us to derive the associated optimal policy in closed form. This allows the standard RLHF objective to be addressed using a straightforward classification loss. The proposed method, termed \textit{Direct Preference Optimization} (DPO), is efficient, reliable, and resource-friendly, as it removes the necessity to sample from the LM during training or extensively tune hyperparameters. Our empirical results demonstrate that DPO can fine-tune LMs to reflect human preferences as well as or better than prior techniques. Remarkably, DPO outperforms PPO-based RLHF in controlling generative sentiment and achieves equal or superior results in summarization and single-turn dialogue tasks, all while offering a much simpler and more accessible training process.
\end{abstract}

\section{Introduction}
Large-scale unsupervised language models (LMs), when trained on extensive corpora, display remarkable and unexpected abilities~\citep{chowdhery2022palm, brown2020language, touvron2023llama,bubeck2023sparks}. Nonetheless, since these models are exposed to data reflecting a diverse range of human intentions, values, and competencies, they inevitably learn from both desirable and undesirable sources. For instance, although we want an AI coding assistant to \textit{recognize} frequent programming errors in order to fix them, we prefer its code generation to reflect the (possibly infrequent) instances of superior coding found in the training set. In a similar vein, it is beneficial for our language model to \textit{know about} a prevalent misconception held by half the population, but we would not want it to repeat this falsehood in half of all user queries about the topic. Thus, the ability to distinguish and elicit the \emph{preferred outputs and conduct} from a model’s vast reservoir of \textit{knowledge and competencies} is essential for crafting AI systems that are reliable, effective, and steerable~\citep{ouyang2022training}. Though common approaches rely on reinforcement learning (RL) to align LMs with human values, we will demonstrate that the RL-based objective prevalent in current methods can be equivalently addressed by a straightforward binary cross-entropy loss, streamlining the entire preference optimization process.

\begin{figure}
    \centering
    \includegraphics[width=0.999\textwidth]{figures/diagrams/teaser.png}
    \caption{\textbf{DPO aligns models to human preferences without reinforcement learning.} Conventional techniques for fine-tuning language models using human feedback typically begin by training a reward model on a collection of prompts paired with human-annotated preference rankings, followed by applying RL to learn a policy that maximizes this reward function. Unlike these approaches, DPO directly tunes the policy to better align with human preferences via a straightforward classification loss, learning an \textit{implicit} reward model from which the optimal policy can be obtained analytically.}
    \vspace{-2mm}
    \label{fig:teaser}
\end{figure}

Broadly speaking, current approaches teach language models the desired behaviors by leveraging curated collections of human preferences that reflect what people consider safe and useful. This preference acquisition phase takes place after an initial stage of large-scale unsupervised pre-training on vast text corpora. Although the simplest technique for preference learning involves supervised fine-tuning on carefully selected human demonstrations that illustrate high-quality outputs, the most effective strategies are based on reinforcement learning from human (or artificial) feedback (RLHF/RLAIF; \citep{christiano2017deep,bai2022constitutional}). RLHF approaches train a reward model on datasets of human preferences and then use reinforcement learning to adapt the language model's policy so it produces outputs that earn high reward, all while preventing the model from diverging too far from its original behavior. Despite leading to language models with strong conversational and coding performance, the RLHF pipeline is much more complicated than standard supervised learning—it requires training several language models and repeatedly sampling from the model policy during training, which results in substantial computational overhead.

In this work, we demonstrate a method for optimizing a language model to align with human preferences without relying on explicit reward modeling or reinforcement learning techniques. We introduce Direct Preference Optimization (DPO), an algorithm that implicitly targets the same goal as traditional RLHF approaches—maximizing reward under a KL-divergence constraint—but offers a simpler implementation and more straightforward training process. The DPO update intuitively raises the relative log likelihood of preferred responses compared to less favored ones, incorporating a dynamic, per-example importance weighting that mitigates the model degradation seen with a naive probability ratio objective. Similar to existing methods, DPO is grounded in a theoretical preference framework (for example, the Bradley-Terry model; \cite{bradley1952rankanalysis}) that evaluates the agreement between a reward function and observed preference data. However, in contrast to previous algorithms that employ the preference model to train a reward predictor followed by a policy optimized for this learned reward, DPO leverages a variable transformation to express the preference loss directly in terms of the policy. With a dataset of human preference judgments on model outputs, DPO optimizes the policy using a simple binary cross entropy loss, resulting in an optimal policy for an implicit reward function tailored to the collected preference data.

The primary contribution of our work is Direct Preference Optimization (DPO), a straightforward algorithm that enables training language models from preferences without relying on RL. Experimental results demonstrate that DPO matches or surpasses established approaches, such as PPO-based RLHF, in tasks like sentiment control, summarization, and conversational modeling, utilizing language models with as many as 6B parameters.

\section{Related Work}

Self-supervised language models of larger sizes have demonstrated the ability to perform certain tasks without any examples (zero-shot) \citep{radford2019language}, or by utilizing a small number of examples as prompts (few-shot) \citep{gpt3,megatron,chowdhery2022palm}. Nonetheless, their effectiveness on downstream tasks and ability to follow user instructions can be markedly enhanced by additional fine-tuning on curated datasets containing instructions and corresponding human completions \citep{mishra-etal-2022-cross,sanh2022multitask,chung2022scaling,thoppilan2022lamda}. This process, often called 'instruction-tuning,' allows LLMs to extend their capabilities to unseen instructions beyond those present in the instruction-tuning corpus and generally boosts their utility \citep{chung2022scaling}. 

Although instruction-tuning has shown promising results, collecting \textit{relative} human assessments of response quality is typically more scalable than obtaining expert demonstrations. As a result, subsequent research has refined LLMs using datasets that capture human preferences, leading to performance gains in tasks like translation \citep{kreutzer-etal-2018-reliability}, summarization \citep{stiennon2022learning,ziegler2020finetuning}, narrative generation \citep{ziegler2020finetuning}, and instruction adherence \citep{ouyang2022training,ramamurthy2023is}. The typical approach involves first training a neural network-based reward model to align with the preference dataset, often employing models such as the Bradley-Terry framework \citep{bradley1952rankanalysis}, and then optimizing the language model to maximize this reward signal via reinforcement learning techniques. Common strategies include REINFORCE \citep{williams1992reinforce}, proximal policy optimization (PPO; \cite{schulman2017proximal}), or related algorithms \citep{ramamurthy2023is}.

A related research avenue leverages LLMs trained for instruction following via human feedback to generate additional synthetic preference data, focusing on particular aspects such as safety or harmlessness \citep{bai2022constitutional}. Here, only minimal human oversight is needed, typically in the form of a textual rubric to guide the LLM's labeling process. 

This set of methodologies represents a synthesis of two areas: one concerned with applying reinforcement learning to language model training for various purposes~\citep{Ranzato2015SequenceLT,paulus2018a,wu2018learning}, and another centered on methods for learning directly from human feedback and preferences \citep{christiano2017deep,kupcsik2018learning}. While leveraging relative human preferences is attractive, fine-tuning LLMs with reinforcement learning remains a considerable practical hurdle; this paper introduces a theoretically-grounded method for optimizing relative preferences that avoids the need for RL.

Beyond linguistic contexts, the study of learning policies from preferences has been explored in both bandit and reinforcement learning frameworks, with numerous strategies proposed. Contextual bandit learning utilizing preferences or rankings instead of explicit rewards is referred to as the contextual dueling bandit (CDB; \cite{yue2012karmed,dudik2015contextual}). When absolute rewards are unavailable, theoretical work on CDBs replaces the concept of an optimal policy with that of a \textit{von Neumann winner}, defined as a policy whose expected probability of outperforming \textit{every} other policy is at least 50\% \citep{dudik2015contextual}. Nonetheless, in the CDB scenario, preference feedback is provided in an online fashion, whereas learning from human preferences typically involves a predetermined set of offline, preference-labeled action pairs \citep{yan2022human}. In a similar vein, \textit{preference-based RL} (PbRL) uses binary preferences derived from an \textit{unknown} 'scoring' mechanism instead of direct rewards \citep{BusaFekete2014,ruiz2023dueling}. A variety of PbRL algorithms are available, some of which leverage off-policy preference datasets, but these often require explicitly modeling the underlying scoring (reward) function first and then optimizing based on it \citep{jain2013learning,BusaFekete2014,christiano2017deep,sadigh2017active,kupcsik2018learning}. In contrast, we propose a unified approach to policy learning that aims to optimize policies directly according to observed preferences.

\section{Preliminaries}\label{section:prelims}

We summarize the RLHF process described in \citeauthor{ziegler2020finetuning}, as well as subsequent works such as \citep{stiennon2022learning, bai2022training, ouyang2022training}. The standard workflow involves three main steps: (1) supervised fine-tuning (SFT), (2) collecting preference data and training a reward model, and (3) reinforcement learning-based optimization.

\textbf{SFT}: RLHF usually starts with supervised fine-tuning of a pre-trained LM on curated, task-specific datasets (such as for dialogue or summarization), resulting in a model denoted as $\pisft$.

\textbf{Reward Modeling Stage}: During this stage, the SFT model is fed prompts $x$ to generate answer pairs $(y_1, y_2) \sim \pisft(y \mid x)$. These paired responses are then evaluated by human annotators, who indicate their preference for one response over the other, recorded as $y_w \succ y_l \mid x$, with $y_w$ and $y_l$ identifying the chosen and rejected answers from $(y_1, y_2)$, respectively. Preferences are modeled as arising from an unknown reward function $r^*(y, x)$, to which we do not have direct access. To capture these preferences, models such as Bradley-Terry (BT) \cite{bradley1952rankanalysis} are commonly used (though alternatives like the Plackett-Luce ranking model \citep{plackett1975analysis, luce2012individual} are also applicable if multi-way rankings are available). According to the BT framework, the probability distribution over human choices $p^*$ takes the form:
\begin{equation}\label{eq:bradley-terry}
    p^*(y_1\succ y_2 \mid x)=\frac{\exp\left(r^*(x, y_1)\right)}{\exp\left(r^*(x, y_1)\right) + \exp\left(r^*(x, y_2)\right)}.
\end{equation}
Given a fixed dataset of preference comparisons $\mathcal{D} = \{x^{(i)}, y_w^{(i)}, y_l^{(i)}\}_{i=1}^N$ sampled according to $p^*$, we can introduce a parameterized reward model $r_{\phi}(x, y)$ and fit its parameters by maximizing the likelihood. By interpreting the setup as a binary classification task, the negative log-likelihood objective is:
\begin{equation}\label{eq:reward_model}
    \mathcal{L}_R(r_{\phi}, \mathcal{D}) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\bigl[\log \sigma(r_{\phi}(x, y_w)- r_{\phi}(x, y_l))\bigr]
\end{equation}
where $\sigma$ is the sigmoid function. In large language models, $r_{\phi}(x, y)$ is typically initialized from the SFT model $\pisft(y \mid x)$, with a linear head appended to the last transformer layer to produce a single scalar reward value \cite{ziegler2020finetuning}. To control reward variance, previous work typically normalizes rewards such that  $\mathbb{E}_{x,y\sim \mathcal{D}}\left[r_\phi(x, y)\right] = 0$ across all $x$.

\textbf{RL Fine-Tuning Stage}: In the RL fine-tuning process, the previously acquired reward model supplies evaluative signals to the language model. Consistent with earlier research~\citep{jaques2017sequence, jaques2020human}, the optimization objective is expressed as:
\begin{equation}\label{eq:RL}
\max_{\pi_{\theta}}  \mathbb{E}_{x\sim \mathcal{D}, y\sim \pi_{\theta}(y \mid x)}\left[r_{\phi}(x, y)\right] - \beta\mathbb{D}_{\textrm{KL}}\left[\pi_{\theta}(y\mid x)\mid \mid \piref(y\mid x)\right],
\end{equation}
where $\beta$ modulates the permissible divergence from the base reference policy $\piref$, which corresponds to the initially supervised fine-tuned model $\pisft$.
In actual implementation, the policy $\pi_\theta$ is typically initialized with $\pisft$. This regularization is essential to ensure that the model does not stray excessively from the data distribution where the reward model provides reliable estimates, and also to preserve output diversity and avoid collapse onto a narrow set of high-reward sequences. Owing to the discrete output space of language, this objective cannot be directly differentiated, so reinforcement learning techniques are employed for optimization. The prevalent methodology~\citep{ziegler2020finetuning, stiennon2022learning, bai2022training, ouyang2022training} forms the reward as ${r(x, y) = r_{\phi}(x, y) -\beta (\log \pi_{\theta}(y\mid x) - \log \piref(y\mid x))}$ and applies PPO \cite{schulman2017proximal} for maximization.

\section{Direct Preference Optimization}\label{sec:DPO}

Driven by the difficulties inherent in using reinforcement learning algorithms for large-scale tasks like language model fine-tuning, we aim to formulate a straightforward method for policy optimization that relies directly on preference data. Distinct from previous RLHF techniques, which first learn a reward function and subsequently apply RL for optimization, our method uses a specific reward model parameterization allowing us to extract the corresponding optimal policy analytically, removing the need for an RL training process.

As we elaborate in the following sections, our central idea is to utilize an analytical relationship between reward functions and their associated optimal policies, allowing us to reparameterize a loss over rewards as a loss over policies. This change-of-variables strategy sidesteps the necessity of explicitly training a separate reward model, while still enabling optimization according to established models of human preferences, such as the Bradley-Terry framework. Effectively, the policy network embodies both the language model and the implicit reward function.

\textbf{Deriving the DPO objective.} We begin with the standard RL objective described in Eq.~\ref{eq:RL}, incorporating a generic reward function $r$. Building on earlier studies~\citep{peters2007reinforcement, peng2019advantage, korbak2022reinforcement, go2023aligning}, it is easy to verify that the optimal policy for the KL-regularized reward maximization problem in Eq.~\ref{eq:RL} has the following structure:
\begin{equation}\label{eq:op_policy}
    \pi_r(y\mid x) = \frac{1}{Z(x)}\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right),
\end{equation}
where the normalization term $Z(x) =\sum_{y}\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)$ is called the partition function. The detailed derivation is provided in Appendix \ref{app:derivation1}. Even when the maximum likelihood estimate $r_{\phi}$ is used to approximate the true reward $r^*$, computing $Z(x)$ remains computationally demanding \citep{korbak2022reinforcement, go2023aligning}, limiting the practicality of this formulation. Nevertheless, Eq.~\ref{eq:op_policy} can be rearranged to express the reward function in relation to the optimal policy $\pi_r$, the reference policy $\piref$, and the partition function $Z(\cdot)$. To do so, we take the logarithm of both sides of Eq.~\ref{eq:op_policy} and manipulate the expression, arriving at:
\begin{equation}\label{eq:main_eq}
    r(x,y) =\beta \log \frac{\pi_r(y\mid x)}{\piref(y\mid x)} + \beta \log Z(x).
\end{equation}
This reparameterization applies to the ground-truth reward $r^*$ and its optimal policy $\pi^*$. Notably, the Bradley-Terry model depends only on reward differences between two outputs: ${p^*(y_1 \succ y_2 \mid x) = \sigma(r^*(x, y_1) - r^*(x, y_2))}$. Substituting the reparameterization from Eq.~\ref{eq:main_eq} for $r^*(x,y)$ into the preference model in Eq.~\ref{eq:bradley-terry}, the partition function $Z(x)$ cancels out, allowing the human preference likelihood to be written purely in terms of $\pi^*$ and $\piref$. As a result, the optimal RLHF policy $\pi^*$ must satisfy the following under the Bradley-Terry framework:
\begin{equation}\label{eq:objective}
    p^*(y_1\succ y_2 \mid x)=\frac{1}{1 + \exp\left(\beta \log \frac{\pi^*(y_2\mid x)}{\piref(y_2\mid x)} - \beta \log \frac{\pi^*(y_1\mid x)}{\piref(y_1\mid x)}\right)}
\end{equation}
Details are provided in Appendix~\ref{app:derivation2}. While Eq.~\ref{eq:objective} employs the Bradley-Terry approach, the same logic extends to the more general Plackett-Luce models~\citep{plackett1975analysis, luce2012individual}, as elaborated in Appendix~\ref{app:plackett_luce_models}.

At this point, with human preference probabilities now expressed in terms of the optimal policy instead of the reward function, we can set up a maximum likelihood objective for a parameterized policy $\pi_\theta$. In parallel with the reward modeling formulation (see Eq.~\ref{eq:reward_model}), our policy objective is given by:
\begin{equation}\label{eq:optimum_model}
    \mathcal{L}_\text{DPO}(\pi_{\theta}; \piref) = -\mathbb{E}_{(x, y_w, y_l)\sim \mathcal{D}}\left[\log \sigma \left(\beta \log \frac{\pi_{\theta}(y_w\mid x)}{\piref(y_w\mid x)} - \beta \log \frac{\pi_{\theta}(y_l\mid x)}{\piref(y_l\mid x)}\right)\right].
\end{equation}
In this framework, we indirectly learn a reward via an alternative parameterization, where $\pi_\theta$ directly represents the optimal policy. Additionally, because this approach can be interpreted as fitting a reparameterized Bradley-Terry model, it possesses desirable theoretical guarantees, such as consistency provided that the preference data distribution meets certain criteria \cite{bong2022generalized}. Further theoretical aspects of DPO and its connections to related work are elaborated in Section~\ref{sec:theory}.

\textbf{What is the effect of the DPO update?} To gain a mechanistic insight into DPO, it is helpful to inspect the gradient of the DPO loss $\mathcal{L}_\text{DPO}$. The gradient with respect to the model parameters $\theta$ is expressed as:
\begin{multline*}\label{eq:gradient}
    \nabla_\theta \mathcal{L}_\text{DPO}(\pi_\theta;\piref) = \\ -\beta\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \bigg[\underbrace{\sigma(\hat{r}_\theta(x, y_l) - \hat{r}_\theta (x, y_w))}_\text{higher contribution when reward ranking is incorrect}\bigg[\underbrace{\nabla_\theta\log \pi(y_w \mid x)}_\text{raise probability of $y_w$} - \underbrace{\nabla_\theta\log\pi(y_l \mid x)}_\text{lower probability of $y_l$}\bigg]\bigg],
\end{multline*}
where $\hat{r}_\theta(x, y) = \beta \log \frac{\pi_\theta(y \mid x)}{\piref(y \mid x)}$ defines the model’s implicit reward as determined by the policy $\pi_\theta$ and the reference $\piref$ (see Section~\ref{sec:theory} for more details). In essence, the gradient of $\mathcal{L}_\text{DPO}$ works to make preferred completions $y_w$ more probable and less preferred completions $y_l$ less probable. Crucially, each training example's influence is weighted by how much the model’s reward function $\hat{r}_\theta$ overestimates the less preferred completion, scaled by $\beta$—that is, the degree to which the reward model’s ranking is wrong, modulated by the KL regularization strength. Our empirical results indicate this weighting is vital: omitting this coefficient (using a simplified variant) can lead to model collapse (see Appendix Table~\ref{tab:unlikelihood_generations}).

\textbf{DPO summary.}  
The typical DPO workflow consists of the following steps: 1) For each prompt $x$, generate completions $y_1, y_2 \sim \piref(\cdot \mid x)$ and assign human preference labels to create the offline preference dataset $\mathcal{D} = \{x^{(i)}, y_w^{(i)}, y_l^{(i)}\}_{i=1}^N$; 2) update the language model $\pi_\theta$ by minimizing $\mathcal{L}_\text{DPO}$, using the chosen $\piref$, dataset $\mathcal{D}$, and specified $\beta$ value.  
In real-world settings, it is preferable to utilize existing public preference datasets rather than generating new samples and collecting human annotations. Since these preference datasets are typically obtained via $\pisft$, we set $\piref = \pisft$ whenever possible. If $\pisft$ is unavailable, we determine $\piref$ by maximizing the likelihood of preferred completions ${(x, y_w)}$, i.e., ${\piref = \argmax_{\pi}\mathbb{E}_{x, y_w \sim \mathcal{D}}\left[\log \pi(y_w \mid x)\right]}$. This method reduces the mismatch between the ideal reference distribution (which we cannot access) and the $\piref$ used in DPO. For more on implementation specifics and hyperparameter choices, refer to Appendix~\ref{app:implementation}.

\section{Theoretical Examination of DPO}
Here, we present a deeper analysis of the DPO approach, supply theoretical justification, and connect the strengths of DPO to challenges observed in actor-critic methods applied in RLHF, like PPO~\cite{schulman2017proximal}.

\label{sec:theory}

\subsection{Your Language Model Is Secretly a Reward Model} DPO achieves both the avoidance of explicit reward modeling and the need for RL by employing a single maximum likelihood objective. Importantly, the optimization target in Eq.~\ref{eq:main_eq} corresponds to a Bradley-Terry model, but with a reward representation of $r^*(x, y) = \beta \log\frac{\pi^*_\theta(y \mid x)}{\piref(y \mid x)}$. Here, we train the parametric model $\pi_{\theta}$, in a way that is equivalent—after a change of variables—to the reward model learning in Eq.~\ref{eq:reward_model}. In this part, we develop the theoretical justification for this reparameterization, demonstrate that it imposes no restrictions on the set of possible reward models that can be captured, and show that it enables exact retrieval of the optimal policy. We start by introducing an equivalence relation for reward functions.

\begin{definition}
Two reward functions $r(x, y)$ and $r'(x, y)$ are defined as equivalent if and only if there exists a function $f$ such that $r(x, y) - r'(x, y) = f(x)$.
\end{definition}
Clearly, this forms an equivalence relation that divides all reward functions into equivalence classes. With this, we can present the next two lemmas:

\begin{lemma}\label{lemma:same_prefrence} Within the Plackett-Luce model, and specifically the Bradley-Terry setup, any two reward functions belonging to the same class generate identical preference distributions.
\end{lemma}

\begin{lemma}\label{lemma:same_policy}
    Any two reward functions that belong to the same equivalence class produce identical optimal policies under the constrained RL setting.
\end{lemma}
The proofs are straightforward and can be found in Appendix \ref{app:lemma1}. The initial lemma reflects a common under-specification challenge present in the Plackett-Luce model family \cite{plackett1975analysis}. Owing to this ambiguity, it is typical to introduce extra identifiability constraints to provide assurances about the MLE solutions from Eq. \ref{eq:reward_model} \cite{bong2022generalized}. The second lemma asserts that all reward functions in the same equivalence class induce the same optimal policy. Thus, our ultimate goal reduces to identifying any representative reward function from the correct equivalence class. We establish the following Theorem, with proof in Appendix~\ref{app:thm1}:
\begin{theorem}\label{thm:main}
    Assuming mild conditions, every equivalence class of rewards consistent with Plackett-Luce (and, specifically, Bradley-Terry) models admits a representation via the reparameterization ${r(x, y) = \beta \log \frac{\pi(y\mid x)}{\piref(y\mid x)}}$ for some model $\pi(y\mid x)$ and a fixed reference model $\piref(y \mid x)$.
\end{theorem}
\begin{sproof}
    Take an arbitrary reward function $r(x, y)$, which leads to an optimal model $\pi_r(y \mid x)$ as defined in Eq. \ref{eq:op_policy}. We demonstrate that a reward function from the same equivalence class as $r$ can be written using the above reparameterization. Define the projection operator $f$ as
\begin{equation}
    f(r; \piref, \beta)(x, y) = r(x, y) - \beta\log\sum_{y}\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)
\end{equation}
Here, $f$ normalizes $r(x, y)$ by subtracting the logarithm of the partition function associated with $\pi_r$. Since this normalization term only depends on $x$, $f(r; \piref, \beta)(x, y)$ belongs to the equivalence class of $r(x, y)$. Now, substituting $r$ from the right-hand side of Eq.~\ref{eq:main_eq} (which is valid for any $r$), we obtain $f(r; \piref, \beta)(x, y) = \beta \log \frac{\pi_r(y\mid x)}{\piref(y\mid x)}$. Therefore, this projection yields a reward in the equivalence class of $r$ with the desired structure, confirming that the proposed reparameterization covers the full generality of the reward model.
\end{sproof}
Alternatively, Theorem~\ref{thm:main} can be seen as identifying exactly which reward function from each equivalence class is chosen by the DPO reparameterization, i.e., the reward function that fulfills:
\begin{equation}\label{eq:lag_p}
     \sum_{y}\underbrace{\piref(y\mid x)\exp\left(\frac{1}{\beta}r(x, y)\right)}_{=\pi(y\mid x)\text{, by Thm.~\ref{thm:main} reparam.}} = 1,
\end{equation}
meaning that $\pi(y\mid x)$ forms a valid probability distribution (i.e., nonnegative and sums to 1).
Moreover, according to Eq.~\ref{eq:op_policy}, Eq.~\ref{eq:lag_p} is precisely the partition function associated with the optimal policy for $r(x, y)$.
The main insight behind the DPO algorithm is that by imposing specific constraints on the under-constrained Plackett-Luce (and, in particular, Bradley-Terry) family of preference models, we can preserve the full set of representable reward functions, while ensuring the optimal policy in Eq. \ref{eq:op_policy} remains analytically tractable for every prompt $x$.

\subsection{Instability of Actor-Critic Algorithms}
Our framework also enables analysis of the instability issues found in standard actor-critic methods employed for RLHF, such as PPO. We adhere to the RLHF workflow, specifically concentrating on the RL fine-tuning stage as described in Section \ref{section:prelims}. This approach allows us to relate to the control as inference perspective \cite{levine2018reinforcement} when dealing with the constrained RL problem outlined in \ref{eq:RL}. Assuming a parameterized policy model $\pi_{\theta}(y\mid x)$, we seek to minimize $\mathbb{D}_{\text{KL}}[\pi_{\theta}(y|x) \parallel \pi^*(y\mid x)]$, where $\pi^*$ is the optimal policy from Eq. \ref{eq:optimum_model} determined by the reward function $r_{\phi}(y, x)$. Through some mathematical manipulation, we arrive at the following objective:
\begin{equation}\label{eq:AC}
    \max_{\pi_{\theta}}\mathbb{E}_{\pi_{\theta}(y\mid x)}\left[\underbrace{r_{\phi}(x, y) -\beta\log\sum_{y}\piref(y\mid x)\exp\left(\frac{1}{\beta}r_{\phi}(x, y)\right)}_{f(r_{\phi}, \piref, \beta)} - \underbrace{\beta\log\frac{\pi_{\theta}(y\mid x)}{\piref(y\mid x)}}_{\text{KL}}\right]
\end{equation}
This objective mirrors those utilized in previous studies \citep{ziegler2020finetuning, stiennon2022learning, bai2022training, ouyang2022training}, which employ the DPO-equivalent reward for the class of $r_{\phi}$. Here, the normalization component inside $f(r_{\phi}, \piref, \beta)$ can be interpreted as the soft value function associated with the reference policy $\piref$. Although this normalization does not change the optimal policy, omitting it may increase the variance of policy gradients, thereby destabilizing learning. One way to handle this normalization is by estimating it with a learned value function; however, this approach can be hard to optimize. Alternatively, earlier works have normalized rewards by referencing a human completion baseline, effectively providing a single-sample Monte Carlo estimate for the normalization. The DPO reparameterization, on the other hand, yields a reward formulation that eliminates the need for such baselines.

\section{Experiments}
In this section, we experimentally assess how well DPO can learn policies directly from preference data. To begin, within a controlled text-generation scenario, we investigate: how effectively does DPO balance reward maximization and minimizing KL divergence to the reference policy when compared with prevalent preference learning methods such as PPO? Following this, we test DPO on larger-scale models and more challenging RLHF tasks, including both summarization and dialogue. Our observations show that DPO generally matches or surpasses strong baselines like RLHF with PPO, as well as approaches that select the best of $N$ sampled outputs according to a learned reward function, even with minimal hyperparameter tuning. Prior to detailing our findings, we outline the experimental methodology; further information can be found in Appendix~\ref{app:exp_details}.

\textbf{Tasks.} We investigate three distinct open-ended text generation challenges in our experiments. In every task, algorithms learn a policy using a dataset of preferences $\mathcal{D} = \bigl\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\bigr\}_{i=1}^N$. For \textbf{controlled sentiment generation}, $x$ serves as the beginning segment of a movie review from the IMDb dataset \cite{maas-EtAl:2011:ACL-HLT2011}, and the policy is required to produce $y$ that expresses positive sentiment. To enable a systematic evaluation, we \textit{generate} preference pairs for model outputs using a pre-trained sentiment classifier, ensuring that $p(\text{positive}\mid x,y_w)>p(\text{positive}\mid x,y_l)$. In the SFT setting, GPT-2-large is fine-tuned on the training portion of the IMDb dataset until performance stabilizes (see App~\ref{app:sentiment_details} for more information). In the \textbf{summarization} task, $x$ is a Reddit forum post, and the policy’s objective is to output a concise summary $y$ covering the main ideas. Following previous approaches, we utilize the Reddit TL;DR dataset \citep{volske-etal-2017-tl} along with human preference annotations collected by \citeauthor{stiennon2022learning}. The SFT model used here is fine-tuned on human-crafted summaries of forum posts\footnote{\url{https://huggingface.co/CarperAI/openai_summarize_tldr_sft}} and employs the TRLX framework \citep{leandro_von_werra_2023_7790115} for RLHF. The preference annotations were sourced by \citeauthor{stiennon2022learning} from samples generated by a different, but similarly trained, SFT model. Lastly, the \textbf{single-turn dialogue} task considers $x$ as a user inquiry—ranging from astrophysics questions to advice requests—where the policy must generate a relevant and useful reply $y$. We adopt the Anthropic Helpful and Harmless dialogue dataset \citep{bai2022training}, which contains 170k conversations between humans and an automated assistant. Each dialogue concludes with two responses from a large, unspecified language model and a human preference label identifying the favored completion. Since there is no pre-existing SFT model in this case, we fine-tune a publicly available language model exclusively on preferred completions to create the SFT model.

\begin{figure}
    \centering
    \includegraphics[width=0.50\textwidth]{figures/results/frontier.pdf}
    \includegraphics[width=0.49\textwidth]{figures/results/tldr_winrate_vs_temp.pdf}
    \caption{\textbf{Left.} The trade-off curve of expected reward against KL divergence with the reference policy. DPO achieves the highest expected reward across all values of KL, indicating superior optimization effectiveness. \textbf{Right.} TL;DR summarization win rates compared to human-authored summaries, evaluated by GPT-4. DPO outperforms the peak performance of PPO on summarization tasks and maintains greater stability with respect to sampling temperature variations.}
    \vspace{-2mm}
    \label{fig:frontier-tldr-main}
\end{figure}

\textbf{Evaluation.} We employ two distinct evaluation strategies in our experiments. To assess how well each algorithm optimizes the constrained reward maximization objective, we conduct controlled sentiment generation tests, measuring each algorithm’s trade-off curve between achieved reward and KL-divergence relative to the reference policy; this curve is feasible to compute since we have access to the true reward function (given by a sentiment classifier). In practical applications, however, the true reward function is unavailable. Thus, we evaluate each algorithm based on its \textit{win rate} compared to a baseline policy, using GPT-4 as a stand-in for human judges to assess summary quality and helpfulness in summarization and single-turn dialogue tasks, respectively. For summarization, the baseline consists of reference summaries from the test set; for dialogue, we use the preferred response in the test data as the baseline. Although prior work indicates that LMs may outperform standard metrics as automated evaluators \citep{Chen2023ExploringTU}, we also perform a human study, detailed in Sec.~\ref{sec:human-judgments}, to support the use of GPT-4 for evaluation. Our results show GPT-4’s assessments are highly correlated with human judgments, with levels of agreement between humans and GPT-4 matching or surpassing typical inter-annotator reliability.

\textbf{Methods.} Beyond DPO, we assess multiple established techniques for aligning language models with human preferences. For the summarization task, we utilize zero-shot prompting with \textbf{GPT-J} \citep{gpt-j}, while in the dialogue task, we use 2-shot prompting with \textbf{Pythia-2.8B} \citep{biderman2023pythia}. We further include evaluations of the \textbf{SFT} model and \textbf{Preferred-FT}, which refers to a model refined via supervised learning on the selected completion $y_w$—sourced either from the SFT model (for sentiment control and summarization) or a standard LM (for single-turn dialogue). Another related supervised approach is \textbf{Unlikelihood}~\citep{welleck2019neural}, where the objective is to maximize the probability of $y_w$ while simultaneously \textit{minimizing} the probability for $y_l$; we introduce an optional weighting factor $\alpha\in[0,1]$ on the unlikelihood component. We also analyze the performance of \textbf{PPO} \citep{schulman2017proximal}, which relies on a reward model trained with preference data, alongside \textbf{PPO-GT}—an idealized variant that accesses the true reward function in the controlled sentiment scenario. In sentiment experiments, two PPO-GT versions are considered: a standard implementation \cite{leandro_von_werra_2023_7790115}, and a custom version that applies reward normalization and hyperparameter adjustments for enhanced results (these customizations are likewise used for standard PPO with learned rewards). Lastly, we incorporate the \textbf{Best of $N$} baseline, which samples $N$ outputs from the SFT (or Preferred-FT for dialogue) and picks the top response according to a reward model trained on preferences. Although this method achieves strong results by separating reward modeling from PPO training, it is computationally intensive for even modest $N$, as it requires generating $N$ candidates per query during evaluation.

\subsection{How well can DPO optimize the RLHF objective?}

\begin{figure}
    \centering
    \includegraphics[width=0.50\textwidth]{figures/results/dialogue_winrate_vs_temp.pdf}
    \includegraphics[width=0.49\textwidth]{figures/results/dialogue_winrate_vs_steps.pdf}
    \caption{\textbf{Left.} GPT-4-calculated win rates for the Anthropic-HH one-step dialogue task; among all approaches, only DPO outperforms the preferred summaries within the Anthropic-HH evaluation set. \textbf{Right.} Win rate trends across training steps for a range of sampling temperatures. Across various temperatures, DPO consistently maintains an advantage over dataset labels throughout training.}
    \vspace{-2mm}
    \label{fig:dialogue-main}
\end{figure}

The KL-regularized reward maximization objective commonly utilized in standard RLHF methods aims to maximize reward while simultaneously preventing the policy from straying significantly from the reference policy. Thus, when evaluating different approaches, it is essential to consider both the obtained reward and the KL divergence; securing marginally greater reward at the expense of a much larger KL is often undesirable. Figure~\ref{fig:frontier-tldr-main} presents the reward-KL frontier for several algorithms within the sentiment task. For each algorithm, we conduct multiple training runs, each with a different setting for policy conservativeness (target KL values of $\{3,6,9,12\}$ for PPO, $\beta$ values of $\{0.05,0.1,1,5\}$, and $\alpha$ values of $\{0.05,0.1,0.5,1\}$ for unlikelihood, and varied random seeds for preferred-FT), amounting to 22 training runs in total. Every 100 training steps until convergence, we assess the policy on a collection of held-out prompts, measuring the mean reward using the true reward function as well as the mean sequence-level KL divergence\footnote{Namely, the sum of KL-divergences across all timesteps.} between the learned and reference policies, $\text{KL}\left(\pi\mid \mid \piref\right)$. Our findings reveal that DPO establishes a markedly superior frontier, securing higher rewards while keeping KL values low. This outcome stands out for multiple reasons. First, although DPO and PPO target the same objective, DPO demonstrates considerably higher efficiency, with its reward/KL frontier consistently outperforming PPO. Second, DPO surpasses PPO’s frontier even in scenarios where PPO has access to the ground truth rewards (PPO-GT).

\subsection{Is DPO applicable to large-scale preference datasets?}
\label{sec:dpo-real-datasets}
We proceed by assessing how well DPO performs when fine-tuned on summarization and single-turn dialogue datasets. In the context of summarization, it is known that automated metrics like ROUGE often show weak correlation with human judgment~\citep{stiennon2022learning}. Previous studies have demonstrated that using PPO to fine-tune language models based on human preferences yields more effective summaries. To compare various approaches, we generate completions from the test set of the TL;DR summarization dataset and calculate the mean win rate relative to the test set reference completions. Samples are drawn at temperatures from 0.0 up to 1.0, with the resulting win rates displayed in Figure~\ref{fig:frontier-tldr-main} (right). The DPO, PPO, and Preferred-FT models all start from the same GPT-J SFT base\footnote{\url{https://huggingface.co/CarperAI/openai_summarize_tldr_sft}}. Our results indicate that DPO reaches a win rate near 61\% at temperature 0.0, outperforming PPO, which achieves around 57\% at its optimal setting. In addition, DPO secures a superior maximum win rate compared to the $N$-best baseline. It should be highlighted that DPO’s $\beta$ hyperparameter was not carefully optimized, suggesting these outcomes could be improved. DPO also displays significantly greater resilience to changes in sampling temperature, whereas PPO's effectiveness diminishes to that of the initial GPT-J model at higher temperatures. The Preferred-FT approach yields little improvement over SFT. We further conduct a direct human evaluation of DPO against PPO in Section~\ref{sec:human-judgments}, where DPO samples at temperature 0.25 were chosen over PPO samples at temperature 0 in 58\% of cases.

For single-turn dialogue, we assess the various approaches using a subset of the test split from the Anthropic HH dataset \citep{bai2022training}, focusing on examples featuring a single exchange between a human and an assistant. For GPT-4 evaluations, we use the preferred completions from the test set as ground truth, and calculate each method's win rate accordingly. Since there isn't an established SFT model for this benchmark, we begin with a pre-trained Pythia-2.8B, apply Preferred-FT to train a reference model on selected completions to ensure outputs remain within distribution, and then continue training with DPO. Our comparisons include the Best of 128 Preferred-FT completions (our experiments show the Best of $N$ baseline levels off at $N=128$ for this task; refer to Appendix Figure~\ref{fig:best-of-n}), as well as a 2-shot prompted variant of the Pythia-2.8B base model. We observe that DPO matches or surpasses the highest-performing temperatures across these methods. Additionally, we evaluate an RLHF model trained with PPO on the Anthropic HH dataset \footnote{\url{https://huggingface.co/reciprocate/ppo_hh_pythia-6B}} provided by a prominent source \footnote{\url{https://github.com/CarperAI/trlx/tree/main/examples/hh}}; however, we are unable to identify a prompt or sampling temperature that results in superior performance compared to the unmodified Pythia-2.8B. Given our findings on TL;DR and the shared reward optimization in both approaches, we use Best of 128 as a rough proxy for PPO performance. In summary, DPO is the only efficient method to outperform the reference completions on the Anthropic HH dataset, and offers comparable or improved results relative to the computationally intensive Best of 128 baseline. Finally, Figure~\ref{fig:dialogue-main} demonstrates that DPO attains peak performance with relatively few iterations.

\subsection{Generalization to a new input distribution}

\begin{wraptable}{r}{0.375\textwidth}
    \small
    \vspace{-10mm}
    \begin{tabular}{ccc}
        \toprule
        & \multicolumn{2}{c}{\textbf{Win rate against ground truth}} \\
        \cmidrule(lr){2-3}
        \textbf{Algorithm} & Temp $0$ & Temp $0.25$ \\
        \midrule
        DPO & 0.36 & 0.31 \\
        PPO & 0.26 & 0.23 \\
        \bottomrule
    \end{tabular}
    \caption{GPT-4 win percentages versus ground truth summaries for out-of-distribution CNN/DailyMail articles.}
    \vspace{-3mm}
    \label{tab:ood}
\end{wraptable}

To further assess how PPO and DPO perform under distributional changes, we test the policies trained in our Reddit TL;DR summarization study on a different domain: news articles from the test split of the CNN/DailyMail dataset \citep{nallapati-etal-2016-abstractive}. We use the optimal sampling temperatures identified for TL;DR (0 and 0.25). Table~\ref{tab:ood} shows the outcomes. We measured the GPT-4 win rate versus the reference summaries in these datasets, employing the same GPT-4 (C) prompt as for Reddit TL;DR but substituting ``forum post'' with ``news article''. On this out-of-distribution data, DPO still outperforms PPO by a notable margin. These results offer preliminary support that DPO policies can generalize at least as well as PPO policies, despite DPO not utilizing the extra unlabeled Reddit TL;DR prompts that are available to PPO.

\subsection{Corroborating GPT-4 assessments with human evaluations}
\label{sec:human-judgments}
We run a human evaluation to assess the trustworthiness of GPT-4's decisions, leveraging results from the TL;DR summarization task and two distinct GPT-4 prompt designs. The \textbf{GPT-4 (S)} (simple) prompt asks which summary better captures the key information in the post. The \textbf{GPT-4 (C)} (concise) prompt requests which summary is not only better but also more concise; we include this prompt as GPT-4, when prompted simply, tends to favor longer and more repetitive summaries compared to human raters. Complete prompt wording is available in Appendix~\ref{app:prompts}. We make three key method comparisons: the strongest (DPO, temperature 0.25), the weakest (PPO, temperature 1.0), and a \begin{wraptable}{r}{0.47\textwidth}
    \centering
    \small
    \vspace{-1.5mm}
    \begin{tabular}{lccc}
    \toprule
        & \textbf{DPO} & \textbf{SFT} & \textbf{PPO-1} \\
        \cmidrule(lr){2-4}
        N respondents & 272 & 122 & 199 \\
        \midrule
        GPT-4 (S) win \% & 47 & 27 & 13 \\
        GPT-4 (C) win \% & 54 & 32 & 12 \\
        Human win \% & 58 & 43 & 17 \\
        \midrule
        GPT-4 (S)-H agree & 70 & 77 & 86 \\
        GPT-4 (C)-H agree & 67 & 79 & 85 \\
        H-H agree & 65 & - & 87 \\
        \bottomrule
    \end{tabular}
    \vspace{-1mm}
    \caption{A comparison of human and GPT-4 win rates along with per-decision agreement in the TL;DR summarization setting. \textbf{Humans are about as likely to agree with GPT-4 as they are with each other.} Each comparison pairs a summary from the listed method with one from PPO using temperature 0.}
    \vspace{-5mm}
    \label{tab:human_results}
\end{wraptable}mid-range (SFT, temperature 0.25) approach, thereby sampling a variety of summary qualities. In each case, these methods are pitted against greedily-sampled PPO outputs (the most competitive temperature). Analysis reveals that, for both prompts, GPT-4 and humans show similar rates of agreement as humans do among themselves, indicating that GPT-4 is a viable stand-in for human judgment (due to limited human resources, multiple rater opinions are gathered only for DPO and PPO-1 cases). Notably, the \textbf{GPT-4 (C)} prompt yields win rates more closely aligned with human preferences, leading us to use this version for the main findings in Section~\ref{sec:dpo-real-datasets}. For further details on the human evaluation process, including the web platform and the roster of human participants, refer to Appendix~\ref{app:human-study}.

\section{Discussion}
Preference-based learning offers an effective and scalable approach for developing robust, aligned language models. In this work, we have presented DPO, a straightforward training methodology that enables language models to learn from preferences without relying on reinforcement learning techniques. Instead of adapting the preference learning task to fit conventional RL frameworks and utilizing standard RL algorithms, DPO establishes a direct correspondence between reward functions and language model policies. This allows for training models to align with human preferences using a basic cross-entropy loss, completely bypassing reinforcement learning while retaining full generality. Remarkably, DPO achieves comparable or superior performance to prevailing RLHF methods, including those that use PPO, with almost no hyperparameter tuning required. As a result, DPO significantly lowers the difficulty of training language models according to human preferences.

\textbf{Limitations \& Future Directions.} Our findings open up multiple avenues for further investigation. One key question is how well the DPO policy generalizes to out-of-distribution data, especially in comparison to approaches that utilize explicit reward functions. Preliminary evidence indicates that DPO policies might generalize on par with models trained via PPO, but more thorough evaluation is required. For instance, it remains to be seen whether employing self-labeling techniques with the DPO policy can also leverage unlabeled prompts effectively. Another point of interest is understanding how reward over-optimization appears within direct preference optimization, and whether the slight performance drop shown in Figure~\ref{fig:dialogue-main}-right can be attributed to this effect. Moreover, while our experiments are limited to models containing up to 6B parameters, scaling DPO to much larger, state-of-the-art models presents a promising research direction. In terms of evaluation, we observe that GPT-4-based win rates are sensitive to the prompts used; future research could investigate optimal strategies for obtaining reliable judgments from automated evaluators. Lastly, DPO has a wide range of potential applications beyond training language models based on human preferences, such as optimizing generative models in other domains.

\end{document}