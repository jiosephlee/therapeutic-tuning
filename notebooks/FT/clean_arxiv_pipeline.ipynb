{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "# Read main.tex file from data/arxiv directory\n",
    "with open('../data/arxiv/DPO.tex', 'r') as f:\n",
    "    main_tex = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_latex(latex_content: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans a LaTeX string by extracting title, abstract, and main body,\n",
    "    and removing commands and environments that don't contribute to the main text.\n",
    "    \"\"\"\n",
    "    # 1. Extract title\n",
    "    title_match = re.search(r'\\\\title\\{(.*?)\\}', latex_content, re.DOTALL)\n",
    "    title = \"\"\n",
    "    if title_match:\n",
    "        title = title_match.group(1)\n",
    "        title = re.sub(r'\\\\\\\\\\s*', ' ', title)\n",
    "        title = title.strip()\n",
    "\n",
    "    # 2. Extract abstract\n",
    "    abstract_match = re.search(r'\\\\begin\\{abstract\\}(.*?)\\\\end\\{abstract\\}', latex_content, re.DOTALL)\n",
    "    abstract = abstract_match.group(1).strip() if abstract_match else \"\"\n",
    "\n",
    "    # 3. Find start of main body (first section after document environment)\n",
    "    body_start_index = -1\n",
    "    doc_start_match = re.search(r'\\\\begin\\{document\\}', latex_content)\n",
    "    if doc_start_match:\n",
    "        # Find the first section after \\begin{document}\n",
    "        section_match = re.search(r'\\\\section', latex_content[doc_start_match.end():])\n",
    "        if section_match:\n",
    "            body_start_index = doc_start_match.end() + section_match.start()\n",
    "\n",
    "    if body_start_index == -1:\n",
    "        # Fallback if structure is unexpected, return what we have so far\n",
    "        body = \"\"\n",
    "    else:\n",
    "        body = latex_content[body_start_index:]\n",
    "    \n",
    "    # 4. Find end of main body (before references, appendix, etc.)\n",
    "    end_markers = [\n",
    "        r'\\\\begin\\{thebibliography\\}', r'\\\\bibliography', r'\\\\appendix',\n",
    "        r'\\\\section\\*?\\{Acknowledgements\\}', r'\\\\section\\*?\\{Author Contributions\\}'\n",
    "    ]\n",
    "    end_index = len(body)\n",
    "    for marker_regex in end_markers:\n",
    "        end_match = re.search(marker_regex, body)\n",
    "        if end_match:\n",
    "            end_index = min(end_index, end_match.start())\n",
    "    \n",
    "    body = body[:end_index]\n",
    "\n",
    "    # Combine the parts we want to keep\n",
    "    full_text = f'\\\\title{{{title}}}\\n\\n\\\\begin{{abstract}}\\n{abstract}\\n\\\\end{{abstract}}\\n\\n{body}\\n\\\\end{{document}}'\n",
    "\n",
    "    # Remove excessive new lines \n",
    "    full_text = re.sub(r'\\n{3,}', '\\n\\n', full_text)\n",
    "    \n",
    "    # Now apply cleaning operations from the original function\n",
    "    cleaned_text = full_text\n",
    "    \n",
    "    # # Environments to remove completely with their content\n",
    "    # envs_to_remove = [\n",
    "    #     'figure', 'figure\\*', 'table', 'table\\*', 'tabular', 'tabular\\*', 'algorithm2e',\n",
    "    #     'equation', 'equation\\*', 'align', 'align\\*', 'multline', 'multline\\*',\n",
    "    #     'wrapfigure', 'wraptable'\n",
    "    # ]\n",
    "    # for env in envs_to_remove:\n",
    "    #     cleaned_text = re.sub(r'\\\\begin{' + env + r'}.*?\\\\end{' + env + r'}', '', cleaned_text, flags=re.DOTALL)\n",
    "\n",
    "    # # Replace commands that have content we want to keep\n",
    "    # cmds_keep_content = [\n",
    "    #     'section', 'subsection', 'subsubsection', 'paragraph', 'subparagraph',\n",
    "    #     'textbf', 'textit', 'emph', 'texttt', 'caption'\n",
    "    # ]\n",
    "    # for cmd in cmds_keep_content:\n",
    "    #     cleaned_text = re.sub(r'\\\\' + cmd + r'\\{([^}]+)\\}', r'\\1', cleaned_text)\n",
    "\n",
    "    # # Handle \\rev{old text}{new text} -> new text\n",
    "    # cleaned_text = re.sub(r'\\\\rev\\{[^}]*\\}\\{([^}]+)\\}', r'\\1', cleaned_text)\n",
    "\n",
    "    # # Remove commands with arguments that we want to discard\n",
    "    # cmds_remove_arg = ['label', 'ref', 'cite', 'citep', 'input', 'url']\n",
    "    # for cmd in cmds_remove_arg:\n",
    "    #     cleaned_text = re.sub(r'\\\\' + cmd + r'\\{[^}]*\\}', '', cleaned_text)\n",
    "\n",
    "    # # Remove commands that don't have arguments\n",
    "    # cmds_to_remove = [\n",
    "    #     'maketitle', 'clearpage', 'AND', 'And', 'footnotemark', 'thanks', 'appendix', 'newpage'\n",
    "    # ]\n",
    "    # for cmd in cmds_to_remove:\n",
    "    #     cleaned_text = re.sub(r'\\\\' + cmd + r'(?!\\w)', '', cleaned_text)\n",
    "    \n",
    "    # # Remove custom commands from this specific paper\n",
    "    # custom_cmds_to_remove = ['piref', 'pisft', 'methodac', 'methodfull', 'se']\n",
    "    # for cmd in custom_cmds_to_remove:\n",
    "    #     if '{' in cmd:\n",
    "    #          cleaned_text = re.sub(r'\\\\' + cmd.split('{')[0] + r'\\{[^}]*\\}', '', cleaned_text)\n",
    "    #     else:\n",
    "    #         cleaned_text = re.sub(r'\\\\' + cmd + r'(?!\\w)', '', cleaned_text)\n",
    "\n",
    "\n",
    "    # # Remove environment tags but keep content\n",
    "    # envs_keep_content = ['sproof'] # abstract and document are handled\n",
    "    # for env in envs_keep_content:\n",
    "    #     cleaned_text = re.sub(r'\\\\begin{' + env + r'\\}', '', cleaned_text)\n",
    "    #     cleaned_text = re.sub(r'\\\\end{' + env + r'\\}', '', cleaned_text)\n",
    "\n",
    "    # # Handle lists\n",
    "    # cleaned_text = re.sub(r'\\\\begin{itemize}', '', cleaned_text)\n",
    "    # cleaned_text = re.sub(r'\\\\end{itemize}', '', cleaned_text)\n",
    "    # cleaned_text = re.sub(r'\\\\begin{enumerate}', '', cleaned_text)\n",
    "    # cleaned_text = re.sub(r'\\\\end{enumerate}', '', cleaned_text)\n",
    "    # cleaned_text = re.sub(r'\\\\item', '\\n- ', cleaned_text)\n",
    "\n",
    "    # # Remove comments\n",
    "    # cleaned_text = re.sub(r'%.*', '', cleaned_text)\n",
    "    \n",
    "    # # Remove inline math expressions\n",
    "    # cleaned_text = re.sub(r'\\$.*?\\$', '', cleaned_text)\n",
    "    \n",
    "    # # Clean up whitespace\n",
    "    # cleaned_text = re.sub(r'~', ' ', cleaned_text)\n",
    "    # cleaned_text = re.sub(r'\\\\ ', ' ', cleaned_text) # Explicit space command\n",
    "    # cleaned_text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', cleaned_text)\n",
    "    # cleaned_text = re.sub(r'\\n\\s*\\n', '\\n\\n', cleaned_text)  # Collapse multiple newlines\n",
    "    # cleaned_text = re.sub(r'[ \\t]+', ' ', cleaned_text)  # Collapse multiple spaces\n",
    "    # cleaned_text = cleaned_text.strip()\n",
    "\n",
    "    return cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\title{Direct Preference Optimization: Your Language Model is Secretly a Reward Model}\n",
      "\n",
      "\\begin{abstract}\n",
      "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training.\n",
      "Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF).\n",
      "However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model.\n",
      "\\rev{In this paper, we leverage a mapping between reward functions and optimal policies to show that this constrained reward maximization problem can be \\emph{optimized exactly} with a single stage of policy training, essentially solving a classification problem on the human preference data.}{In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss.}\n",
      "The resulting algorithm, which we call \\textit{Direct Preference Optimization} (DPO), is stable, performant, and computationally lightweight, eliminating the need for \\rev{fitting a reward model,}{} sampling from the LM during fine-tuning or performing significant hyperparameter tuning.\n",
      "Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.\n",
      "\\end{abstract}\n",
      "\n",
      "\\section{Introduction}\n",
      "Large unsupervised language models (LMs) trained on very large datasets\n",
      "acquire surprising capabilities~\\citep{chowdhery2022palm, brown2020language, touvron2023llama,bubeck2023sparks}. However, these models are trained on data generated by humans with a wide variety of goals, priorities, and skillsets. Some of these goals and skillsets may not be desirable to imitate; for example, while we may want our AI coding assistant to \\textit{understand} common programming mistakes in order to correct them, nevertheless, when generating code, we would like to bias our model toward the (potentially rare) high-quality coding ability present in its training data. Similarly, we might want our language model to be \\textit{aware} of a common misconception believed by 50\\% of people, but we certainly do not want the model to claim this misconception to be true in 50\\% of queries about it! In other words, selecting the model's \\emph{desired responses and behavior} from its very wide \\textit{knowledge and abilities} is crucial to building AI systems that are safe, performant, and controllable \\citep{ouyang2022training}. While existing methods typically steer LMs to match human preferences using reinforcement learning (RL), we will show that the RL-based objective used by existing methods can be optimized exactly with a simple binary cross-entropy objective, greatly simplifying the preference learning pipeline.\n",
      "\n",
      "\\begin{figure}\n",
      "    \\centering\n",
      "    \\includegraphics[width=0.999\\textwidth]{figures/diagrams/teaser.png}\n",
      "    \\caption{\\textbf{{\\methodac} optimizes for human preferences while avoiding reinforcement learning.} Existing methods for fine-tuning language models with human feedback first fit a reward model to a dataset of prompts and human preferences over pairs of responses, and then use RL to find a policy that maximizes the learned reward. In contrast, {\\methodac} directly optimizes for the policy best satisfying the preferences with a simple classification objective, \\rev{without an explicit standalone reward model or RL}{fitting an \\textit{implicit} reward model whose corresponding optimal policy can be extracted in closed form}.}\n",
      "    \\vspace{-2mm}\n",
      "    \\label{fig:teaser}\n",
      "\\end{figure}\n",
      "\n",
      "At a high level, existing methods instill the desired behaviors into a language model using curated sets of human preferences representing the types of behaviors that humans find safe and helpful. This preference learning stage occurs after an initial stage of large-scale unsupervised pre-training on a large text dataset. While the most straightforward approach to preference learning is supervised fine-tuning on human demonstrations of high quality responses, the most successful class of methods is reinforcement learning from human (or AI) feedback (RLHF/RLAIF; \\citep{christiano2017deep,bai2022constitutional}). RLHF methods fit a reward model to a dataset of human preferences and then use RL to optimize a language model policy to produce responses assigned high reward without drifting excessively far from the original model. While RLHF produces models with impressive conversational and coding abilities, the RLHF pipeline is considerably more complex than supervised learning, involving training multiple LMs and sampling from the LM policy in the loop of training, incurring significant computational costs.\n",
      "\n",
      "In this paper, we show how to directly optimize a language model to adhere to human preferences, without explicit reward modeling or reinforcement learning.\n",
      "We propose\n",
      "\\textit{{\\methodfull} (\\methodac)}, an algorithm that implicitly optimizes the same objective as existing RLHF algorithms (reward maximization with a KL-divergence constraint) but is simple to implement and straightforward to train. Intuitively, the {\\methodac} update increases the relative log probability of preferred to dispreferred responses, but it incorporates a dynamic, per-example importance weight that prevents the model degeneration that we find occurs with a naive probability ratio objective. Like existing algorithms, {\\methodac} relies on a theoretical preference model (such as the Bradley-Terry model; \\cite{bradley1952rankanalysis}) that measures how well a given reward function aligns with empirical preference data. However, while existing methods use the preference model to define a preference loss to train a reward model and then train a policy that optimizes the learned reward model, {\\methodac} uses a change of variables to define the preference loss as a function of the policy directly. Given a dataset of human preferences over model responses, {\\methodac} can therefore optimize a policy using a simple binary cross entropy objective, \\rev{without learning an explicit, standalone reward model or sampling from the policy during training}{producing the optimal policy to an implicit reward function fit to the preference data}.\n",
      "\n",
      "Our main contribution is {\\methodfull} (\\methodac), a simple RL-free algorithm for training language models from preferences. Our experiments show that {\\methodac} is at least as effective as existing methods, including PPO-based RLHF, for learning from preferences in tasks such as sentiment modulation, summarization, and dialogue, using language models with up to 6B parameters.\n",
      "\n",
      "\\section{Related Work}\n",
      "\n",
      "Self-supervised language models of increasing scale learn to complete some tasks zero-shot \\citep{radford2019language} or with few-shot prompts \\citep{gpt3,megatron,chowdhery2022palm}. However, their performance on downstream tasks and alignment with user intent can be significantly improved by fine-tuning on datasets of instructions and human-written completions \\citep{mishra-etal-2022-cross,sanh2022multitask,chung2022scaling,thoppilan2022lamda}. This `instruction-tuning' procedure\n",
      "enables LLMs to generalize to instructions outside of the instruction-tuning set and generally increase their usability \\citep{chung2022scaling}. Despite the success of instruction tuning, \\textit{relative} human judgments of response quality are often easier to collect than expert demonstrations, and thus subsequent works have fine-tuned LLMs with datasets of human preferences, improving proficiency in translation \\citep{kreutzer-etal-2018-reliability}, summarization \\citep{stiennon2022learning,ziegler2020finetuning}, story-telling \\citep{ziegler2020finetuning}, and instruction-following \\citep{ouyang2022training,ramamurthy2023is}. These methods first optimize a neural network reward function for compatibility with the dataset of preferences under a preference model such as the Bradley-Terry model \\citep{bradley1952rankanalysis}, then fine-tune a language model to maximize the given reward using reinforcement learning algorithms, commonly REINFORCE \\citep{williams1992reinforce}, proximal policy optimization (PPO; \\cite{schulman2017proximal}), or variants \\citep{ramamurthy2023is}. A closely-related line of work leverages LLMs fine-tuned for instruction following with human feedback to generate additional synthetic preference data for targeted attributes such as safety or harmlessness \\citep{bai2022constitutional}, using only weak supervision from humans in the form of a text rubric for the LLM's annotations. These methods represent a convergence of two bodies of work: one body of work on training language models with reinforcement learning for a variety of objectives~\\citep{Ranzato2015SequenceLT,paulus2018a,wu2018learning} and another body of work on general methods for learning from human preferences \\citep{christiano2017deep,kupcsik2018learning}. \n",
      "Despite the appeal of using relative human preferences, fine-tuning large language models with reinforcement learning remains a major practical challenge; this work provides a theoretically-justified approach to optimizing relative preferences without RL.\n",
      "\n",
      "Outside of the context of language, learning policies from preferences has been studied in both bandit and reinforcement learning settings, and several approaches have been proposed. Contextual bandit learning using preferences or rankings of actions, rather than rewards, is known as a contextual dueling bandit (CDB; \\cite{yue2012karmed,dudik2015contextual}). In the absence of absolute rewards, theoretical analysis of CDBs substitutes the notion of an optimal policy with a \\textit{von Neumann winner}, a policy whose expected win rate against \\textit{any} other policy is at least 50\\% \\citep{dudik2015contextual}. However, in the CDB setting, preference labels are given online, while in learning from human preferences, we typically learn from a fixed batch of offline preference-annotated action pairs \\citep{yan2022human}. Similarly, \\textit{preference-based RL} (PbRL) learns from binary preferences generated by an \\textit{unknown} `scoring' function rather than rewards \\citep{BusaFekete2014,ruiz2023dueling}. Various algorithms for PbRL exist, including methods that can reuse off-policy preference data, but generally involve first explicitly estimating the latent scoring function (i.e. the reward model) and subsequently optimizing it \\citep{jain2013learning,BusaFekete2014,christiano2017deep,sadigh2017active,kupcsik2018learning}. We instead present a single stage policy learning approach that directly optimizes a policy to satisfy preferences.\n",
      "\n",
      "\\section{Preliminaries}\\label{section:prelims}\n",
      "\n",
      "We review the RLHF pipeline in \\citeauthor{ziegler2020finetuning} (and later \\citep{stiennon2022learning, bai2022training, ouyang2022training}). It usually includes three phases: 1) supervised fine-tuning (SFT); 2) preference sampling and reward learning and 3) RL optimization.\n",
      "\n",
      "\\textbf{SFT}: RLHF typically begins by fine-tuning a pre-trained LM with supervised learning on high-quality data for the downstream task(s) of interest (dialogue, summarization, etc.), to obtain a model $\\pisft$. \n",
      "\n",
      "\\textbf{Reward Modelling Phase}: In the second phase the SFT model is prompted with prompts $x$ to produce pairs of answers $(y_1, y_2)\\sim \\pisft(y \\mid x)$. These are then presented to human labelers who express preferences for one answer, denoted as $y_w\\succ y_l \\mid x$ where $y_w$ and $y_l$ denotes the preferred and dispreferred completion amongst $(y_1, y_2)$ respectively. The preferences are assumed to be generated by some latent reward model $r^*(y, x)$, which we do not have access to. There are a number of approaches used to model preferences, the Bradley-Terry (BT) \\cite{bradley1952rankanalysis} model being a popular choice (although more general Plackett-Luce ranking models \\citep{plackett1975analysis, luce2012individual} are also compatible with the framework if we have access to several ranked answers). The BT model stipulates that the human preference distribution $p^*$ can be written as:\n",
      "\\begin{equation}\\label{eq:bradley-terry}\n",
      "    p^*(y_1\\succ y_2 \\mid x)=\\frac{\\exp\\left(r^*(x, y_1)\\right)}{\\exp\\left(r^*(x, y_1)\\right) + \\exp\\left(r^*(x, y_2)\\right)}.\n",
      "\\end{equation}\n",
      "Assuming access to a static dataset of comparisons $\\mathcal{D}=\\bigl\\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\\bigr\\}_{i=1}^N$ sampled from $p^*$, we can parametrize a reward model $r_{\\phi}(x, y)$ and estimate the parameters via maximum likelihood. Framing the problem as a binary classification we have the negative log-likelihood loss:\n",
      "\\begin{equation}\\label{eq:reward_model}\n",
      "    \\mathcal{L}_R(r_{\\phi}, \\mathcal{D}) = -\\mathbb{E}_{(x, y_w, y_l)\\sim \\mathcal{D}}\\bigl[\\log \\sigma(r_{\\phi}(x, y_w)- r_{\\phi}(x, y_l))\\bigr]\n",
      "\\end{equation}\n",
      "where $\\sigma$ is the logistic function. In the context of LMs, the network $r_{\\phi}(x, y)$ is often initialized from the SFT model $\\pisft(y \\mid x)$ with the addition of a linear layer on top of the final transformer layer that produces a single scalar prediction for the reward value \\cite{ziegler2020finetuning}. To ensure a reward function with lower variance, prior works normalize the rewards, such that  $\\mathbb{E}_{x,y\\sim \\mathcal{D}}\\left[r_\\phi(x, y)\\right] = 0$ for all $x$.\n",
      "\n",
      "\\textbf{RL Fine-Tuning Phase}: During the RL phase, the learned reward function is used to provide feedback to the language model. Following prior works~\\citep{jaques2017sequence, jaques2020human}, the optimization is formulated as\n",
      "\\begin{equation}\\label{eq:RL}\n",
      "\\max_{\\pi_{\\theta}}  \\mathbb{E}_{x\\sim \\mathcal{D}, y\\sim \\pi_{\\theta}(y \\mid x)}\\bigl[r_{\\phi}(x, y)\\bigr] - \\beta\\mathbb{D}_{\\textrm{KL}}\\bigl[\\pi_{\\theta}(y\\mid x)\\mid \\mid \\piref(y\\mid x)\\bigr],\n",
      "\\end{equation}\n",
      "where $\\beta$ is a parameter controlling the deviation from the base reference policy $\\piref$, namely the initial SFT model $\\pisft$. \n",
      "In practice, the language model policy $\\pi_\\theta$ is also initialized to $\\pisft$. The added constraint is important, as it prevents the model from deviating too far from the distribution on which the reward model is accurate, as well as maintaining the generation diversity and preventing mode-collapse to single high-reward answers. Due to the discrete nature of language generation, this objective is not differentiable and is typically optimized with reinforcement learning. The standard approach \\citep{ziegler2020finetuning, stiennon2022learning, bai2022training, ouyang2022training} has been to construct the reward function ${r(x, y) = r_{\\phi}(x, y) -\\beta (\\log \\pi_{\\theta}(y\\mid x) - \\log \\piref(y\\mid x))}$, and maximize using PPO \\cite{schulman2017proximal}. \n",
      "\n",
      "\\section{Direct Preference Optimization}\\label{sec:DPO}\n",
      "\n",
      "Motivated by the challenges of applying reinforcement learning algorithms on large-scale problems such as fine-tuning language models, our goal is to derive a simple approach for policy optimization using preferences directly. Unlike prior RLHF methods, which learn a reward and then optimize it via RL, our approach \\rev{bypasses the reward modeling step and directly optimizes a language model using preference data}{leverages a particular choice of reward model parameterization that enables extraction of its optimal policy in closed form, without an RL training loop}. \n",
      "As we will describe next in detail, our key insight is to leverage an analytical mapping from reward functions to optimal policies, which enables us to transform a loss function over reward functions into a loss function over policies.\n",
      "This change-of-variables approach \\rev{allows us to skip the explicit reward modeling step}{avoids fitting an explicit, standalone reward model}, while still optimizing under existing models of human preferences, such as the Bradley-Terry model. In essence, the policy network represents both the language model and the \\rev{}{(implicit)} reward.\n",
      "\n",
      "\\textbf{Deriving the DPO objective.} We start with the same RL objective as prior work, Eq.~\\ref{eq:RL}, under a general reward function $r$. Following prior work~\\citep{peters2007reinforcement, peng2019advantage, korbak2022reinforcement, go2023aligning}, it is straightforward to show that the optimal solution to the KL-constrained reward maximization objective in Eq.~\\ref{eq:RL} takes the form:\n",
      "\\begin{equation}\\label{eq:op_policy}\n",
      "    \\pi_r(y\\mid x) = \\frac{1}{Z(x)}\\piref(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right),\n",
      "\\end{equation}%\n",
      "where $Z(x) =\\sum_{y}\\piref(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)$ is the partition function. See Appendix \\ref{app:derivation1} for a complete derivation. Even if we use the MLE estimate $r_{\\phi}$ of the ground-truth reward function $r^*$, it is still expensive to estimate the partition function $Z(x)$ \\citep{korbak2022reinforcement, go2023aligning}, which makes this representation hard to utilize in practice. However, we can rearrange Eq.~\\ref{eq:op_policy} to express the reward function in terms of its corresponding optimal policy $\\pi_r$, the reference policy $\\piref$, and the unknown partition function $Z(\\cdot)$. Specifically, we first take the logarithm of both sides of Eq.~\\ref{eq:op_policy} and then with some algebra we obtain:\n",
      "\\begin{equation}\\label{eq:main_eq}\n",
      "    r(x,y) =\\beta \\log \\frac{\\pi_r(y\\mid x)}{\\piref(y\\mid x)} + \\beta \\log Z(x).\n",
      "\\end{equation}\n",
      "We can apply this reparameterization to the ground-truth reward $r^*$ and corresponding optimal model $\\pi^*$. Fortunately, the Bradley-Terry model depends only on the difference of rewards between two completions, i.e., ${p^*(y_1 \\succ y_2 \\mid x) = \\sigma(r^*(x, y_1) - r^*(x, y_2))}$. Substituting the reparameterization in Eq.~\\ref{eq:main_eq} for $r^*(x,y)$ into the preference model Eq.~\\ref{eq:bradley-terry}, the partition function cancels, and we can express the human preference probability in terms of only the optimal policy $\\pi^*$ and reference policy $\\piref$. Thus, the optimal RLHF policy $\\pi^*$ under the Bradley-Terry model satisfies the preference model:\n",
      "\\begin{equation}\\label{eq:objective}\n",
      "    p^*(y_1\\succ y_2 \\mid x)=\\frac{1}{1 + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2\\mid x)}{\\piref(y_2\\mid x)} - \\beta \\log \\frac{\\pi^*(y_1\\mid x)}{\\piref(y_1\\mid x)}\\right)}\n",
      "\\end{equation}\n",
      "The derivation is in Appendix~\\ref{app:derivation2}. While Eq.~\\ref{eq:objective} uses the Bradley-Terry model, we can similarly derive expressions under the more general Plackett-Luce models~\\citep{plackett1975analysis, luce2012individual}, shown in Appendix~\\ref{app:plackett_luce_models}.\n",
      "\n",
      "Now that we have \n",
      "the probability of human preference data in terms of the optimal policy rather than the reward model, we can formulate a maximum likelihood objective for a parametrized policy $\\pi_\\theta$. Analogous to the reward modeling approach (i.e. Eq.~\\ref{eq:reward_model}), our policy objective becomes:\n",
      "\\begin{equation}\\label{eq:optimum_model}\n",
      "    \\mathcal{L}_\\text{DPO}(\\pi_{\\theta}; \\piref) = -\\mathbb{E}_{(x, y_w, y_l)\\sim \\mathcal{D}}\\left[\\log \\sigma \\left(\\beta \\log \\frac{\\pi_{\\theta}(y_w\\mid x)}{\\piref(y_w\\mid x)} - \\beta \\log \\frac{\\pi_{\\theta}(y_l\\mid x)}{\\piref(y_l\\mid x)}\\right)\\right].\n",
      "\\end{equation}\n",
      "\\rev{This way, we simultaneously bypass the explicit reward modeling step while also avoiding the need to perform reinforcement learning optimization.}{This way, we fit an implicit reward using an alternative parameterization, whose optimal policy is simply $\\pi_\\theta$.} Moreover, since our procedure is equivalent to fitting a reparametrized Bradley-Terry model, it enjoys certain theoretical properties, such as consistencies under suitable assumption of the preference data distribution \\cite{bong2022generalized}. In Section~\\ref{sec:theory}, we further discuss theoretical properties of DPO in relation to other works.\n",
      "\n",
      "\\textbf{What does the DPO update do?} For a mechanistic understanding of DPO, it is useful to analyze the gradient of the loss function $\\mathcal{L}_\\text{DPO}$. The gradient with respect to the parameters $\\theta$ can be written as:\n",
      "\\begin{multline*}\\label{eq:gradient}\n",
      "    \\nabla_\\theta \\mathcal{L}_\\text{DPO}(\\pi_\\theta;\\piref) = \\\\ -\\beta\\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}} \\bigg[\\underbrace{\\sigma(\\hat{r}_\\theta(x, y_l) - \\hat{r}_\\theta (x, y_w))}_\\text{higher weight when reward estimate is wrong}\\bigg[\\underbrace{\\nabla_\\theta\\log \\pi(y_w \\mid x)}_\\text{increase likelihood of $y_w$} - \\underbrace{\\nabla_\\theta\\log\\pi(y_l \\mid x)}_\\text{decrease likelihood of $y_l$}\\bigg]\\bigg],\n",
      "\\end{multline*}\n",
      "where $\\hat{r}_\\theta(x, y) = \\beta \\log \\frac{\\pi_\\theta(y \\mid x)}{\\piref(y \\mid x)}$ is the reward implicitly defined by the language model $\\pi_\\theta$ and reference model $\\piref$ (more in Section~\\ref{sec:theory}). Intuitively, the gradient of the loss function $\\mathcal{L}_\\text{DPO}$ increases the likelihood of the preferred completions $y_w$ and decreases the likelihood of dispreferred completions $y_l$. Importantly, the examples are weighed by how much higher the implicit reward model $\\hat{r}_\\theta$ rates the dispreferred completions, scaled by $\\beta$, i.e, how incorrectly the implicit reward model orders the completions, accounting for the strength of the KL constraint. Our experiments suggest the importance of this weighting, as a na\\\"ive version of this method without the weighting coefficient can cause the language model to degenerate (Appendix Table~\\ref{tab:unlikelihood_generations}).\n",
      "\n",
      "\\textbf{DPO outline.} \n",
      "The general DPO pipeline is as follows: 1) Sample completions $y_1, y_2 \\sim \\piref(\\cdot \\mid x)$ for every prompt $x$, label with human preferences to construct the offline dataset of preferences $\\mathcal{D} = \\{x^{(i)}, y_w^{(i)}, y_l)^{(i)}\\}_{i=1}^N$ and 2) optimize the language model $\\pi_\\theta$ to minimize $\\mathcal{L}_\\text{DPO}$ for the given $\\piref$ and $\\mathcal{D}$ and desired $\\beta$. \n",
      "In practice, one would like to reuse preference datasets publicly available, rather than generating samples and gathering human preferences. Since the preference datasets are sampled using $\\pisft$, we initialize $\\piref = \\pisft$ whenever available. However, when $\\pisft$ is not available, we initialize $\\piref$ by maximizing likelihood of preferred completions ${(x, y_w)}$, that is, ${\\piref = \\argmax_{\\pi}\\mathbb{E}_{x, y_w \\sim \\mathcal{D}}\\left[\\log \\pi(y_w \\mid x)\\right]}$. This procedure helps mitigate the distribution shift between the true reference distribution which is unavailable, and $\\piref$ used by DPO. Further details related to the implementation and hyperparameters can be found in Appendix~\\ref{app:implementation}.\n",
      "\n",
      "\\section{Theoretical Analysis of DPO}\n",
      "In this section, we give further interpretation of the DPO method, provide theoretical backing, and relate advantages of DPO to issues with actor critic algorithms used for RLHF (such as PPO~\\cite{schulman2017proximal}).\n",
      "\n",
      "\\label{sec:theory}\n",
      "\n",
      "\\subsection{Your Language Model Is Secretly a Reward Model} DPO is able to bypass both \\rev{explicit reward estimation}{fitting an explicit reward} and performing RL to learn the policy using a single maximum likelihood objective. Note the optimization objective Eq. \\ref{eq:main_eq} is equivalent to a Bradley-Terry model with a reward parameterization $r^*(x, y) = \\beta \\log\\frac{\\pi^*_\\theta(y \\mid x)}{\\piref(y \\mid x)}$ and we optimize our parametric model $\\pi_{\\theta}$, equivalently to the reward model optimization in Eq. \\ref{eq:reward_model} under the change of variables. In this section we will build the theory behind this reparameterization, show that it does not constrain the class of learned reward models, and allows for the exact recovery of the optimal policy. We begin with by defining an equivalence relation between reward functions. \n",
      "\n",
      "\\begin{definition}\n",
      "We say that two reward functions $r(x, y)$ and $r'(x, y)$ are equivalent iff ${r(x, y)-r'(x, y) = f(x)}$ for some function $f$.     \n",
      "\\end{definition}\n",
      "It is easy to see that this is indeed an equivalence relation, which partitions the set of reward functions into classes. We can state the following two lemmas:\n",
      "\n",
      "\\begin{lemma}\\label{lemma:same_prefrence} Under the Plackett-Luce, and in particular the Bradley-Terry, preference framework, two reward functions from the same class induce the same preference distribution.\n",
      "\\end{lemma}\n",
      "\n",
      "\\begin{lemma}\\label{lemma:same_policy}\n",
      "    Two reward functions from the same equivalence class induce the same optimal policy under the constrained RL problem.\n",
      "\\end{lemma}\n",
      "The proofs are straightforward and we defer them to Appendix \\ref{app:lemma1}. The first lemma is a well-known under-specification issue with the Plackett-Luce family of models \\cite{plackett1975analysis}. Due to this under-specification, we usually have to impose additional identifiability constraints to achieve any guarantees on the MLE estimates from Eq. \\ref{eq:reward_model} \\cite{bong2022generalized}. The second lemma states that all reward functions from the same class yield the same optimal policy, hence for our final objective, we are only interested in recovering an arbitrary reward function from the optimal class. We prove the following Theorem in Appendix~\\ref{app:thm1}:\n",
      "\\begin{theorem}\\label{thm:main}\n",
      "    Under mild assumptions, all reward classes consistent with the Plackett-Luce (and Bradley-Terry in particular) models can be represented with the reparameterization ${r(x, y) = \\beta \\log \\frac{\\pi(y\\mid x)}{\\piref(y\\mid x)}}$ for some model $\\pi(y\\mid x)$ and a given reference model $\\piref(y \\mid x)$.\n",
      "\\end{theorem}\n",
      "\\begin{sproof}\n",
      "    Consider any reward function $r(x, y)$, which induces a corresponding optimal model $\\pi_r(y \\mid x)$, specified by Eq. \\ref{eq:op_policy}. We will show that a reward function from the equivalence class of $r$ can be represented using the reparameterization given above. We define the projection $f$ as  \n",
      "\\begin{equation}\n",
      "    f(r; \\piref, \\beta)(x, y) = r(x, y) - \\beta\\log\\sum_{y}\\piref(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)\n",
      "\\end{equation}\n",
      "The operator $f$ simply normalizes the reward function with the logarithm of the partition function of $\\pi_r$. Since the added normalization term is only a function of the prefix $x$, $f(r; \\piref, \\beta)(x, y) $ is a reward function in the equivalence class of $r(x, y)$. Finally, replacing $r$ with the RHS of Eq.~\\ref{eq:main_eq} (which holds for any reward function), we have $f(r; \\piref, \\beta)(x, y) = \\beta \\log \\frac{\\pi_r(y\\mid x)}{\\piref(y\\mid x)}$. That is, the projection $f$ produces a member of the equivalence class of $r$ with the desired form, and we do not lose any generality in our reward model from the proposed reparameterization.\n",
      "\\end{sproof}\n",
      "We can alternatively view Theorem~\\ref{thm:main} as specifying exactly which reward function within each equivalence class the DPO reparameterization selects, that is, the reward function satisfying:\n",
      "\\begin{equation}\\label{eq:lag_p}\n",
      "     \\sum_{y}\\underbrace{\\piref(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)}_{=\\pi(y\\mid x)\\text{, using Thm.~\\ref{thm:main} reparam.}} = 1,\n",
      "\\end{equation}\n",
      "i.e., $\\pi(y\\mid x)$ is a valid distribution (probabilities are positive and sum to 1).\n",
      "However, following Eq.~\\ref{eq:op_policy}, we can see that Eq.~\\ref{eq:lag_p} is the partition function of the optimal policy induced by the reward function $r(x, y)$.\n",
      "The key insight of the DPO algorithm is that we can impose certain constraints on the under-constrained Plackett-Luce (and Bradley-Terry in particular) family of preference models, such that we preserve the class of representable reward models, but explicitly make the optimal policy in Eq. \\ref{eq:op_policy} analytically tractable for all prompts $x$.\n",
      "\n",
      "\\subsection{Instability of Actor-Critic Algorithms}\n",
      "We can also use our framework to diagnose instabilities with standard actor-critic algorithms used for the RLHF, such as PPO. We follow the RLHF pipeline and focus on the RL fine-tuning step outlined in Section \\ref{section:prelims}. We can draw connections to the control as inference framework \\cite{levine2018reinforcement} for the constrained RL problem outlined in \\ref{eq:RL}. We assume a parameterized model $\\pi_{\\theta}(y\\mid x)$ and minimize $\\mathbb{D}_{\\text{KL}}[\\pi_{\\theta}(y|x) \\mid \\mid \\pi^*(y\\mid x)]$ where $\\pi^*$ is the optimal policy from Eq. \\ref{eq:optimum_model} induced by the reward function $r_{\\phi}(y, x)$. With some algebra this leads to the optimization objective:\n",
      "\\begin{equation}\\label{eq:AC}\n",
      "    \\max_{\\pi_{\\theta}}\\mathbb{E}_{\\pi_{\\theta}(y\\mid x)}\\bigg[\\underbrace{r_{\\phi}(x, y) -\\beta\\log\\sum_{y}\\piref(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r_{\\phi}(x, y)\\right)}_{f(r_{\\phi}, \\piref, \\beta)} - \\underbrace{\\beta\\log\\frac{\\pi_{\\theta}(y\\mid x)}{\\piref(y\\mid x)}}_{\\text{KL}}\\bigg]\n",
      "\\end{equation}\n",
      "This is the same objective optimized in prior works \n",
      "\\citep{ziegler2020finetuning, stiennon2022learning, bai2022training, ouyang2022training} using the DPO-equivalent reward for the reward class of $r_{\\phi}$. In this setting, we can interpret the normalization term in $f(r_{\\phi}, \\piref, \\beta)$ as the soft value function of the reference policy $\\piref$. While this term does not affect the optimal solution, without it, the policy gradient of the objective could have high variance, making learning unstable. We can accommodate for the normalization term using a learned value function, but that can also be difficult to optimize. Alternatively, prior works have normalized rewards using a human completion baseline, essentially a single sample Monte-Carlo estimate of the normalizing term. In contrast the DPO reparameterization yields a reward function that does not require any baselines. \n",
      "\n",
      "\\section{Experiments}\n",
      "In this section, we empirically evaluate DPO's ability to train policies directly from preferences. First, in a well-controlled text-generation setting, we ask: how efficiently does DPO trade off maximizing reward and minimizing KL-divergence with the reference policy, compared to common preference learning algorithms such as PPO? Next, we evaluate DPO's performance on larger models and more difficult RLHF tasks, including summarization and dialogue. We find that with almost no tuning of hyperparameters, DPO tends to perform as well or better than strong baselines like RLHF with PPO as well as returning the best of $N$ sampled trajectories under a learned reward function. Before presenting these results, we describe the experimental set-up; additional details are in Appendix~\\ref{app:exp_details}.\n",
      "\n",
      "\\textbf{Tasks.} Our experiments explore three different open-ended text generation tasks. For all experiments, algorithms learn a policy from a dataset of preferences $\\mathcal{D}=\\bigl\\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\\bigr\\}_{i=1}^N$. In \\textbf{controlled sentiment generation}, $x$ is a prefix of a movie review from the IMDb dataset \\cite{maas-EtAl:2011:ACL-HLT2011}, and the policy must generate $y$ with positive sentiment. In order to perform a controlled evaluation, for this experiment we \\textit{generate} preference pairs over generations using a pre-trained sentiment classifier, where $p(\\text{positive}\\mid x,y_w)>p(\\text{positive}\\mid x,y_l)$. For SFT, we fine-tune GPT-2-large until convergence on reviews from the train split of the IMDB dataset (further details in App~\\ref{app:sentiment_details}). In \\textbf{summarization}, $x$ is a forum post from Reddit; the policy must generate a summary $y$ of the main points in the post. Following prior work, we use the Reddit TL;DR summarization dataset \\citep{volske-etal-2017-tl} along with human preferences gathered by \\citeauthor{stiennon2022learning}. We use an SFT model fine-tuned on human-written forum post summaries\\footnote{\\url{https://huggingface.co/CarperAI/openai_summarize_tldr_sft}} with the TRLX \\citep{leandro_von_werra_2023_7790115} framework for RLHF. The human preference dataset was gathered by \\citeauthor{stiennon2022learning} on samples from a different, but similarly-trained, SFT model. Finally, in \\textbf{single-turn dialogue}, \n",
      "$x$ is a human query, which may be anything from a question about astrophysics to a request for relationship advice. A policy must produce an engaging and helpful response $y$ to a user's query; we use the Anthropic Helpful and Harmless dialogue dataset \\citep{bai2022training}, containing 170k dialogues between a human and an automated assistant. Each transcript ends with a pair of responses generated by a large (although unknown) language model along with a preference label denoting the human-preferred response. In this setting, no pre-trained SFT model is available; we therefore fine-tune an off-the-shelf language model on only the preferred completions to form the SFT model.\n",
      "\n",
      "\\begin{figure}\n",
      "    \\centering\n",
      "    \\includegraphics[width=0.50\\textwidth]{figures/results/frontier.pdf}\n",
      "    \\includegraphics[width=0.49\\textwidth]{figures/results/tldr_winrate_vs_temp.pdf}\n",
      "    \\caption{\\textbf{Left.} The frontier of expected reward vs KL to the reference policy. DPO provides the highest expected reward for all KL values, demonstrating the quality of the optimization. \\textbf{Right.} TL;DR summarization win rates vs. human-written summaries, using GPT-4 as evaluator. DPO exceeds PPO's best-case performance on summarization, while being more robust to changes in the sampling temperature.}\n",
      "    \\vspace{-2mm}\n",
      "    \\label{fig:frontier-tldr-main}\n",
      "\\end{figure}\n",
      "\n",
      "\\textbf{Evaluation.} Our experiments use two different approaches to evaluation. In order to analyze the effectiveness of each algorithm in optimizing the constrained reward maximization objective, in the controlled sentiment generation setting we evaluate each algorithm by its frontier of achieved reward and KL-divergence from the reference policy; this frontier is computable because we have acccess to the ground-truth reward function (a sentiment classifier). However, in the real world, the ground truth reward function is not known; therefore, we evaluate algorithms with their \\textit{win rate} against a baseline policy, using GPT-4 as a proxy for human evaluation of summary quality and response helpfulness in the summarization and single-turn dialogue settings, respectively. For summarization, we use reference summaries in the test set as the baseline; for dialogue, we use the preferred response in the test dataset as the baseline. While existing studies suggest LMs can be better automated evaluators than existing metrics \\citep{Chen2023ExploringTU}, we conduct a human study to justify our usage of GPT-4 for evaluation in Sec.~\\ref{sec:human-judgments}. We find GPT-4 judgments correlate strongly with humans, with human agreement with GPT-4 typically similar or higher than inter-human annotator agreement. \n",
      "\n",
      "\\textbf{Methods.} In addition to DPO, we evaluate several existing approaches to training language models to adhere to human preferences. Most simply, we explore zero-shot prompting with \\textbf{GPT-J} \\citep{gpt-j} in the summarization task and 2-shot prompting with \\textbf{Pythia-2.8B} \\citep{biderman2023pythia} in the dialogue task. In addition, we evaluate the \\textbf{SFT} model as well as \\textbf{Preferred-FT}, which is a model fine-tuned with supervised learning on the chosen completion $y_w$ from either the SFT model (in controlled sentiment and summarization) or a generic LM (in single-turn dialogue). Another pseudo-supervised method is \\textbf{Unlikelihood}~\\citep{welleck2019neural}, which simply optimizes the policy to maximize the probability assigned to $y_w$ and \\textit{minimize} the probability assigned to $y_l$; we use an optional coefficient $\\alpha\\in[0,1]$ on the `unlikelihood' term. We also consider \\textbf{PPO} \\citep{schulman2017proximal} using a reward function learned from the preference data and \\textbf{PPO-GT}, which is an oracle that learns from the ground truth reward function available in the controlled sentiment setting. In our sentiment experiments, we use two implementations of PPO-GT, one of-the-shelf version \\cite{leandro_von_werra_2023_7790115} as well as a modified version that normalizes rewards and further tunes hyperparameters to improve performance (we also use these modifications when running `normal' PPO with learned rewards). Finally, we consider the \\textbf{Best of $N$} baseline, sampling $N$ responses from the SFT model (or Preferred-FT in dialogue) and returning the highest-scoring response according to a reward function learned from the preference dataset. This high-performing method decouples the quality of the reward model from the PPO optimization, but is computationally impractical even for moderate $N$ as it requires sampling $N$ completions for every query at test time.\n",
      "\n",
      "\\subsection{How well can DPO optimize the RLHF objective?}\n",
      "\n",
      "\\begin{figure}\n",
      "    \\centering\n",
      "    \\includegraphics[width=0.50\\textwidth]{figures/results/dialogue_winrate_vs_temp.pdf}\n",
      "    \\includegraphics[width=0.49\\textwidth]{figures/results/dialogue_winrate_vs_steps.pdf}\n",
      "    \\caption{\\textbf{Left.} Win rates computed by GPT-4 for Anthropic-HH one-step dialogue; DPO is the only method that improves over chosen summaries in the Anthropic-HH test set. \\textbf{Right.} Win rates for different sampling temperatures over the course of training. DPO's improvement over the dataset labels is fairly stable over the course of training for different sampling temperatures.}\n",
      "    \\vspace{-2mm}\n",
      "    \\label{fig:dialogue-main}\n",
      "\\end{figure}\n",
      "\n",
      "The KL-constrained reward maximization objective used in typical RLHF algorithms balances exploitation of reward while restricting the policy from deviating far from the reference policy. Therefore, when comparing algorithms, we must take into account both reward achieved as well as the KL discrepancy; achieving slightly higher reward but with much higher KL is not necessarily desirable. Figure~\\ref{fig:frontier-tldr-main} shows the reward-KL frontier for various algorithms in the sentiment setting. We execute multiple training runs for each algorithm, using a different hyperparameter for policy conservativeness in each run (target KL $\\in\\{3,6,9,12\\}$ for PPO, $\\beta \\in \\{0.05,0.1,1,5\\}$, $\\alpha\\in\\{0.05,0.1,0.5,1\\}$ for unlikelihood, random seeds for preferred-FT). This sweep includes 22 runs in total. After each 100 training steps until convergence, we evaluate each policy on a set of test prompts, computing the average reward under the true reward function as well as the average sequence-level KL\\footnote{That is, the sum of the per-timestep KL-divergences.} with the reference policy $\\text{KL}\\left(\\pi\\mid \\mid \\piref\\right)$. We find that DPO produces by far the most efficient frontier, achieving the highest reward while still achieving low KL. This result is particularly notable for multiple reasons. First, DPO and PPO optimize the same objective, but DPO is notably more efficient; DPO's reward/KL tradeoff strictly dominates PPO. Second, DPO achieves a better frontier than PPO, \\emph{even when PPO can access ground truth rewards} (PPO-GT).\n",
      "\n",
      "\\subsection{Can DPO scale to real preference datasets?}\n",
      "\\label{sec:dpo-real-datasets}\n",
      "Next, we evaluate fine-tuning performance of DPO on summarization and single-turn dialogue. For summarization, \n",
      "automatic evaluation metrics such as ROUGE can be poorly correlated with human preferences~\\citep{stiennon2022learning}, and prior work has found that fine-tuning LMs using PPO on human preferences to provide more effective summaries. We evaluate different methods by sampling completions on the test split of TL;DR summarization dataset, and computing the average win rate against reference completions in the test set. The completions for all methods are sampled at temperatures varying from 0.0 to 1.0, and the win rates are shown in Figure~\\ref{fig:frontier-tldr-main} (right). DPO, PPO and Preferred-FT all fine-tune the same GPT-J SFT model\\footnote{\\url{https://huggingface.co/CarperAI/openai_summarize_tldr_sft}}. We find that DPO has a win rate of approximately 61\\% at a temperature of 0.0, exceeding the performance of PPO at ~57\\% at its optimal sampling temperature of 0.0. DPO also achieves a higher maximum win rate compared to the best of $N$ baseline. We note that we did not meaningfully tune DPO's $\\beta$ hyperparameter, so these results may underestimate DPO's potential. Moreover, we find DPO to be much more robust to the sampling temperature than PPO, the performance of which can degrade to that of the base GPT-J model at high temperatures. Preferred-FT does not improve significantly over the SFT model. We also compare DPO and PPO head-to-head in human evaluations in Section~\\ref{sec:human-judgments}, where DPO samples at temperature 0.25 were preferred 58\\% times over PPO samples at temperature 0.\n",
      "\n",
      "On single-turn dialogue, we evaluate the different methods on the subset of the test split of the Anthropic HH dataset \\citep{bai2022training} with one step of human-assistant interaction. GPT-4 evaluations use the preferred completions on the test as the reference to compute the win rate for different methods. As there is no standard SFT model for this task, we start with a pre-trained Pythia-2.8B, use Preferred-FT to train a reference model on the chosen completions such that completions are within distribution of the model, and then train using DPO. We also compare against the best of 128 Preferred-FT completions (we found the Best of $N$ baseline plateaus at 128 completions for this task; see Appendix Figure~\\ref{fig:best-of-n}) and a 2-shot prompted version of the Pythia-2.8B base model, finding DPO performs as well or better for the best-performing temperatures for each method. We also evaluate an RLHF model trained with PPO on the Anthropic HH dataset \\footnote{\\url{https://huggingface.co/reciprocate/ppo_hh_pythia-6B}} from a well-known source \\footnote{\\url{https://github.com/CarperAI/trlx/tree/main/examples/hh}}, but are unable to find a prompt or sampling temperature that gives performance better than the base Pythia-2.8B model. Based on our results from TL;DR and the fact that both methods optimize the same reward function, we consider Best of 128 a rough proxy for PPO-level performance. Overall, DPO is the only computationally efficient method that improves over the preferred completions in the Anthropic HH dataset, and provides similar or better performance to the computationally demanding Best of 128 baseline. Finally, Figure~\\ref{fig:dialogue-main} shows that DPO converges to its best performance relatively quickly.\n",
      "\n",
      "\\subsection{Generalization to a new input distribution}\n",
      "\n",
      "\\begin{wraptable}{r}{0.375\\textwidth}\n",
      "    \\small\n",
      "    \\vspace{-10mm}\n",
      "    \\begin{tabular}{ccc}\n",
      "        \\toprule\n",
      "        & \\multicolumn{2}{c}{\\textbf{Win rate vs. ground truth}} \\\\\n",
      "        \\cmidrule(lr){2-3}\n",
      "        \\textbf{Alg.} & Temp $0$ & Temp $0.25$ \\\\\n",
      "        \\midrule\n",
      "        DPO & 0.36 & 0.31 \\\\\n",
      "        PPO & 0.26 & 0.23 \\\\\n",
      "        \\bottomrule\n",
      "    \\end{tabular}\n",
      "    \\caption{GPT-4 win rates vs. ground truth summaries for out-of-distribution CNN/DailyMail input articles.}\n",
      "    \\vspace{-3mm}\n",
      "    \\label{tab:ood}\n",
      "\\end{wraptable}\n",
      "\n",
      "To further compare the performance of PPO and DPO under distribution shifts, we evaluate the PPO and DPO policies from our Reddit TL;DR summarization experiment on a different distribution, news articles in the test split of the CNN/DailyMail dataset \\citep{nallapati-etal-2016-abstractive}, using the best sampling temperatures from TL;DR (0 and 0.25). The results are presented in Table~\\ref{tab:ood}. We computed the GPT-4 win rate against the ground-truth summaries in the datasets, using the same GPT-4 (C) prompt we used for Reddit TL;DR, but replacing the words ``forum post'' with ``news article''. For this new distribution, DPO continues to outperform the PPO policy by a significant margin. This experiment provides initial evidence that DPO policies can generalize similarly well to PPO policies, even though DPO does not use the additional unlabeled Reddit TL;DR prompts that PPO uses.\n",
      "\n",
      "\\subsection{Validating GPT-4 judgments with human judgments}\n",
      "\\label{sec:human-judgments}\n",
      "We conduct a human study to verify the reliability of GPT-4's judgments, using the results of the TL;DR summarization experiment and two different GPT-4 prompts. The \\textbf{GPT-4 (S)} (simple) prompt simply asks for which summary better-summarizes the important information in the post. The \\textbf{GPT-4 (C)} (concise) prompt also asks for which summary is more concise; we evaluate this prompt because we find that GPT-4 prefers longer, more repetitive summaries than humans do with the \\textbf{GPT-4 (S)} prompt. See Appendix~\\ref{app:prompts} for the complete prompts. We perform three comparisons, using the highest (DPO, temp. 0.25), the lowest (PPO, temp. 1.0), and a \\begin{wraptable}{r}{0.47\\textwidth}\n",
      "    \\centering\n",
      "    \\small\n",
      "    \\vspace{-1.5mm}\n",
      "    \\begin{tabular}{lccc}\n",
      "    \\toprule\n",
      "        & \\textbf{DPO} & \\textbf{SFT} & \\textbf{PPO-1} \\\\\n",
      "        \\cmidrule(lr){2-4}\n",
      "        N respondents & 272 & 122 & 199 \\\\\n",
      "        \\midrule\n",
      "        GPT-4 (S) win \\% & 47 & 27 & 13 \\\\\n",
      "        GPT-4 (C) win \\% & 54 & 32 & 12 \\\\\n",
      "        Human win \\% & 58 & 43 & 17 \\\\\n",
      "        \\midrule\n",
      "        GPT-4 (S)-H agree & 70 & 77 & 86 \\\\\n",
      "        GPT-4 (C)-H agree & 67 & 79 & 85 \\\\\n",
      "        H-H agree & 65 & - & 87 \\\\\n",
      "        \\bottomrule\n",
      "    \\end{tabular}\n",
      "    \\vspace{-1mm}\n",
      "    \\caption{Comparing human and GPT-4 win rates and per-judgment agreement on TL;DR summarization samples. \\textbf{Humans agree with GPT-4 about as much as they agree with each other.} Each experiment compares a summary from the stated method with a summary from PPO with temperature 0.}\n",
      "    \\vspace{-5mm}\n",
      "    \\label{tab:human_results}\n",
      "\\end{wraptable}middle-performing (SFT, temp. 0.25) method with the aim of covering a diversity of sample qualities; all three methods are compared against greedily-sampled PPO (its best-performing temperature). We find that with both prompts, GPT-4 tends to agree with humans about as often as humans agree with each other, suggesting that GPT-4 is a reasonable proxy for human evaluations (due to limited human raters, we only collect multiple human judgments for the DPO and PPO-1 comparisons). Overall, the \\textbf{GPT-4 (C)} prompt generally provides win rates more representative of humans; we therefore use this prompt for the main results in Section~\\ref{sec:dpo-real-datasets}. For additional details about the human study, including the web interface presented to raters and the list of human volunteers, see Appendix~\\ref{app:human-study}.\n",
      "\n",
      "\\section{Discussion}\n",
      "Learning from preferences is a powerful, scalable framework for training capable, aligned language models. We have introduced DPO, a simple training paradigm for training language models from preferences without reinforcement learning. Rather than coercing the preference learning problem into a standard RL setting in order to use off-the-shelf RL algorithms, DPO identifies a mapping between language model policies and reward functions that enables training a language model to satisfy human preferences \\textit{directly}, with a simple cross-entropy loss, without reinforcement learning or loss of generality. With virtually no tuning of hyperparameters, DPO performs similarly or better than existing RLHF algorithms, including those based on PPO; DPO thus meaningfully reduces the barrier to training more language models from human preferences.\n",
      "\n",
      "\\textbf{Limitations \\& Future Work.} Our results raise several important questions for future work. How does the DPO policy generalize out of distribution, compared with learning from an explicit reward function? Our initial results suggest that DPO policies can generalize similarly to PPO-based models, but more comprehensive study is needed. For example, can training with self-labeling from the DPO policy similarly make effective use of unlabeled prompts? On another front, how does reward over-optimization manifest in the direct preference optimization setting, and is the slight decrease in performance in Figure~\\ref{fig:dialogue-main}-right an instance of it? Additionally, while we evaluate models up to 6B parameters, exploration of scaling DPO to state-of-the-art models orders of magnitude larger is an exciting direction for future work. Regarding evaluations, we find that the win rates computed by GPT-4 are impacted by the prompt; future work may study the best way to elicit high-quality judgments from automated systems. Finally, many possible applications of DPO exist beyond training language models from human preferences, including training generative models in other modalities.\n",
      "\n",
      "\\end{document}\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = clean_latex(main_tex)\n",
    "print(cleaned_text)\n",
    "\n",
    "# Save the cleaned text\n",
    "with open('../data/arxiv/cleaned_DPO.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we clean it more?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Original Text ---\n",
      "\n",
      "# 1. Simple case (SHOULD BE FIXED)\n",
      "This is a sentence\n",
      "that should be joined.\n",
      "\n",
      "# 2. Word with backslash (SHOULD BE SKIPPED)\n",
      "The command \\section\n",
      "is followed by \\subsection.\n",
      "\n",
      "# 3. LaTeX table (SHOULD BE SKIPPED)\n",
      "\\midrule\n",
      "DPO & 0.36 & 0.31 \\\n",
      "PPO & 0.26 & 0.23 \\\n",
      "\\midrule\n",
      "\n",
      "# 4. Multi-line paragraph (SHOULD BE FULLY FIXED)\n",
      "This is another\n",
      "multi\n",
      "line paragraph.\n",
      "\n",
      "\n",
      "--- After Final Correct Fix ---\n",
      "\n",
      "# 1. Simple case (SHOULD BE FIXED)\n",
      "This is a sentence that should be joined.\n",
      "\n",
      "# 2. Word with backslash (SHOULD BE SKIPPED)\n",
      "The command \\section\n",
      "is followed by \\subsection.\n",
      "\n",
      "# 3. LaTeX table (SHOULD BE SKIPPED)\n",
      "\\midrule\n",
      "DPO & 0.36 & 0.31 \\\n",
      "PPO & 0.26 & 0.23 \\\n",
      "\\midrule\n",
      "\n",
      "# 4. Multi-line paragraph (SHOULD BE FULLY FIXED)\n",
      "This is another multi line paragraph.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def fix_newlines_final(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Fixes misplaced newlines with a strict set of rules:\n",
    "    - Finds any two non-whitespace chunks separated by clean whitespace.\n",
    "    - A fix is only applied IF AND ONLY IF both chunks are purely alphabetic.\n",
    "    \"\"\"\n",
    "    # A general pattern to find candidates: two non-whitespace chunks\n",
    "    # separated by whitespace that does NOT contain a backslash.\n",
    "    # [^\\\\s] is a negated set: any character that is NOT a backslash or whitespace.\n",
    "    # We use this to avoid matching the '\\\\' from a LaTeX line break.\n",
    "    pattern = r'(\\S+)([ \\t]*\\n[ \\t\\n]*)(\\S+)'\n",
    "    def conditional_replacer(match: re.Match) -> str:\n",
    "        \"\"\"\n",
    "        The core logic: only act on purely alphabetic words.\n",
    "        \"\"\"\n",
    "        word1 = match.group(1)\n",
    "        word2 = match.group(3)\n",
    "        # print(word1, word2)\n",
    "        # THE CRITICAL CHECK: Are both words composed *only* of letters?\n",
    "        if word1.isalpha() and word2.isalpha():\n",
    "            # Yes. Perform the replacement.\n",
    "            return f'{word1} {word2}'\n",
    "        else:\n",
    "            # No. One or both words contain non-alphabetic characters (\\, &, . etc).\n",
    "            # Return the original matched text to skip the replacement.\n",
    "            return match.group(0)\n",
    "\n",
    "    # Loop until the text stabilizes\n",
    "    previous_text = \"\"\n",
    "    while text != previous_text:\n",
    "        # print(\"New iteration....\")\n",
    "        previous_text = text\n",
    "        # We use our replacer function to make the decision for each match\n",
    "        text = re.sub(pattern, conditional_replacer, text)\n",
    "    return text\n",
    "\n",
    "# --- Final, Comprehensive Test Cases ---\n",
    "\n",
    "text_to_fix = \"\"\"\n",
    "# 1. Simple case (SHOULD BE FIXED)\n",
    "This is a sentence\n",
    "that should be joined.\n",
    "\n",
    "# 2. Word with backslash (SHOULD BE SKIPPED)\n",
    "The command \\section\n",
    "is followed by \\subsection.\n",
    "\n",
    "# 3. LaTeX table (SHOULD BE SKIPPED)\n",
    "\\midrule\n",
    "DPO & 0.36 & 0.31 \\\\\n",
    "PPO & 0.26 & 0.23 \\\\\n",
    "\\midrule\n",
    "\n",
    "# 4. Multi-line paragraph (SHOULD BE FULLY FIXED)\n",
    "This is another\n",
    "multi\n",
    "line paragraph.\n",
    "\"\"\"\n",
    "\n",
    "# Apply the final, correct function\n",
    "fixed_text = fix_newlines_final(text_to_fix)\n",
    "\n",
    "print(\"--- Original Text ---\")\n",
    "print(text_to_fix)\n",
    "print(\"\\n--- After Final Correct Fix ---\")\n",
    "print(fixed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/arxiv/cleaned_DPO.txt', 'r', encoding='utf-8') as f:\n",
    "    arxiv_paper = f.read()\n",
    "text = arxiv_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\title{Direct Preference Optimization: Your Language Model is Secretly a Reward Model}\n",
      "\n",
      "\\begin{abstract}\n",
      "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training.\n",
      "Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF).\n",
      "However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model.\n",
      "In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss.\n",
      "The resulting algorithm, which we call \\textit{Direct Preference Optimization} (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning.\n",
      "Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.\n",
      "\\end{abstract}\n",
      "\n",
      "\\section{Introduction}\n",
      "Large unsupervised language models (LMs) trained on very large datasets acquire surprising capabilities~\\citep{chowdhery2022palm, brown2020language, touvron2023llama,bubeck2023sparks}. However, these models are trained on data generated by humans with a wide variety of goals, priorities, and skillsets. Some of these goals and skillsets may not be desirable to imitate; for example, while we may want our AI coding assistant to \\textit{understand} common programming mistakes in order to correct them, nevertheless, when generating code, we would like to bias our model toward the (potentially rare) high-quality coding ability present in its training data. Similarly, we might want our language model to be \\textit{aware} of a common misconception believed by 50\\% of people, but we certainly do not want the model to claim this misconception to be true in 50\\% of queries about it! In other words, selecting the model's \\emph{desired responses and behavior} from its very wide \\textit{knowledge and abilities} is crucial to building AI systems that are safe, performant, and controllable \\citep{ouyang2022training}. While existing methods typically steer LMs to match human preferences using reinforcement learning (RL), we will show that the RL-based objective used by existing methods can be optimized exactly with a simple binary cross-entropy objective, greatly simplifying the preference learning pipeline.\n",
      "\n",
      "\\begin{figure}\n",
      "    \\centering\n",
      "    \\includegraphics[width=0.999\\textwidth]{figures/diagrams/teaser.png}\n",
      "    \\caption{\\textbf{DPO optimizes for human preferences while avoiding reinforcement learning.} Existing methods for fine-tuning language models with human feedback first fit a reward model to a dataset of prompts and human preferences over pairs of responses, and then use RL to find a policy that maximizes the learned reward. In contrast, DPO directly optimizes for the policy best satisfying the preferences with a simple classification objective, fitting an \\textit{implicit} reward model whose corresponding optimal policy can be extracted in closed form.}\n",
      "    \\vspace{-2mm}\n",
      "    \\label{fig:teaser}\n",
      "\\end{figure}\n",
      "\n",
      "At a high level, existing methods instill the desired behaviors into a language model using curated sets of human preferences representing the types of behaviors that humans find safe and helpful. This preference learning stage occurs after an initial stage of large-scale unsupervised pre-training on a large text dataset. While the most straightforward approach to preference learning is supervised fine-tuning on human demonstrations of high quality responses, the most successful class of methods is reinforcement learning from human (or AI) feedback (RLHF/RLAIF; \\citep{christiano2017deep,bai2022constitutional}). RLHF methods fit a reward model to a dataset of human preferences and then use RL to optimize a language model policy to produce responses assigned high reward without drifting excessively far from the original model. While RLHF produces models with impressive conversational and coding abilities, the RLHF pipeline is considerably more complex than supervised learning, involving training multiple LMs and sampling from the LM policy in the loop of training, incurring significant computational costs.\n",
      "\n",
      "In this paper, we show how to directly optimize a language model to adhere to human preferences, without explicit reward modeling or reinforcement learning. We propose Direct Preference Optimization (DPO), an algorithm that implicitly optimizes the same objective as existing RLHF algorithms (reward maximization with a KL-divergence constraint) but is simple to implement and straightforward to train. Intuitively, the DPO update increases the relative log probability of preferred to dispreferred responses, but it incorporates a dynamic, per-example importance weight that prevents the model degeneration that we find occurs with a naive probability ratio objective. Like existing algorithms, DPO relies on a theoretical preference model (such as the Bradley-Terry model; \\cite{bradley1952rankanalysis}) that measures how well a given reward function aligns with empirical preference data. However, while existing methods use the preference model to define a preference loss to train a reward model and then train a policy that optimizes the learned reward model, DPO uses a change of variables to define the preference loss as a function of the policy directly. Given a dataset of human preferences over model responses, DPO can therefore optimize a policy using a simple binary cross entropy objective, producing the optimal policy to an implicit reward function fit to the preference data}.\n",
      "\n",
      "Our main contribution is Direct Preference Optimization (DPO), a simple RL-free algorithm for training language models from preferences. Our experiments show that DPO is at least as effective as existing methods, including PPO-based RLHF, for learning from preferences in tasks such as sentiment modulation, summarization, and dialogue, using language models with up to 6B parameters.\n",
      "\n",
      "\\section{Related Work}\n",
      "\n",
      "Self-supervised language models of increasing scale learn to complete some tasks zero-shot \\citep{radford2019language} or with few-shot prompts \\citep{gpt3,megatron,chowdhery2022palm}. However, their performance on downstream tasks and alignment with user intent can be significantly improved by fine-tuning on datasets of instructions and human-written completions \\citep{mishra-etal-2022-cross,sanh2022multitask,chung2022scaling,thoppilan2022lamda}. This `instruction-tuning' procedure enables LLMs to generalize to instructions outside of the instruction-tuning set and generally increase their usability \\citep{chung2022scaling}. Despite the success of instruction tuning, \\textit{relative} human judgments of response quality are often easier to collect than expert demonstrations, and thus subsequent works have fine-tuned LLMs with datasets of human preferences, improving proficiency in translation \\citep{kreutzer-etal-2018-reliability}, summarization \\citep{stiennon2022learning,ziegler2020finetuning}, story-telling \\citep{ziegler2020finetuning}, and instruction-following \\citep{ouyang2022training,ramamurthy2023is}. These methods first optimize a neural network reward function for compatibility with the dataset of preferences under a preference model such as the Bradley-Terry model \\citep{bradley1952rankanalysis}, then fine-tune a language model to maximize the given reward using reinforcement learning algorithms, commonly REINFORCE \\citep{williams1992reinforce}, proximal policy optimization (PPO; \\cite{schulman2017proximal}), or variants \\citep{ramamurthy2023is}. A closely-related line of work leverages LLMs fine-tuned for instruction following with human feedback to generate additional synthetic preference data for targeted attributes such as safety or harmlessness \\citep{bai2022constitutional}, using only weak supervision from humans in the form of a text rubric for the LLM's annotations. These methods represent a convergence of two bodies of work: one body of work on training language models with reinforcement learning for a variety of objectives~\\citep{Ranzato2015SequenceLT,paulus2018a,wu2018learning} and another body of work on general methods for learning from human preferences \\citep{christiano2017deep,kupcsik2018learning}. \n",
      "Despite the appeal of using relative human preferences, fine-tuning large language models with reinforcement learning remains a major practical challenge; this work provides a theoretically-justified approach to optimizing relative preferences without RL.\n",
      "\n",
      "Outside of the context of language, learning policies from preferences has been studied in both bandit and reinforcement learning settings, and several approaches have been proposed. Contextual bandit learning using preferences or rankings of actions, rather than rewards, is known as a contextual dueling bandit (CDB; \\cite{yue2012karmed,dudik2015contextual}). In the absence of absolute rewards, theoretical analysis of CDBs substitutes the notion of an optimal policy with a \\textit{von Neumann winner}, a policy whose expected win rate against \\textit{any} other policy is at least 50\\% \\citep{dudik2015contextual}. However, in the CDB setting, preference labels are given online, while in learning from human preferences, we typically learn from a fixed batch of offline preference-annotated action pairs \\citep{yan2022human}. Similarly, \\textit{preference-based RL} (PbRL) learns from binary preferences generated by an \\textit{unknown} `scoring' function rather than rewards \\citep{BusaFekete2014,ruiz2023dueling}. Various algorithms for PbRL exist, including methods that can reuse off-policy preference data, but generally involve first explicitly estimating the latent scoring function (i.e. the reward model) and subsequently optimizing it \\citep{jain2013learning,BusaFekete2014,christiano2017deep,sadigh2017active,kupcsik2018learning}. We instead present a single stage policy learning approach that directly optimizes a policy to satisfy preferences.\n",
      "\n",
      "\\section{Preliminaries}\\label{section:prelims}\n",
      "\n",
      "We review the RLHF pipeline in \\citeauthor{ziegler2020finetuning} (and later \\citep{stiennon2022learning, bai2022training, ouyang2022training}). It usually includes three phases: 1) supervised fine-tuning (SFT); 2) preference sampling and reward learning and 3) RL optimization.\n",
      "\n",
      "\\textbf{SFT}: RLHF typically begins by fine-tuning a pre-trained LM with supervised learning on high-quality data for the downstream task(s) of interest (dialogue, summarization, etc.), to obtain a model $\\pisft$. \n",
      "\n",
      "\\textbf{Reward Modelling Phase}: In the second phase the SFT model is prompted with prompts $x$ to produce pairs of answers $(y_1, y_2)\\sim \\pisft(y \\mid x)$. These are then presented to human labelers who express preferences for one answer, denoted as $y_w\\succ y_l \\mid x$ where $y_w$ and $y_l$ denotes the preferred and dispreferred completion amongst $(y_1, y_2)$ respectively. The preferences are assumed to be generated by some latent reward model $r^*(y, x)$, which we do not have access to. There are a number of approaches used to model preferences, the Bradley-Terry (BT) \\cite{bradley1952rankanalysis} model being a popular choice (although more general Plackett-Luce ranking models \\citep{plackett1975analysis, luce2012individual} are also compatible with the framework if we have access to several ranked answers). The BT model stipulates that the human preference distribution $p^*$ can be written as:\n",
      "\\begin{equation}\\label{eq:bradley-terry}\n",
      "    p^*(y_1\\succ y_2 \\mid x)=\\frac{\\exp\\left(r^*(x, y_1)\\right)}{\\exp\\left(r^*(x, y_1)\\right) + \\exp\\left(r^*(x, y_2)\\right)}.\n",
      "\\end{equation}\n",
      "Assuming access to a static dataset of comparisons $\\mathcal{D}=\\bigl\\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\\bigr\\}_{i=1}^N$ sampled from $p^*$, we can parametrize a reward model $r_{\\phi}(x, y)$ and estimate the parameters via maximum likelihood. Framing the problem as a binary classification we have the negative log-likelihood loss:\n",
      "\\begin{equation}\\label{eq:reward_model}\n",
      "    \\mathcal{L}_R(r_{\\phi}, \\mathcal{D}) = -\\mathbb{E}_{(x, y_w, y_l)\\sim \\mathcal{D}}\\bigl[\\log \\sigma(r_{\\phi}(x, y_w)- r_{\\phi}(x, y_l))\\bigr]\n",
      "\\end{equation}\n",
      "where $\\sigma$ is the logistic function. In the context of LMs, the network $r_{\\phi}(x, y)$ is often initialized from the SFT model $\\pisft(y \\mid x)$ with the addition of a linear layer on top of the final transformer layer that produces a single scalar prediction for the reward value \\cite{ziegler2020finetuning}. To ensure a reward function with lower variance, prior works normalize the rewards, such that  $\\mathbb{E}_{x,y\\sim \\mathcal{D}}\\left[r_\\phi(x, y)\\right] = 0$ for all $x$.\n",
      "\n",
      "\\textbf{RL Fine-Tuning Phase}: During the RL phase, the learned reward function is used to provide feedback to the language model. Following prior works~\\citep{jaques2017sequence, jaques2020human}, the optimization is formulated as\n",
      "\\begin{equation}\\label{eq:RL}\n",
      "\\max_{\\pi_{\\theta}}  \\mathbb{E}_{x\\sim \\mathcal{D}, y\\sim \\pi_{\\theta}(y \\mid x)}\\bigl[r_{\\phi}(x, y)\\bigr] - \\beta\\mathbb{D}_{\\textrm{KL}}\\bigl[\\pi_{\\theta}(y\\mid x)\\mid \\mid \\piref(y\\mid x)\\bigr],\n",
      "\\end{equation}\n",
      "where $\\beta$ is a parameter controlling the deviation from the base reference policy $\\piref$, namely the initial SFT model $\\pisft$. \n",
      "In practice, the language model policy $\\pi_\\theta$ is also initialized to $\\pisft$. The added constraint is important, as it prevents the model from deviating too far from the distribution on which the reward model is accurate, as well as maintaining the generation diversity and preventing mode-collapse to single high-reward answers. Due to the discrete nature of language generation, this objective is not differentiable and is typically optimized with reinforcement learning. The standard approach \\citep{ziegler2020finetuning, stiennon2022learning, bai2022training, ouyang2022training} has been to construct the reward function ${r(x, y) = r_{\\phi}(x, y) -\\beta (\\log \\pi_{\\theta}(y\\mid x) - \\log \\piref(y\\mid x))}$, and maximize using PPO \\cite{schulman2017proximal}. \n",
      "\n",
      "\\section{Direct Preference Optimization}\\label{sec:DPO}\n",
      "\n",
      "Motivated by the challenges of applying reinforcement learning algorithms on large-scale problems such as fine-tuning language models, our goal is to derive a simple approach for policy optimization using preferences directly. Unlike prior RLHF methods, which learn a reward and then optimize it via RL, our approach leverages a particular choice of reward model parameterization that enables extraction of its optimal policy in closed form, without an RL training loop. \n",
      "As we will describe next in detail, our key insight is to leverage an analytical mapping from reward functions to optimal policies, which enables us to transform a loss function over reward functions into a loss function over policies.\n",
      "This change-of-variables approach avoids fitting an explicit, standalone reward model, while still optimizing under existing models of human preferences, such as the Bradley-Terry model. In essence, the policy network represents both the language model and the (implicit) reward.\n",
      "\n",
      "\\textbf{Deriving the DPO objective.} We start with the same RL objective as prior work, Eq.~\\ref{eq:RL}, under a general reward function $r$. Following prior work~\\citep{peters2007reinforcement, peng2019advantage, korbak2022reinforcement, go2023aligning}, it is straightforward to show that the optimal solution to the KL-constrained reward maximization objective in Eq.~\\ref{eq:RL} takes the form:\n",
      "\\begin{equation}\\label{eq:op_policy}\n",
      "    \\pi_r(y\\mid x) = \\frac{1}{Z(x)}\\piref(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right),\n",
      "\\end{equation}%\n",
      "where $Z(x) =\\sum_{y}\\piref(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)$ is the partition function. See Appendix \\ref{app:derivation1} for a complete derivation. Even if we use the MLE estimate $r_{\\phi}$ of the ground-truth reward function $r^*$, it is still expensive to estimate the partition function $Z(x)$ \\citep{korbak2022reinforcement, go2023aligning}, which makes this representation hard to utilize in practice. However, we can rearrange Eq.~\\ref{eq:op_policy} to express the reward function in terms of its corresponding optimal policy $\\pi_r$, the reference policy $\\piref$, and the unknown partition function $Z(\\cdot)$. Specifically, we first take the logarithm of both sides of Eq.~\\ref{eq:op_policy} and then with some algebra we obtain:\n",
      "\\begin{equation}\\label{eq:main_eq}\n",
      "    r(x,y) =\\beta \\log \\frac{\\pi_r(y\\mid x)}{\\piref(y\\mid x)} + \\beta \\log Z(x).\n",
      "\\end{equation}\n",
      "We can apply this reparameterization to the ground-truth reward $r^*$ and corresponding optimal model $\\pi^*$. Fortunately, the Bradley-Terry model depends only on the difference of rewards between two completions, i.e., ${p^*(y_1 \\succ y_2 \\mid x) = \\sigma(r^*(x, y_1) - r^*(x, y_2))}$. Substituting the reparameterization in Eq.~\\ref{eq:main_eq} for $r^*(x,y)$ into the preference model Eq.~\\ref{eq:bradley-terry}, the partition function cancels, and we can express the human preference probability in terms of only the optimal policy $\\pi^*$ and reference policy $\\piref$. Thus, the optimal RLHF policy $\\pi^*$ under the Bradley-Terry model satisfies the preference model:\n",
      "\\begin{equation}\\label{eq:objective}\n",
      "    p^*(y_1\\succ y_2 \\mid x)=\\frac{1}{1 + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2\\mid x)}{\\piref(y_2\\mid x)} - \\beta \\log \\frac{\\pi^*(y_1\\mid x)}{\\piref(y_1\\mid x)}\\right)}\n",
      "\\end{equation}\n",
      "The derivation is in Appendix~\\ref{app:derivation2}. While Eq.~\\ref{eq:objective} uses the Bradley-Terry model, we can similarly derive expressions under the more general Plackett-Luce models~\\citep{plackett1975analysis, luce2012individual}, shown in Appendix~\\ref{app:plackett_luce_models}.\n",
      "\n",
      "Now that we have the probability of human preference data in terms of the optimal policy rather than the reward model, we can formulate a maximum likelihood objective for a parametrized policy $\\pi_\\theta$. Analogous to the reward modeling approach (i.e. Eq.~\\ref{eq:reward_model}), our policy objective becomes:\n",
      "\\begin{equation}\\label{eq:optimum_model}\n",
      "    \\mathcal{L}_\\text{DPO}(\\pi_{\\theta}; \\piref) = -\\mathbb{E}_{(x, y_w, y_l)\\sim \\mathcal{D}}\\left[\\log \\sigma \\left(\\beta \\log \\frac{\\pi_{\\theta}(y_w\\mid x)}{\\piref(y_w\\mid x)} - \\beta \\log \\frac{\\pi_{\\theta}(y_l\\mid x)}{\\piref(y_l\\mid x)}\\right)\\right].\n",
      "\\end{equation}\n",
      "This way, we fit an implicit reward using an alternative parameterization, whose optimal policy is simply $\\pi_\\theta$. Moreover, since our procedure is equivalent to fitting a reparametrized Bradley-Terry model, it enjoys certain theoretical properties, such as consistencies under suitable assumption of the preference data distribution \\cite{bong2022generalized}. In Section~\\ref{sec:theory}, we further discuss theoretical properties of DPO in relation to other works.\n",
      "\n",
      "\\textbf{What does the DPO update do?} For a mechanistic understanding of DPO, it is useful to analyze the gradient of the loss function $\\mathcal{L}_\\text{DPO}$. The gradient with respect to the parameters $\\theta$ can be written as:\n",
      "\\begin{multline*}\\label{eq:gradient}\n",
      "    \\nabla_\\theta \\mathcal{L}_\\text{DPO}(\\pi_\\theta;\\piref) = \\\\ -\\beta\\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}} \\bigg[\\underbrace{\\sigma(\\hat{r}_\\theta(x, y_l) - \\hat{r}_\\theta (x, y_w))}_\\text{higher weight when reward estimate is wrong}\\bigg[\\underbrace{\\nabla_\\theta\\log \\pi(y_w \\mid x)}_\\text{increase likelihood of $y_w$} - \\underbrace{\\nabla_\\theta\\log\\pi(y_l \\mid x)}_\\text{decrease likelihood of $y_l$}\\bigg]\\bigg],\n",
      "\\end{multline*}\n",
      "where $\\hat{r}_\\theta(x, y) = \\beta \\log \\frac{\\pi_\\theta(y \\mid x)}{\\piref(y \\mid x)}$ is the reward implicitly defined by the language model $\\pi_\\theta$ and reference model $\\piref$ (more in Section~\\ref{sec:theory}). Intuitively, the gradient of the loss function $\\mathcal{L}_\\text{DPO}$ increases the likelihood of the preferred completions $y_w$ and decreases the likelihood of dispreferred completions $y_l$. Importantly, the examples are weighed by how much higher the implicit reward model $\\hat{r}_\\theta$ rates the dispreferred completions, scaled by $\\beta$, i.e, how incorrectly the implicit reward model orders the completions, accounting for the strength of the KL constraint. Our experiments suggest the importance of this weighting, as a na\\\"ive version of this method without the weighting coefficient can cause the language model to degenerate (Appendix Table~\\ref{tab:unlikelihood_generations}).\n",
      "\n",
      "\\textbf{DPO outline.} \n",
      "The general DPO pipeline is as follows: 1) Sample completions $y_1, y_2 \\sim \\piref(\\cdot \\mid x)$ for every prompt $x$, label with human preferences to construct the offline dataset of preferences $\\mathcal{D} = \\{x^{(i)}, y_w^{(i)}, y_l)^{(i)}\\}_{i=1}^N$ and 2) optimize the language model $\\pi_\\theta$ to minimize $\\mathcal{L}_\\text{DPO}$ for the given $\\piref$ and $\\mathcal{D}$ and desired $\\beta$. \n",
      "In practice, one would like to reuse preference datasets publicly available, rather than generating samples and gathering human preferences. Since the preference datasets are sampled using $\\pisft$, we initialize $\\piref = \\pisft$ whenever available. However, when $\\pisft$ is not available, we initialize $\\piref$ by maximizing likelihood of preferred completions ${(x, y_w)}$, that is, ${\\piref = \\argmax_{\\pi}\\mathbb{E}_{x, y_w \\sim \\mathcal{D}}\\left[\\log \\pi(y_w \\mid x)\\right]}$. This procedure helps mitigate the distribution shift between the true reference distribution which is unavailable, and $\\piref$ used by DPO. Further details related to the implementation and hyperparameters can be found in Appendix~\\ref{app:implementation}.\n",
      "\n",
      "\\section{Theoretical Analysis of DPO}\n",
      "In this section, we give further interpretation of the DPO method, provide theoretical backing, and relate advantages of DPO to issues with actor critic algorithms used for RLHF (such as PPO~\\cite{schulman2017proximal}).\n",
      "\n",
      "\\label{sec:theory}\n",
      "\n",
      "\\subsection{Your Language Model Is Secretly a Reward Model} DPO is able to bypass both fitting an explicit reward and performing RL to learn the policy using a single maximum likelihood objective. Note the optimization objective Eq. \\ref{eq:main_eq} is equivalent to a Bradley-Terry model with a reward parameterization $r^*(x, y) = \\beta \\log\\frac{\\pi^*_\\theta(y \\mid x)}{\\piref(y \\mid x)}$ and we optimize our parametric model $\\pi_{\\theta}$, equivalently to the reward model optimization in Eq. \\ref{eq:reward_model} under the change of variables. In this section we will build the theory behind this reparameterization, show that it does not constrain the class of learned reward models, and allows for the exact recovery of the optimal policy. We begin with by defining an equivalence relation between reward functions. \n",
      "\n",
      "\\begin{definition}\n",
      "We say that two reward functions $r(x, y)$ and $r'(x, y)$ are equivalent iff ${r(x, y)-r'(x, y) = f(x)}$ for some function $f$.     \n",
      "\\end{definition}\n",
      "It is easy to see that this is indeed an equivalence relation, which partitions the set of reward functions into classes. We can state the following two lemmas:\n",
      "\n",
      "\\begin{lemma}\\label{lemma:same_prefrence} Under the Plackett-Luce, and in particular the Bradley-Terry, preference framework, two reward functions from the same class induce the same preference distribution.\n",
      "\\end{lemma}\n",
      "\n",
      "\\begin{lemma}\\label{lemma:same_policy}\n",
      "    Two reward functions from the same equivalence class induce the same optimal policy under the constrained RL problem.\n",
      "\\end{lemma}\n",
      "The proofs are straightforward and we defer them to Appendix \\ref{app:lemma1}. The first lemma is a well-known under-specification issue with the Plackett-Luce family of models \\cite{plackett1975analysis}. Due to this under-specification, we usually have to impose additional identifiability constraints to achieve any guarantees on the MLE estimates from Eq. \\ref{eq:reward_model} \\cite{bong2022generalized}. The second lemma states that all reward functions from the same class yield the same optimal policy, hence for our final objective, we are only interested in recovering an arbitrary reward function from the optimal class. We prove the following Theorem in Appendix~\\ref{app:thm1}:\n",
      "\\begin{theorem}\\label{thm:main}\n",
      "    Under mild assumptions, all reward classes consistent with the Plackett-Luce (and Bradley-Terry in particular) models can be represented with the reparameterization ${r(x, y) = \\beta \\log \\frac{\\pi(y\\mid x)}{\\piref(y\\mid x)}}$ for some model $\\pi(y\\mid x)$ and a given reference model $\\piref(y \\mid x)$.\n",
      "\\end{theorem}\n",
      "\\begin{sproof}\n",
      "    Consider any reward function $r(x, y)$, which induces a corresponding optimal model $\\pi_r(y \\mid x)$, specified by Eq. \\ref{eq:op_policy}. We will show that a reward function from the equivalence class of $r$ can be represented using the reparameterization given above. We define the projection $f$ as  \n",
      "\\begin{equation}\n",
      "    f(r; \\piref, \\beta)(x, y) = r(x, y) - \\beta\\log\\sum_{y}\\piref(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)\n",
      "\\end{equation}\n",
      "The operator $f$ simply normalizes the reward function with the logarithm of the partition function of $\\pi_r$. Since the added normalization term is only a function of the prefix $x$, $f(r; \\piref, \\beta)(x, y) $ is a reward function in the equivalence class of $r(x, y)$. Finally, replacing $r$ with the RHS of Eq.~\\ref{eq:main_eq} (which holds for any reward function), we have $f(r; \\piref, \\beta)(x, y) = \\beta \\log \\frac{\\pi_r(y\\mid x)}{\\piref(y\\mid x)}$. That is, the projection $f$ produces a member of the equivalence class of $r$ with the desired form, and we do not lose any generality in our reward model from the proposed reparameterization.\n",
      "\\end{sproof}\n",
      "We can alternatively view Theorem~\\ref{thm:main} as specifying exactly which reward function within each equivalence class the DPO reparameterization selects, that is, the reward function satisfying:\n",
      "\\begin{equation}\\label{eq:lag_p}\n",
      "     \\sum_{y}\\underbrace{\\piref(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)}_{=\\pi(y\\mid x)\\text{, using Thm.~\\ref{thm:main} reparam.}} = 1,\n",
      "\\end{equation}\n",
      "i.e., $\\pi(y\\mid x)$ is a valid distribution (probabilities are positive and sum to 1).\n",
      "However, following Eq.~\\ref{eq:op_policy}, we can see that Eq.~\\ref{eq:lag_p} is the partition function of the optimal policy induced by the reward function $r(x, y)$.\n",
      "The key insight of the DPO algorithm is that we can impose certain constraints on the under-constrained Plackett-Luce (and Bradley-Terry in particular) family of preference models, such that we preserve the class of representable reward models, but explicitly make the optimal policy in Eq. \\ref{eq:op_policy} analytically tractable for all prompts $x$.\n",
      "\n",
      "\\subsection{Instability of Actor-Critic Algorithms}\n",
      "We can also use our framework to diagnose instabilities with standard actor-critic algorithms used for the RLHF, such as PPO. We follow the RLHF pipeline and focus on the RL fine-tuning step outlined in Section \\ref{section:prelims}. We can draw connections to the control as inference framework \\cite{levine2018reinforcement} for the constrained RL problem outlined in \\ref{eq:RL}. We assume a parameterized model $\\pi_{\\theta}(y\\mid x)$ and minimize $\\mathbb{D}_{\\text{KL}}[\\pi_{\\theta}(y|x) \\mid \\mid \\pi^*(y\\mid x)]$ where $\\pi^*$ is the optimal policy from Eq. \\ref{eq:optimum_model} induced by the reward function $r_{\\phi}(y, x)$. With some algebra this leads to the optimization objective:\n",
      "\\begin{equation}\\label{eq:AC}\n",
      "    \\max_{\\pi_{\\theta}}\\mathbb{E}_{\\pi_{\\theta}(y\\mid x)}\\bigg[\\underbrace{r_{\\phi}(x, y) -\\beta\\log\\sum_{y}\\piref(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r_{\\phi}(x, y)\\right)}_{f(r_{\\phi}, \\piref, \\beta)} - \\underbrace{\\beta\\log\\frac{\\pi_{\\theta}(y\\mid x)}{\\piref(y\\mid x)}}_{\\text{KL}}\\bigg]\n",
      "\\end{equation}\n",
      "This is the same objective optimized in prior works \\citep{ziegler2020finetuning, stiennon2022learning, bai2022training, ouyang2022training} using the DPO-equivalent reward for the reward class of $r_{\\phi}$. In this setting, we can interpret the normalization term in $f(r_{\\phi}, \\piref, \\beta)$ as the soft value function of the reference policy $\\piref$. While this term does not affect the optimal solution, without it, the policy gradient of the objective could have high variance, making learning unstable. We can accommodate for the normalization term using a learned value function, but that can also be difficult to optimize. Alternatively, prior works have normalized rewards using a human completion baseline, essentially a single sample Monte-Carlo estimate of the normalizing term. In contrast the DPO reparameterization yields a reward function that does not require any baselines. \n",
      "\n",
      "\\section{Experiments}\n",
      "In this section, we empirically evaluate DPO's ability to train policies directly from preferences. First, in a well-controlled text-generation setting, we ask: how efficiently does DPO trade off maximizing reward and minimizing KL-divergence with the reference policy, compared to common preference learning algorithms such as PPO? Next, we evaluate DPO's performance on larger models and more difficult RLHF tasks, including summarization and dialogue. We find that with almost no tuning of hyperparameters, DPO tends to perform as well or better than strong baselines like RLHF with PPO as well as returning the best of $N$ sampled trajectories under a learned reward function. Before presenting these results, we describe the experimental set-up; additional details are in Appendix~\\ref{app:exp_details}.\n",
      "\n",
      "\\textbf{Tasks.} Our experiments explore three different open-ended text generation tasks. For all experiments, algorithms learn a policy from a dataset of preferences $\\mathcal{D}=\\bigl\\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\\bigr\\}_{i=1}^N$. In \\textbf{controlled sentiment generation}, $x$ is a prefix of a movie review from the IMDb dataset \\cite{maas-EtAl:2011:ACL-HLT2011}, and the policy must generate $y$ with positive sentiment. In order to perform a controlled evaluation, for this experiment we \\textit{generate} preference pairs over generations using a pre-trained sentiment classifier, where $p(\\text{positive}\\mid x,y_w)>p(\\text{positive}\\mid x,y_l)$. For SFT, we fine-tune GPT-2-large until convergence on reviews from the train split of the IMDB dataset (further details in App~\\ref{app:sentiment_details}). In \\textbf{summarization}, $x$ is a forum post from Reddit; the policy must generate a summary $y$ of the main points in the post. Following prior work, we use the Reddit TL;DR summarization dataset \\citep{volske-etal-2017-tl} along with human preferences gathered by \\citeauthor{stiennon2022learning}. We use an SFT model fine-tuned on human-written forum post summaries\\footnote{\\url{https://huggingface.co/CarperAI/openai_summarize_tldr_sft}} with the TRLX \\citep{leandro_von_werra_2023_7790115} framework for RLHF. The human preference dataset was gathered by \\citeauthor{stiennon2022learning} on samples from a different, but similarly-trained, SFT model. Finally, in \\textbf{single-turn dialogue}, \n",
      "$x$ is a human query, which may be anything from a question about astrophysics to a request for relationship advice. A policy must produce an engaging and helpful response $y$ to a user's query; we use the Anthropic Helpful and Harmless dialogue dataset \\citep{bai2022training}, containing 170k dialogues between a human and an automated assistant. Each transcript ends with a pair of responses generated by a large (although unknown) language model along with a preference label denoting the human-preferred response. In this setting, no pre-trained SFT model is available; we therefore fine-tune an off-the-shelf language model on only the preferred completions to form the SFT model.\n",
      "\n",
      "\\begin{figure}\n",
      "    \\centering\n",
      "    \\includegraphics[width=0.50\\textwidth]{figures/results/frontier.pdf}\n",
      "    \\includegraphics[width=0.49\\textwidth]{figures/results/tldr_winrate_vs_temp.pdf}\n",
      "    \\caption{\\textbf{Left.} The frontier of expected reward vs KL to the reference policy. DPO provides the highest expected reward for all KL values, demonstrating the quality of the optimization. \\textbf{Right.} TL;DR summarization win rates vs. human-written summaries, using GPT-4 as evaluator. DPO exceeds PPO's best-case performance on summarization, while being more robust to changes in the sampling temperature.}\n",
      "    \\vspace{-2mm}\n",
      "    \\label{fig:frontier-tldr-main}\n",
      "\\end{figure}\n",
      "\n",
      "\\textbf{Evaluation.} Our experiments use two different approaches to evaluation. In order to analyze the effectiveness of each algorithm in optimizing the constrained reward maximization objective, in the controlled sentiment generation setting we evaluate each algorithm by its frontier of achieved reward and KL-divergence from the reference policy; this frontier is computable because we have acccess to the ground-truth reward function (a sentiment classifier). However, in the real world, the ground truth reward function is not known; therefore, we evaluate algorithms with their \\textit{win rate} against a baseline policy, using GPT-4 as a proxy for human evaluation of summary quality and response helpfulness in the summarization and single-turn dialogue settings, respectively. For summarization, we use reference summaries in the test set as the baseline; for dialogue, we use the preferred response in the test dataset as the baseline. While existing studies suggest LMs can be better automated evaluators than existing metrics \\citep{Chen2023ExploringTU}, we conduct a human study to justify our usage of GPT-4 for evaluation in Sec.~\\ref{sec:human-judgments}. We find GPT-4 judgments correlate strongly with humans, with human agreement with GPT-4 typically similar or higher than inter-human annotator agreement. \n",
      "\n",
      "\\textbf{Methods.} In addition to DPO, we evaluate several existing approaches to training language models to adhere to human preferences. Most simply, we explore zero-shot prompting with \\textbf{GPT-J} \\citep{gpt-j} in the summarization task and 2-shot prompting with \\textbf{Pythia-2.8B} \\citep{biderman2023pythia} in the dialogue task. In addition, we evaluate the \\textbf{SFT} model as well as \\textbf{Preferred-FT}, which is a model fine-tuned with supervised learning on the chosen completion $y_w$ from either the SFT model (in controlled sentiment and summarization) or a generic LM (in single-turn dialogue). Another pseudo-supervised method is \\textbf{Unlikelihood}~\\citep{welleck2019neural}, which simply optimizes the policy to maximize the probability assigned to $y_w$ and \\textit{minimize} the probability assigned to $y_l$; we use an optional coefficient $\\alpha\\in[0,1]$ on the `unlikelihood' term. We also consider \\textbf{PPO} \\citep{schulman2017proximal} using a reward function learned from the preference data and \\textbf{PPO-GT}, which is an oracle that learns from the ground truth reward function available in the controlled sentiment setting. In our sentiment experiments, we use two implementations of PPO-GT, one of-the-shelf version \\cite{leandro_von_werra_2023_7790115} as well as a modified version that normalizes rewards and further tunes hyperparameters to improve performance (we also use these modifications when running `normal' PPO with learned rewards). Finally, we consider the \\textbf{Best of $N$} baseline, sampling $N$ responses from the SFT model (or Preferred-FT in dialogue) and returning the highest-scoring response according to a reward function learned from the preference dataset. This high-performing method decouples the quality of the reward model from the PPO optimization, but is computationally impractical even for moderate $N$ as it requires sampling $N$ completions for every query at test time.\n",
      "\n",
      "\\subsection{How well can DPO optimize the RLHF objective?}\n",
      "\n",
      "\\begin{figure}\n",
      "    \\centering\n",
      "    \\includegraphics[width=0.50\\textwidth]{figures/results/dialogue_winrate_vs_temp.pdf}\n",
      "    \\includegraphics[width=0.49\\textwidth]{figures/results/dialogue_winrate_vs_steps.pdf}\n",
      "    \\caption{\\textbf{Left.} Win rates computed by GPT-4 for Anthropic-HH one-step dialogue; DPO is the only method that improves over chosen summaries in the Anthropic-HH test set. \\textbf{Right.} Win rates for different sampling temperatures over the course of training. DPO's improvement over the dataset labels is fairly stable over the course of training for different sampling temperatures.}\n",
      "    \\vspace{-2mm}\n",
      "    \\label{fig:dialogue-main}\n",
      "\\end{figure}\n",
      "\n",
      "The KL-constrained reward maximization objective used in typical RLHF algorithms balances exploitation of reward while restricting the policy from deviating far from the reference policy. Therefore, when comparing algorithms, we must take into account both reward achieved as well as the KL discrepancy; achieving slightly higher reward but with much higher KL is not necessarily desirable. Figure~\\ref{fig:frontier-tldr-main} shows the reward-KL frontier for various algorithms in the sentiment setting. We execute multiple training runs for each algorithm, using a different hyperparameter for policy conservativeness in each run (target KL $\\in\\{3,6,9,12\\}$ for PPO, $\\beta \\in \\{0.05,0.1,1,5\\}$, $\\alpha\\in\\{0.05,0.1,0.5,1\\}$ for unlikelihood, random seeds for preferred-FT). This sweep includes 22 runs in total. After each 100 training steps until convergence, we evaluate each policy on a set of test prompts, computing the average reward under the true reward function as well as the average sequence-level KL\\footnote{That is, the sum of the per-timestep KL-divergences.} with the reference policy $\\text{KL}\\left(\\pi\\mid \\mid \\piref\\right)$. We find that DPO produces by far the most efficient frontier, achieving the highest reward while still achieving low KL. This result is particularly notable for multiple reasons. First, DPO and PPO optimize the same objective, but DPO is notably more efficient; DPO's reward/KL tradeoff strictly dominates PPO. Second, DPO achieves a better frontier than PPO, \\emph{even when PPO can access ground truth rewards} (PPO-GT).\n",
      "\n",
      "\\subsection{Can DPO scale to real preference datasets?}\n",
      "\\label{sec:dpo-real-datasets}\n",
      "Next, we evaluate fine-tuning performance of DPO on summarization and single-turn dialogue. For summarization, \n",
      "automatic evaluation metrics such as ROUGE can be poorly correlated with human preferences~\\citep{stiennon2022learning}, and prior work has found that fine-tuning LMs using PPO on human preferences to provide more effective summaries. We evaluate different methods by sampling completions on the test split of TL;DR summarization dataset, and computing the average win rate against reference completions in the test set. The completions for all methods are sampled at temperatures varying from 0.0 to 1.0, and the win rates are shown in Figure~\\ref{fig:frontier-tldr-main} (right). DPO, PPO and Preferred-FT all fine-tune the same GPT-J SFT model\\footnote{\\url{https://huggingface.co/CarperAI/openai_summarize_tldr_sft}}. We find that DPO has a win rate of approximately 61\\% at a temperature of 0.0, exceeding the performance of PPO at ~57\\% at its optimal sampling temperature of 0.0. DPO also achieves a higher maximum win rate compared to the best of $N$ baseline. We note that we did not meaningfully tune DPO's $\\beta$ hyperparameter, so these results may underestimate DPO's potential. Moreover, we find DPO to be much more robust to the sampling temperature than PPO, the performance of which can degrade to that of the base GPT-J model at high temperatures. Preferred-FT does not improve significantly over the SFT model. We also compare DPO and PPO head-to-head in human evaluations in Section~\\ref{sec:human-judgments}, where DPO samples at temperature 0.25 were preferred 58\\% times over PPO samples at temperature 0.\n",
      "\n",
      "On single-turn dialogue, we evaluate the different methods on the subset of the test split of the Anthropic HH dataset \\citep{bai2022training} with one step of human-assistant interaction. GPT-4 evaluations use the preferred completions on the test as the reference to compute the win rate for different methods. As there is no standard SFT model for this task, we start with a pre-trained Pythia-2.8B, use Preferred-FT to train a reference model on the chosen completions such that completions are within distribution of the model, and then train using DPO. We also compare against the best of 128 Preferred-FT completions (we found the Best of $N$ baseline plateaus at 128 completions for this task; see Appendix Figure~\\ref{fig:best-of-n}) and a 2-shot prompted version of the Pythia-2.8B base model, finding DPO performs as well or better for the best-performing temperatures for each method. We also evaluate an RLHF model trained with PPO on the Anthropic HH dataset \\footnote{\\url{https://huggingface.co/reciprocate/ppo_hh_pythia-6B}} from a well-known source \\footnote{\\url{https://github.com/CarperAI/trlx/tree/main/examples/hh}}, but are unable to find a prompt or sampling temperature that gives performance better than the base Pythia-2.8B model. Based on our results from TL;DR and the fact that both methods optimize the same reward function, we consider Best of 128 a rough proxy for PPO-level performance. Overall, DPO is the only computationally efficient method that improves over the preferred completions in the Anthropic HH dataset, and provides similar or better performance to the computationally demanding Best of 128 baseline. Finally, Figure~\\ref{fig:dialogue-main} shows that DPO converges to its best performance relatively quickly.\n",
      "\n",
      "\\subsection{Generalization to a new input distribution}\n",
      "\n",
      "\\begin{wraptable}{r}{0.375\\textwidth}\n",
      "    \\small\n",
      "    \\vspace{-10mm}\n",
      "    \\begin{tabular}{ccc}\n",
      "        \\toprule\n",
      "        & \\multicolumn{2}{c}{\\textbf{Win rate vs. ground truth}} \\\\\n",
      "        \\cmidrule(lr){2-3}\n",
      "        \\textbf{Alg.} & Temp $0$ & Temp $0.25$ \\\\\n",
      "        \\midrule DPO & 0.36 & 0.31 \\\\\n",
      "        PPO & 0.26 & 0.23 \\\\\n",
      "        \\bottomrule\n",
      "    \\end{tabular}\n",
      "    \\caption{GPT-4 win rates vs. ground truth summaries for out-of-distribution CNN/DailyMail input articles.}\n",
      "    \\vspace{-3mm}\n",
      "    \\label{tab:ood}\n",
      "\\end{wraptable}\n",
      "\n",
      "To further compare the performance of PPO and DPO under distribution shifts, we evaluate the PPO and DPO policies from our Reddit TL;DR summarization experiment on a different distribution, news articles in the test split of the CNN/DailyMail dataset \\citep{nallapati-etal-2016-abstractive}, using the best sampling temperatures from TL;DR (0 and 0.25). The results are presented in Table~\\ref{tab:ood}. We computed the GPT-4 win rate against the ground-truth summaries in the datasets, using the same GPT-4 (C) prompt we used for Reddit TL;DR, but replacing the words ``forum post'' with ``news article''. For this new distribution, DPO continues to outperform the PPO policy by a significant margin. This experiment provides initial evidence that DPO policies can generalize similarly well to PPO policies, even though DPO does not use the additional unlabeled Reddit TL;DR prompts that PPO uses.\n",
      "\n",
      "\\subsection{Validating GPT-4 judgments with human judgments}\n",
      "\\label{sec:human-judgments}\n",
      "We conduct a human study to verify the reliability of GPT-4's judgments, using the results of the TL;DR summarization experiment and two different GPT-4 prompts. The \\textbf{GPT-4 (S)} (simple) prompt simply asks for which summary better-summarizes the important information in the post. The \\textbf{GPT-4 (C)} (concise) prompt also asks for which summary is more concise; we evaluate this prompt because we find that GPT-4 prefers longer, more repetitive summaries than humans do with the \\textbf{GPT-4 (S)} prompt. See Appendix~\\ref{app:prompts} for the complete prompts. We perform three comparisons, using the highest (DPO, temp. 0.25), the lowest (PPO, temp. 1.0), and a \\begin{wraptable}{r}{0.47\\textwidth}\n",
      "    \\centering\n",
      "    \\small\n",
      "    \\vspace{-1.5mm}\n",
      "    \\begin{tabular}{lccc}\n",
      "    \\toprule\n",
      "        & \\textbf{DPO} & \\textbf{SFT} & \\textbf{PPO-1} \\\\\n",
      "        \\cmidrule(lr){2-4}\n",
      "        N respondents & 272 & 122 & 199 \\\\\n",
      "        \\midrule GPT-4 (S) win \\% & 47 & 27 & 13 \\\\\n",
      "        GPT-4 (C) win \\% & 54 & 32 & 12 \\\\\n",
      "        Human win \\% & 58 & 43 & 17 \\\\\n",
      "        \\midrule GPT-4 (S)-H agree & 70 & 77 & 86 \\\\\n",
      "        GPT-4 (C)-H agree & 67 & 79 & 85 \\\\\n",
      "        H-H agree & 65 & - & 87 \\\\\n",
      "        \\bottomrule\n",
      "    \\end{tabular}\n",
      "    \\vspace{-1mm}\n",
      "    \\caption{Comparing human and GPT-4 win rates and per-judgment agreement on TL;DR summarization samples. \\textbf{Humans agree with GPT-4 about as much as they agree with each other.} Each experiment compares a summary from the stated method with a summary from PPO with temperature 0.}\n",
      "    \\vspace{-5mm}\n",
      "    \\label{tab:human_results}\n",
      "\\end{wraptable}middle-performing (SFT, temp. 0.25) method with the aim of covering a diversity of sample qualities; all three methods are compared against greedily-sampled PPO (its best-performing temperature). We find that with both prompts, GPT-4 tends to agree with humans about as often as humans agree with each other, suggesting that GPT-4 is a reasonable proxy for human evaluations (due to limited human raters, we only collect multiple human judgments for the DPO and PPO-1 comparisons). Overall, the \\textbf{GPT-4 (C)} prompt generally provides win rates more representative of humans; we therefore use this prompt for the main results in Section~\\ref{sec:dpo-real-datasets}. For additional details about the human study, including the web interface presented to raters and the list of human volunteers, see Appendix~\\ref{app:human-study}.\n",
      "\n",
      "\\section{Discussion}\n",
      "Learning from preferences is a powerful, scalable framework for training capable, aligned language models. We have introduced DPO, a simple training paradigm for training language models from preferences without reinforcement learning. Rather than coercing the preference learning problem into a standard RL setting in order to use off-the-shelf RL algorithms, DPO identifies a mapping between language model policies and reward functions that enables training a language model to satisfy human preferences \\textit{directly}, with a simple cross-entropy loss, without reinforcement learning or loss of generality. With virtually no tuning of hyperparameters, DPO performs similarly or better than existing RLHF algorithms, including those based on PPO; DPO thus meaningfully reduces the barrier to training more language models from human preferences.\n",
      "\n",
      "\\textbf{Limitations \\& Future Work.} Our results raise several important questions for future work. How does the DPO policy generalize out of distribution, compared with learning from an explicit reward function? Our initial results suggest that DPO policies can generalize similarly to PPO-based models, but more comprehensive study is needed. For example, can training with self-labeling from the DPO policy similarly make effective use of unlabeled prompts? On another front, how does reward over-optimization manifest in the direct preference optimization setting, and is the slight decrease in performance in Figure~\\ref{fig:dialogue-main}-right an instance of it? Additionally, while we evaluate models up to 6B parameters, exploration of scaling DPO to state-of-the-art models orders of magnitude larger is an exciting direction for future work. Regarding evaluations, we find that the win rates computed by GPT-4 are impacted by the prompt; future work may study the best way to elicit high-quality judgments from automated systems. Finally, many possible applications of DPO exist beyond training language models from human preferences, including training generative models in other modalities.\n",
      "\n",
      "\\end{document}\n"
     ]
    }
   ],
   "source": [
    "# 1. fix \\rev{..}{..}\n",
    "import re\n",
    "text = re.sub(r'\\\\rev\\{[^{}]*\\}\\{([^{}]*)\\}', r'\\1', text)\n",
    "\n",
    "# 2. collapse erroneous line breaks between words\n",
    "pattern = r'\\b([a-zA-Z]+)\\b\\s+\\b([a-zA-Z]+)\\b'\n",
    "while True:\n",
    "    new = re.sub(pattern, r'\\1 \\2', text)\n",
    "    if new == text:\n",
    "        break\n",
    "    text = new\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's clean CoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "# Read main.tex file from data/arxiv directory\n",
    "with open('../data/arxiv/CoT.tex', 'r') as f:\n",
    "    main_tex = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def fix_newlines_final(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Fixes misplaced newlines with a strict set of rules:\n",
    "    - Finds any two non-whitespace chunks separated by clean whitespace.\n",
    "    - A fix is only applied IF AND ONLY IF both chunks are purely alphabetic.\n",
    "    \"\"\"\n",
    "    # A general pattern to find candidates: two non-whitespace chunks\n",
    "    # separated by whitespace that does NOT contain a backslash.\n",
    "    # [^\\\\s] is a negated set: any character that is NOT a backslash or whitespace.\n",
    "    # We use this to avoid matching the '\\\\' from a LaTeX line break.\n",
    "    pattern = r'(\\S+)([ \\t]*\\n[ \\t\\n]*)(\\S+)'\n",
    "    def conditional_replacer(match: re.Match) -> str:\n",
    "        \"\"\"\n",
    "        The core logic: only act on purely alphabetic words.\n",
    "        \"\"\"\n",
    "        word1 = match.group(1)\n",
    "        word2 = match.group(3)\n",
    "        # print(word1, word2)\n",
    "        # THE CRITICAL CHECK: Are both words composed *only* of letters?\n",
    "        if word1.isalpha() and word2.isalpha():\n",
    "            # Yes. Perform the replacement.\n",
    "            return f'{word1} {word2}'\n",
    "        else:\n",
    "            # No. One or both words contain non-alphabetic characters (\\, &, . etc).\n",
    "            # Return the original matched text to skip the replacement.\n",
    "            return match.group(0)\n",
    "\n",
    "    # Loop until the text stabilizes\n",
    "    previous_text = \"\"\n",
    "    while text != previous_text:\n",
    "        # print(\"New iteration....\")\n",
    "        previous_text = text\n",
    "        # We use our replacer function to make the decision for each match\n",
    "        text = re.sub(pattern, conditional_replacer, text)\n",
    "    return text\n",
    "\n",
    "def clean_latex(latex_content: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans a LaTeX string by extracting title, abstract, and main body,\n",
    "    and removing commands and environments that don't contribute to the main text.\n",
    "    \"\"\"\n",
    "    # 1. Extract title\n",
    "    title_match = re.search(r'\\\\title\\{(.*?)\\}', latex_content, re.DOTALL)\n",
    "    title = \"\"\n",
    "    if title_match:\n",
    "        title = title_match.group(1)\n",
    "        title = re.sub(r'\\\\\\\\\\s*', ' ', title)\n",
    "        title = title.strip()\n",
    "\n",
    "    # 2. Extract abstract\n",
    "    abstract_match = re.search(r'\\\\begin\\{abstract\\}(.*?)\\\\end\\{abstract\\}', latex_content, re.DOTALL)\n",
    "    abstract = abstract_match.group(1).strip() if abstract_match else \"\"\n",
    "\n",
    "    # 3. Find start of main body (first section after document environment)\n",
    "    body_start_index = -1\n",
    "    doc_start_match = re.search(r'\\\\begin\\{document\\}', latex_content)\n",
    "    if doc_start_match:\n",
    "        # Find the first section after \\begin{document}\n",
    "        section_match = re.search(r'\\\\section', latex_content[doc_start_match.end():])\n",
    "        if section_match:\n",
    "            body_start_index = doc_start_match.end() + section_match.start()\n",
    "\n",
    "    if body_start_index == -1:\n",
    "        # Fallback if structure is unexpected, return what we have so far\n",
    "        body = \"\"\n",
    "    else:\n",
    "        body = latex_content[body_start_index:]\n",
    "    \n",
    "    # 4. Find end of main body (before references, appendix, etc.)\n",
    "    end_markers = [\n",
    "        r'\\\\begin\\{thebibliography\\}', r'\\\\bibliography', r'\\\\appendix',\n",
    "        r'\\\\section\\*?\\{Acknowledgements\\}', r'\\\\section\\*?\\{Author Contributions\\}'\n",
    "    ]\n",
    "    end_index = len(body)\n",
    "    for marker_regex in end_markers:\n",
    "        end_match = re.search(marker_regex, body)\n",
    "        if end_match:\n",
    "            end_index = min(end_index, end_match.start())\n",
    "    \n",
    "    body = body[:end_index]\n",
    "\n",
    "    # Combine the parts we want to keep\n",
    "    full_text = f'\\\\title{{{title}}}\\n\\n\\\\begin{{abstract}}\\n{abstract}\\n\\\\end{{abstract}}\\n\\n{body}\\n\\\\end{{document}}'\n",
    "\n",
    "    # Now apply cleaning operations from the original function\n",
    "    cleaned_text = full_text\n",
    "\n",
    "    # Get rid of \\rev{..}{..}\n",
    "    cleaned_text = re.sub(r'\\\\rev\\{[^{}]*\\}\\{([^{}]*)\\}', r'\\1', cleaned_text)\n",
    "\n",
    "    # Fix newlines between words\n",
    "    cleaned_text = fix_newlines_final(cleaned_text)\n",
    "    \n",
    "    # remove \\clearpage\n",
    "    cleaned_text = re.sub(r'\\\\clearpage', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\n{3,}', '\\n\\n', cleaned_text)\n",
    "\n",
    "\n",
    "    return cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = clean_latex(main_tex)\n",
    "\n",
    "# Save cleaned text to file\n",
    "with open('../data/arxiv/cleaned_CoT.txt', 'w') as f:\n",
    "    f.write(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's paraphrase the article 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/arxiv/cleaned_DPO.txt', 'r', encoding='utf-8') as f:\n",
    "    arxiv_paper = f.read()\n",
    "cleaned_paper = arxiv_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\title{Direct Preference Optimization: Your Language Model is Secretly a Reward Model}\n",
      "\n",
      "\\begin{abstract}\n",
      "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training.\n",
      "Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF).\n",
      "However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model.\n",
      "In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss.\n",
      "The resulting algorithm, which we call \\textit{Direct Preference Optimization} (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning.\n",
      "Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.\n",
      "\\end{abstract}\n",
      "\n",
      "\\section{Introduction}\n",
      "Large unsupervised language models (LMs) trained on very large datasets acquire surprising capabilities~\\citep{chowdhery2022palm, brown2020language, touvron2023llama,bubeck2023sparks}. However, these models are trained on data generated by humans with a wide variety of goals, priorities, and skillsets. Some of these goals and skillsets may not be desirable to imitate; for example, while we may want our AI coding assistant to \\textit{understand} common programming mistakes in order to correct them, nevertheless, when generating code, we would like to bias our model toward the (potentially rare) high-quality coding ability present in its training data. Similarly, we might want our language model to be \\textit{aware} of a common misconception believed by 50\\% of people, but we certainly do not want the model to claim this misconception to be true in 50\\% of queries about it! In other words, selecting the model's \\emph{desired responses and behavior} from its very wide \\textit{knowledge and abilities} is crucial to building AI systems that are safe, performant, and controllable \\citep{ouyang2022training}. While existing methods typically steer LMs to match human preferences using reinforcement learning (RL), we will show that the RL-based objective used by existing methods can be optimized exactly with a simple binary cross-entropy objective, greatly simplifying the preference learning pipeline.\n",
      "\n",
      "\\begin{figure}\n",
      "    \\centering\n",
      "    \\includegraphics[width=0.999\\textwidth]{figures/diagrams/teaser.png}\n",
      "    \\caption{\\textbf{DPO optimizes for human preferences while avoiding reinforcement learning.} Existing methods for fine-tuning language models with human feedback first fit a reward model to a dataset of prompts and human preferences over pairs of responses, and then use RL to find a policy that maximizes the learned reward. In contrast, DPO directly optimizes for the policy best satisfying the preferences with a simple classification objective, fitting an \\textit{implicit} reward model whose corresponding optimal policy can be extracted in closed form.}\n",
      "    \\vspace{-2mm}\n",
      "    \\label{fig:teaser}\n",
      "\\end{figure}\n",
      "\n",
      "At a high level, existing methods instill the desired behaviors into a language model using curated sets of human preferences representing the types of behaviors that humans find safe and helpful. This preference learning stage occurs after an initial stage of large-scale unsupervised pre-training on a large text dataset. While the most straightforward approach to preference learning is supervised fine-tuning on human demonstrations of high quality responses, the most successful class of methods is reinforcement learning from human (or AI) feedback (RLHF/RLAIF; \\citep{christiano2017deep,bai2022constitutional}). RLHF methods fit a reward model to a dataset of human preferences and then use RL to optimize a language model policy to produce responses assigned high reward without drifting excessively far from the original model. While RLHF produces models with impressive conversational and coding abilities, the RLHF pipeline is considerably more complex than supervised learning, involving training multiple LMs and sampling from the LM policy in the loop of training, incurring significant computational costs.\n",
      "\n",
      "In this paper, we show how to directly optimize a language model to adhere to human preferences, without explicit reward modeling or reinforcement learning. We propose Direct Preference Optimization (DPO), an algorithm that implicitly optimizes the same objective as existing RLHF algorithms (reward maximization with a KL-divergence constraint) but is simple to implement and straightforward to train. Intuitively, the DPO update increases the relative log probability of preferred to dispreferred responses, but it incorporates a dynamic, per-example importance weight that prevents the model degeneration that we find occurs with a naive probability ratio objective. Like existing algorithms, DPO relies on a theoretical preference model (such as the Bradley-Terry model; \\cite{bradley1952rankanalysis}) that measures how well a given reward function aligns with empirical preference data. However, while existing methods use the preference model to define a preference loss to train a reward model and then train a policy that optimizes the learned reward model, DPO uses a change of variables to define the preference loss as a function of the policy directly. Given a dataset of human preferences over model responses, DPO can therefore optimize a policy using a simple binary cross entropy objective, producing the optimal policy to an implicit reward function fit to the preference data}.\n",
      "\n",
      "Our main contribution is Direct Preference Optimization (DPO), a simple RL-free algorithm for training language models from preferences. Our experiments show that DPO is at least as effective as existing methods, including PPO-based RLHF, for learning from preferences in tasks such as sentiment modulation, summarization, and dialogue, using language models with up to 6B parameters.\n",
      "\n",
      "\\section{Related Work}\n",
      "\n",
      "Self-supervised language models of increasing scale learn to complete some tasks zero-shot \\citep{radford2019language} or with few-shot prompts \\citep{gpt3,megatron,chowdhery2022palm}. However, their performance on downstream tasks and alignment with user intent can be significantly improved by fine-tuning on datasets of instructions and human-written completions \\citep{mishra-etal-2022-cross,sanh2022multitask,chung2022scaling,thoppilan2022lamda}. This `instruction-tuning' procedure\n",
      "enables LLMs to generalize to instructions outside of the instruction-tuning set and generally increase their usability \\citep{chung2022scaling}. Despite the success of instruction tuning, \\textit{relative} human judgments of response quality are often easier to collect than expert demonstrations, and thus subsequent works have fine-tuned LLMs with datasets of human preferences, improving proficiency in translation \\citep{kreutzer-etal-2018-reliability}, summarization \\citep{stiennon2022learning,ziegler2020finetuning}, story-telling \\citep{ziegler2020finetuning}, and instruction-following \\citep{ouyang2022training,ramamurthy2023is}. These methods first optimize a neural network reward function for compatibility with the dataset of preferences under a preference model such as the Bradley-Terry model \\citep{bradley1952rankanalysis}, then fine-tune a language model to maximize the given reward using reinforcement learning algorithms, commonly REINFORCE \\citep{williams1992reinforce}, proximal policy optimization (PPO; \\cite{schulman2017proximal}), or variants \\citep{ramamurthy2023is}. A closely-related line of work leverages LLMs fine-tuned for instruction following with human feedback to generate additional synthetic preference data for targeted attributes such as safety or harmlessness \\citep{bai2022constitutional}, using only weak supervision from humans in the form of a text rubric for the LLM's annotations. These methods represent a convergence of two bodies of work: one body of work on training language models with reinforcement learning for a variety of objectives~\\citep{Ranzato2015SequenceLT,paulus2018a,wu2018learning} and another body of work on general methods for learning from human preferences \\citep{christiano2017deep,kupcsik2018learning}. \n",
      "Despite the appeal of using relative human preferences, fine-tuning large language models with reinforcement learning remains a major practical challenge; this work provides a theoretically-justified approach to optimizing relative preferences without RL.\n",
      "\n",
      "Outside of the context of language, learning policies from preferences has been studied in both bandit and reinforcement learning settings, and several approaches have been proposed. Contextual bandit learning using preferences or rankings of actions, rather than rewards, is known as a contextual dueling bandit (CDB; \\cite{yue2012karmed,dudik2015contextual}). In the absence of absolute rewards, theoretical analysis of CDBs substitutes the notion of an optimal policy with a \\textit{von Neumann winner}, a policy whose expected win rate against \\textit{any} other policy is at least 50\\% \\citep{dudik2015contextual}. However, in the CDB setting, preference labels are given online, while in learning from human preferences, we typically learn from a fixed batch of offline preference-annotated action pairs \\citep{yan2022human}. Similarly, \\textit{preference-based RL} (PbRL) learns from binary preferences generated by an \\textit{unknown} `scoring' function rather than rewards \\citep{BusaFekete2014,ruiz2023dueling}. Various algorithms for PbRL exist, including methods that can reuse off-policy preference data, but generally involve first explicitly estimating the latent scoring function (i.e. the reward model) and subsequently optimizing it \\citep{jain2013learning,BusaFekete2014,christiano2017deep,sadigh2017active,kupcsik2018learning}. We instead present a single stage policy learning approach that directly optimizes a policy to satisfy preferences.\n",
      "\n",
      "\\section{Preliminaries}\\label{section:prelims}\n",
      "\n",
      "We review the RLHF pipeline in \\citeauthor{ziegler2020finetuning} (and later \\citep{stiennon2022learning, bai2022training, ouyang2022training}). It usually includes three phases: 1) supervised fine-tuning (SFT); 2) preference sampling and reward learning and 3) RL optimization.\n",
      "\n",
      "\\textbf{SFT}: RLHF typically begins by fine-tuning a pre-trained LM with supervised learning on high-quality data for the downstream task(s) of interest (dialogue, summarization, etc.), to obtain a model $\\pisft$. \n",
      "\n",
      "\\textbf{Reward Modelling Phase}: In the second phase the SFT model is prompted with prompts $x$ to produce pairs of answers $(y_1, y_2)\\sim \\pisft(y \\mid x)$. These are then presented to human labelers who express preferences for one answer, denoted as $y_w\\succ y_l \\mid x$ where $y_w$ and $y_l$ denotes the preferred and dispreferred completion amongst $(y_1, y_2)$ respectively. The preferences are assumed to be generated by some latent reward model $r^*(y, x)$, which we do not have access to. There are a number of approaches used to model preferences, the Bradley-Terry (BT) \\cite{bradley1952rankanalysis} model being a popular choice (although more general Plackett-Luce ranking models \\citep{plackett1975analysis, luce2012individual} are also compatible with the framework if we have access to several ranked answers). The BT model stipulates that the human preference distribution $p^*$ can be written as:\n",
      "\\begin{equation}\\label{eq:bradley-terry}\n",
      "    p^*(y_1\\succ y_2 \\mid x)=\\frac{\\exp\\left(r^*(x, y_1)\\right)}{\\exp\\left(r^*(x, y_1)\\right) + \\exp\\left(r^*(x, y_2)\\right)}.\n",
      "\\end{equation}\n",
      "Assuming access to a static dataset of comparisons $\\mathcal{D}=\\bigl\\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\\bigr\\}_{i=1}^N$ sampled from $p^*$, we can parametrize a reward model $r_{\\phi}(x, y)$ and estimate the parameters via maximum likelihood. Framing the problem as a binary classification we have the negative log-likelihood loss:\n",
      "\\begin{equation}\\label{eq:reward_model}\n",
      "    \\mathcal{L}_R(r_{\\phi}, \\mathcal{D}) = -\\mathbb{E}_{(x, y_w, y_l)\\sim \\mathcal{D}}\\bigl[\\log \\sigma(r_{\\phi}(x, y_w)- r_{\\phi}(x, y_l))\\bigr]\n",
      "\\end{equation}\n",
      "where $\\sigma$ is the logistic function. In the context of LMs, the network $r_{\\phi}(x, y)$ is often initialized from the SFT model $\\pisft(y \\mid x)$ with the addition of a linear layer on top of the final transformer layer that produces a single scalar prediction for the reward value \\cite{ziegler2020finetuning}. To ensure a reward function with lower variance, prior works normalize the rewards, such that  $\\mathbb{E}_{x,y\\sim \\mathcal{D}}\\left[r_\\phi(x, y)\\right] = 0$ for all $x$.\n",
      "\n",
      "\\textbf{RL Fine-Tuning Phase}: During the RL phase, the learned reward function is used to provide feedback to the language model. Following prior works~\\citep{jaques2017sequence, jaques2020human}, the optimization is formulated as\n",
      "\\begin{equation}\\label{eq:RL}\n",
      "\\max_{\\pi_{\\theta}}  \\mathbb{E}_{x\\sim \\mathcal{D}, y\\sim \\pi_{\\theta}(y \\mid x)}\\bigl[r_{\\phi}(x, y)\\bigr] - \\beta\\mathbb{D}_{\\textrm{KL}}\\bigl[\\pi_{\\theta}(y\\mid x)\\mid \\mid \\piref(y\\mid x)\\bigr],\n",
      "\\end{equation}\n",
      "where $\\beta$ is a parameter controlling the deviation from the base reference policy $\\piref$, namely the initial SFT model $\\pisft$. \n",
      "In practice, the language model policy $\\pi_\\theta$ is also initialized to $\\pisft$. The added constraint is important, as it prevents the model from deviating too far from the distribution on which the reward model is accurate, as well as maintaining the generation diversity and preventing mode-collapse to single high-reward answers. Due to the discrete nature of language generation, this objective is not differentiable and is typically optimized with reinforcement learning. The standard approach \\citep{ziegler2020finetuning, stiennon2022learning, bai2022training, ouyang2022training} has been to construct the reward function ${r(x, y) = r_{\\phi}(x, y) -\\beta (\\log \\pi_{\\theta}(y\\mid x) - \\log \\piref(y\\mid x))}$, and maximize using PPO \\cite{schulman2017proximal}. \n",
      "\n",
      "\\section{Direct Preference Optimization}\\label{sec:DPO}\n",
      "\n",
      "Motivated by the challenges of applying reinforcement learning algorithms on large-scale problems such as fine-tuning language models, our goal is to derive a simple approach for policy optimization using preferences directly. Unlike prior RLHF methods, which learn a reward and then optimize it via RL, our approach leverages a particular choice of reward model parameterization that enables extraction of its optimal policy in closed form, without an RL training loop. \n",
      "As we will describe next in detail, our key insight is to leverage an analytical mapping from reward functions to optimal policies, which enables us to transform a loss function over reward functions into a loss function over policies.\n",
      "This change-of-variables approach avoids fitting an explicit, standalone reward model, while still optimizing under existing models of human preferences, such as the Bradley-Terry model. In essence, the policy network represents both the language model and the (implicit) reward.\n",
      "\n",
      "\\textbf{Deriving the DPO objective.} We start with the same RL objective as prior work, Eq.~\\ref{eq:RL}, under a general reward function $r$. Following prior work~\\citep{peters2007reinforcement, peng2019advantage, korbak2022reinforcement, go2023aligning}, it is straightforward to show that the optimal solution to the KL-constrained reward maximization objective in Eq.~\\ref{eq:RL} takes the form:\n",
      "\\begin{equation}\\label{eq:op_policy}\n",
      "    \\pi_r(y\\mid x) = \\frac{1}{Z(x)}\\piref(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right),\n",
      "\\end{equation}%\n",
      "where $Z(x) =\\sum_{y}\\piref(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)$ is the partition function. See Appendix \\ref{app:derivation1} for a complete derivation. Even if we use the MLE estimate $r_{\\phi}$ of the ground-truth reward function $r^*$, it is still expensive to estimate the partition function $Z(x)$ \\citep{korbak2022reinforcement, go2023aligning}, which makes this representation hard to utilize in practice. However, we can rearrange Eq.~\\ref{eq:op_policy} to express the reward function in terms of its corresponding optimal policy $\\pi_r$, the reference policy $\\piref$, and the unknown partition function $Z(\\cdot)$. Specifically, we first take the logarithm of both sides of Eq.~\\ref{eq:op_policy} and then with some algebra we obtain:\n",
      "\\begin{equation}\\label{eq:main_eq}\n",
      "    r(x,y) =\\beta \\log \\frac{\\pi_r(y\\mid x)}{\\piref(y\\mid x)} + \\beta \\log Z(x).\n",
      "\\end{equation}\n",
      "We can apply this reparameterization to the ground-truth reward $r^*$ and corresponding optimal model $\\pi^*$. Fortunately, the Bradley-Terry model depends only on the difference of rewards between two completions, i.e., ${p^*(y_1 \\succ y_2 \\mid x) = \\sigma(r^*(x, y_1) - r^*(x, y_2))}$. Substituting the reparameterization in Eq.~\\ref{eq:main_eq} for $r^*(x,y)$ into the preference model Eq.~\\ref{eq:bradley-terry}, the partition function cancels, and we can express the human preference probability in terms of only the optimal policy $\\pi^*$ and reference policy $\\piref$. Thus, the optimal RLHF policy $\\pi^*$ under the Bradley-Terry model satisfies the preference model:\n",
      "\\begin{equation}\\label{eq:objective}\n",
      "    p^*(y_1\\succ y_2 \\mid x)=\\frac{1}{1 + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2\\mid x)}{\\piref(y_2\\mid x)} - \\beta \\log \\frac{\\pi^*(y_1\\mid x)}{\\piref(y_1\\mid x)}\\right)}\n",
      "\\end{equation}\n",
      "The derivation is in Appendix~\\ref{app:derivation2}. While Eq.~\\ref{eq:objective} uses the Bradley-Terry model, we can similarly derive expressions under the more general Plackett-Luce models~\\citep{plackett1975analysis, luce2012individual}, shown in Appendix~\\ref{app:plackett_luce_models}.\n",
      "\n",
      "Now that we have the probability of human preference data in terms of the optimal policy rather than the reward model, we can formulate a maximum likelihood objective for a parametrized policy $\\pi_\\theta$. Analogous to the reward modeling approach (i.e. Eq.~\\ref{eq:reward_model}), our policy objective becomes:\n",
      "\\begin{equation}\\label{eq:optimum_model}\n",
      "    \\mathcal{L}_\\text{DPO}(\\pi_{\\theta}; \\piref) = -\\mathbb{E}_{(x, y_w, y_l)\\sim \\mathcal{D}}\\left[\\log \\sigma \\left(\\beta \\log \\frac{\\pi_{\\theta}(y_w\\mid x)}{\\piref(y_w\\mid x)} - \\beta \\log \\frac{\\pi_{\\theta}(y_l\\mid x)}{\\piref(y_l\\mid x)}\\right)\\right].\n",
      "\\end{equation}\n",
      "This way, we fit an implicit reward using an alternative parameterization, whose optimal policy is simply $\\pi_\\theta$. Moreover, since our procedure is equivalent to fitting a reparametrized Bradley-Terry model, it enjoys certain theoretical properties, such as consistencies under suitable assumption of the preference data distribution \\cite{bong2022generalized}. In Section~\\ref{sec:theory}, we further discuss theoretical properties of DPO in relation to other works.\n",
      "\n",
      "\\textbf{What does the DPO update do?} For a mechanistic understanding of DPO, it is useful to analyze the gradient of the loss function $\\mathcal{L}_\\text{DPO}$. The gradient with respect to the parameters $\\theta$ can be written as:\n",
      "\\begin{multline*}\\label{eq:gradient}\n",
      "    \\nabla_\\theta \\mathcal{L}_\\text{DPO}(\\pi_\\theta;\\piref) = \\\\ -\\beta\\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}} \\bigg[\\underbrace{\\sigma(\\hat{r}_\\theta(x, y_l) - \\hat{r}_\\theta (x, y_w))}_\\text{higher weight when reward estimate is wrong}\\bigg[\\underbrace{\\nabla_\\theta\\log \\pi(y_w \\mid x)}_\\text{increase likelihood of $y_w$} - \\underbrace{\\nabla_\\theta\\log\\pi(y_l \\mid x)}_\\text{decrease likelihood of $y_l$}\\bigg]\\bigg],\n",
      "\\end{multline*}\n",
      "where $\\hat{r}_\\theta(x, y) = \\beta \\log \\frac{\\pi_\\theta(y \\mid x)}{\\piref(y \\mid x)}$ is the reward implicitly defined by the language model $\\pi_\\theta$ and reference model $\\piref$ (more in Section~\\ref{sec:theory}). Intuitively, the gradient of the loss function $\\mathcal{L}_\\text{DPO}$ increases the likelihood of the preferred completions $y_w$ and decreases the likelihood of dispreferred completions $y_l$. Importantly, the examples are weighed by how much higher the implicit reward model $\\hat{r}_\\theta$ rates the dispreferred completions, scaled by $\\beta$, i.e, how incorrectly the implicit reward model orders the completions, accounting for the strength of the KL constraint. Our experiments suggest the importance of this weighting, as a na\\\"ive version of this method without the weighting coefficient can cause the language model to degenerate (Appendix Table~\\ref{tab:unlikelihood_generations}).\n",
      "\n",
      "\\textbf{DPO outline.} \n",
      "The general DPO pipeline is as follows: 1) Sample completions $y_1, y_2 \\sim \\piref(\\cdot \\mid x)$ for every prompt $x$, label with human preferences to construct the offline dataset of preferences $\\mathcal{D} = \\{x^{(i)}, y_w^{(i)}, y_l)^{(i)}\\}_{i=1}^N$ and 2) optimize the language model $\\pi_\\theta$ to minimize $\\mathcal{L}_\\text{DPO}$ for the given $\\piref$ and $\\mathcal{D}$ and desired $\\beta$. \n",
      "In practice, one would like to reuse preference datasets publicly available, rather than generating samples and gathering human preferences. Since the preference datasets are sampled using $\\pisft$, we initialize $\\piref = \\pisft$ whenever available. However, when $\\pisft$ is not available, we initialize $\\piref$ by maximizing likelihood of preferred completions ${(x, y_w)}$, that is, ${\\piref = \\argmax_{\\pi}\\mathbb{E}_{x, y_w \\sim \\mathcal{D}}\\left[\\log \\pi(y_w \\mid x)\\right]}$. This procedure helps mitigate the distribution shift between the true reference distribution which is unavailable, and $\\piref$ used by DPO. Further details related to the implementation and hyperparameters can be found in Appendix~\\ref{app:implementation}.\n",
      "\n",
      "\\section{Theoretical Analysis of DPO}\n",
      "In this section, we give further interpretation of the DPO method, provide theoretical backing, and relate advantages of DPO to issues with actor critic algorithms used for RLHF (such as PPO~\\cite{schulman2017proximal}).\n",
      "\n",
      "\\label{sec:theory}\n",
      "\n",
      "\\subsection{Your Language Model Is Secretly a Reward Model} DPO is able to bypass both fitting an explicit reward and performing RL to learn the policy using a single maximum likelihood objective. Note the optimization objective Eq. \\ref{eq:main_eq} is equivalent to a Bradley-Terry model with a reward parameterization $r^*(x, y) = \\beta \\log\\frac{\\pi^*_\\theta(y \\mid x)}{\\piref(y \\mid x)}$ and we optimize our parametric model $\\pi_{\\theta}$, equivalently to the reward model optimization in Eq. \\ref{eq:reward_model} under the change of variables. In this section we will build the theory behind this reparameterization, show that it does not constrain the class of learned reward models, and allows for the exact recovery of the optimal policy. We begin with by defining an equivalence relation between reward functions. \n",
      "\n",
      "\\begin{definition}\n",
      "We say that two reward functions $r(x, y)$ and $r'(x, y)$ are equivalent iff ${r(x, y)-r'(x, y) = f(x)}$ for some function $f$.     \n",
      "\\end{definition}\n",
      "It is easy to see that this is indeed an equivalence relation, which partitions the set of reward functions into classes. We can state the following two lemmas:\n",
      "\n",
      "\\begin{lemma}\\label{lemma:same_prefrence} Under the Plackett-Luce, and in particular the Bradley-Terry, preference framework, two reward functions from the same class induce the same preference distribution.\n",
      "\\end{lemma}\n",
      "\n",
      "\\begin{lemma}\\label{lemma:same_policy}\n",
      "    Two reward functions from the same equivalence class induce the same optimal policy under the constrained RL problem.\n",
      "\\end{lemma}\n",
      "The proofs are straightforward and we defer them to Appendix \\ref{app:lemma1}. The first lemma is a well-known under-specification issue with the Plackett-Luce family of models \\cite{plackett1975analysis}. Due to this under-specification, we usually have to impose additional identifiability constraints to achieve any guarantees on the MLE estimates from Eq. \\ref{eq:reward_model} \\cite{bong2022generalized}. The second lemma states that all reward functions from the same class yield the same optimal policy, hence for our final objective, we are only interested in recovering an arbitrary reward function from the optimal class. We prove the following Theorem in Appendix~\\ref{app:thm1}:\n",
      "\\begin{theorem}\\label{thm:main}\n",
      "    Under mild assumptions, all reward classes consistent with the Plackett-Luce (and Bradley-Terry in particular) models can be represented with the reparameterization ${r(x, y) = \\beta \\log \\frac{\\pi(y\\mid x)}{\\piref(y\\mid x)}}$ for some model $\\pi(y\\mid x)$ and a given reference model $\\piref(y \\mid x)$.\n",
      "\\end{theorem}\n",
      "\\begin{sproof}\n",
      "    Consider any reward function $r(x, y)$, which induces a corresponding optimal model $\\pi_r(y \\mid x)$, specified by Eq. \\ref{eq:op_policy}. We will show that a reward function from the equivalence class of $r$ can be represented using the reparameterization given above. We define the projection $f$ as  \n",
      "\\begin{equation}\n",
      "    f(r; \\piref, \\beta)(x, y) = r(x, y) - \\beta\\log\\sum_{y}\\piref(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)\n",
      "\\end{equation}\n",
      "The operator $f$ simply normalizes the reward function with the logarithm of the partition function of $\\pi_r$. Since the added normalization term is only a function of the prefix $x$, $f(r; \\piref, \\beta)(x, y) $ is a reward function in the equivalence class of $r(x, y)$. Finally, replacing $r$ with the RHS of Eq.~\\ref{eq:main_eq} (which holds for any reward function), we have $f(r; \\piref, \\beta)(x, y) = \\beta \\log \\frac{\\pi_r(y\\mid x)}{\\piref(y\\mid x)}$. That is, the projection $f$ produces a member of the equivalence class of $r$ with the desired form, and we do not lose any generality in our reward model from the proposed reparameterization.\n",
      "\\end{sproof}\n",
      "We can alternatively view Theorem~\\ref{thm:main} as specifying exactly which reward function within each equivalence class the DPO reparameterization selects, that is, the reward function satisfying:\n",
      "\\begin{equation}\\label{eq:lag_p}\n",
      "     \\sum_{y}\\underbrace{\\piref(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)}_{=\\pi(y\\mid x)\\text{, using Thm.~\\ref{thm:main} reparam.}} = 1,\n",
      "\\end{equation}\n",
      "i.e., $\\pi(y\\mid x)$ is a valid distribution (probabilities are positive and sum to 1).\n",
      "However, following Eq.~\\ref{eq:op_policy}, we can see that Eq.~\\ref{eq:lag_p} is the partition function of the optimal policy induced by the reward function $r(x, y)$.\n",
      "The key insight of the DPO algorithm is that we can impose certain constraints on the under-constrained Plackett-Luce (and Bradley-Terry in particular) family of preference models, such that we preserve the class of representable reward models, but explicitly make the optimal policy in Eq. \\ref{eq:op_policy} analytically tractable for all prompts $x$.\n",
      "\n",
      "\\subsection{Instability of Actor-Critic Algorithms}\n",
      "We can also use our framework to diagnose instabilities with standard actor-critic algorithms used for the RLHF, such as PPO. We follow the RLHF pipeline and focus on the RL fine-tuning step outlined in Section \\ref{section:prelims}. We can draw connections to the control as inference framework \\cite{levine2018reinforcement} for the constrained RL problem outlined in \\ref{eq:RL}. We assume a parameterized model $\\pi_{\\theta}(y\\mid x)$ and minimize $\\mathbb{D}_{\\text{KL}}[\\pi_{\\theta}(y|x) \\mid \\mid \\pi^*(y\\mid x)]$ where $\\pi^*$ is the optimal policy from Eq. \\ref{eq:optimum_model} induced by the reward function $r_{\\phi}(y, x)$. With some algebra this leads to the optimization objective:\n",
      "\\begin{equation}\\label{eq:AC}\n",
      "    \\max_{\\pi_{\\theta}}\\mathbb{E}_{\\pi_{\\theta}(y\\mid x)}\\bigg[\\underbrace{r_{\\phi}(x, y) -\\beta\\log\\sum_{y}\\piref(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r_{\\phi}(x, y)\\right)}_{f(r_{\\phi}, \\piref, \\beta)} - \\underbrace{\\beta\\log\\frac{\\pi_{\\theta}(y\\mid x)}{\\piref(y\\mid x)}}_{\\text{KL}}\\bigg]\n",
      "\\end{equation}\n",
      "This is the same objective optimized in prior works \\citep{ziegler2020finetuning, stiennon2022learning, bai2022training, ouyang2022training} using the DPO-equivalent reward for the reward class of $r_{\\phi}$. In this setting, we can interpret the normalization term in $f(r_{\\phi}, \\piref, \\beta)$ as the soft value function of the reference policy $\\piref$. While this term does not affect the optimal solution, without it, the policy gradient of the objective could have high variance, making learning unstable. We can accommodate for the normalization term using a learned value function, but that can also be difficult to optimize. Alternatively, prior works have normalized rewards using a human completion baseline, essentially a single sample Monte-Carlo estimate of the normalizing term. In contrast the DPO reparameterization yields a reward function that does not require any baselines. \n",
      "\n",
      "\\section{Experiments}\n",
      "In this section, we empirically evaluate DPO's ability to train policies directly from preferences. First, in a well-controlled text-generation setting, we ask: how efficiently does DPO trade off maximizing reward and minimizing KL-divergence with the reference policy, compared to common preference learning algorithms such as PPO? Next, we evaluate DPO's performance on larger models and more difficult RLHF tasks, including summarization and dialogue. We find that with almost no tuning of hyperparameters, DPO tends to perform as well or better than strong baselines like RLHF with PPO as well as returning the best of $N$ sampled trajectories under a learned reward function. Before presenting these results, we describe the experimental set-up; additional details are in Appendix~\\ref{app:exp_details}.\n",
      "\n",
      "\\textbf{Tasks.} Our experiments explore three different open-ended text generation tasks. For all experiments, algorithms learn a policy from a dataset of preferences $\\mathcal{D}=\\bigl\\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\\bigr\\}_{i=1}^N$. In \\textbf{controlled sentiment generation}, $x$ is a prefix of a movie review from the IMDb dataset \\cite{maas-EtAl:2011:ACL-HLT2011}, and the policy must generate $y$ with positive sentiment. In order to perform a controlled evaluation, for this experiment we \\textit{generate} preference pairs over generations using a pre-trained sentiment classifier, where $p(\\text{positive}\\mid x,y_w)>p(\\text{positive}\\mid x,y_l)$. For SFT, we fine-tune GPT-2-large until convergence on reviews from the train split of the IMDB dataset (further details in App~\\ref{app:sentiment_details}). In \\textbf{summarization}, $x$ is a forum post from Reddit; the policy must generate a summary $y$ of the main points in the post. Following prior work, we use the Reddit TL;DR summarization dataset \\citep{volske-etal-2017-tl} along with human preferences gathered by \\citeauthor{stiennon2022learning}. We use an SFT model fine-tuned on human-written forum post summaries\\footnote{\\url{https://huggingface.co/CarperAI/openai_summarize_tldr_sft}} with the TRLX \\citep{leandro_von_werra_2023_7790115} framework for RLHF. The human preference dataset was gathered by \\citeauthor{stiennon2022learning} on samples from a different, but similarly-trained, SFT model. Finally, in \\textbf{single-turn dialogue}, \n",
      "$x$ is a human query, which may be anything from a question about astrophysics to a request for relationship advice. A policy must produce an engaging and helpful response $y$ to a user's query; we use the Anthropic Helpful and Harmless dialogue dataset \\citep{bai2022training}, containing 170k dialogues between a human and an automated assistant. Each transcript ends with a pair of responses generated by a large (although unknown) language model along with a preference label denoting the human-preferred response. In this setting, no pre-trained SFT model is available; we therefore fine-tune an off-the-shelf language model on only the preferred completions to form the SFT model.\n",
      "\n",
      "\\begin{figure}\n",
      "    \\centering\n",
      "    \\includegraphics[width=0.50\\textwidth]{figures/results/frontier.pdf}\n",
      "    \\includegraphics[width=0.49\\textwidth]{figures/results/tldr_winrate_vs_temp.pdf}\n",
      "    \\caption{\\textbf{Left.} The frontier of expected reward vs KL to the reference policy. DPO provides the highest expected reward for all KL values, demonstrating the quality of the optimization. \\textbf{Right.} TL;DR summarization win rates vs. human-written summaries, using GPT-4 as evaluator. DPO exceeds PPO's best-case performance on summarization, while being more robust to changes in the sampling temperature.}\n",
      "    \\vspace{-2mm}\n",
      "    \\label{fig:frontier-tldr-main}\n",
      "\\end{figure}\n",
      "\n",
      "\\textbf{Evaluation.} Our experiments use two different approaches to evaluation. In order to analyze the effectiveness of each algorithm in optimizing the constrained reward maximization objective, in the controlled sentiment generation setting we evaluate each algorithm by its frontier of achieved reward and KL-divergence from the reference policy; this frontier is computable because we have acccess to the ground-truth reward function (a sentiment classifier). However, in the real world, the ground truth reward function is not known; therefore, we evaluate algorithms with their \\textit{win rate} against a baseline policy, using GPT-4 as a proxy for human evaluation of summary quality and response helpfulness in the summarization and single-turn dialogue settings, respectively. For summarization, we use reference summaries in the test set as the baseline; for dialogue, we use the preferred response in the test dataset as the baseline. While existing studies suggest LMs can be better automated evaluators than existing metrics \\citep{Chen2023ExploringTU}, we conduct a human study to justify our usage of GPT-4 for evaluation in Sec.~\\ref{sec:human-judgments}. We find GPT-4 judgments correlate strongly with humans, with human agreement with GPT-4 typically similar or higher than inter-human annotator agreement. \n",
      "\n",
      "\\textbf{Methods.} In addition to DPO, we evaluate several existing approaches to training language models to adhere to human preferences. Most simply, we explore zero-shot prompting with \\textbf{GPT-J} \\citep{gpt-j} in the summarization task and 2-shot prompting with \\textbf{Pythia-2.8B} \\citep{biderman2023pythia} in the dialogue task. In addition, we evaluate the \\textbf{SFT} model as well as \\textbf{Preferred-FT}, which is a model fine-tuned with supervised learning on the chosen completion $y_w$ from either the SFT model (in controlled sentiment and summarization) or a generic LM (in single-turn dialogue). Another pseudo-supervised method is \\textbf{Unlikelihood}~\\citep{welleck2019neural}, which simply optimizes the policy to maximize the probability assigned to $y_w$ and \\textit{minimize} the probability assigned to $y_l$; we use an optional coefficient $\\alpha\\in[0,1]$ on the `unlikelihood' term. We also consider \\textbf{PPO} \\citep{schulman2017proximal} using a reward function learned from the preference data and \\textbf{PPO-GT}, which is an oracle that learns from the ground truth reward function available in the controlled sentiment setting. In our sentiment experiments, we use two implementations of PPO-GT, one of-the-shelf version \\cite{leandro_von_werra_2023_7790115} as well as a modified version that normalizes rewards and further tunes hyperparameters to improve performance (we also use these modifications when running `normal' PPO with learned rewards). Finally, we consider the \\textbf{Best of $N$} baseline, sampling $N$ responses from the SFT model (or Preferred-FT in dialogue) and returning the highest-scoring response according to a reward function learned from the preference dataset. This high-performing method decouples the quality of the reward model from the PPO optimization, but is computationally impractical even for moderate $N$ as it requires sampling $N$ completions for every query at test time.\n",
      "\n",
      "\\subsection{How well can DPO optimize the RLHF objective?}\n",
      "\n",
      "\\begin{figure}\n",
      "    \\centering\n",
      "    \\includegraphics[width=0.50\\textwidth]{figures/results/dialogue_winrate_vs_temp.pdf}\n",
      "    \\includegraphics[width=0.49\\textwidth]{figures/results/dialogue_winrate_vs_steps.pdf}\n",
      "    \\caption{\\textbf{Left.} Win rates computed by GPT-4 for Anthropic-HH one-step dialogue; DPO is the only method that improves over chosen summaries in the Anthropic-HH test set. \\textbf{Right.} Win rates for different sampling temperatures over the course of training. DPO's improvement over the dataset labels is fairly stable over the course of training for different sampling temperatures.}\n",
      "    \\vspace{-2mm}\n",
      "    \\label{fig:dialogue-main}\n",
      "\\end{figure}\n",
      "\n",
      "The KL-constrained reward maximization objective used in typical RLHF algorithms balances exploitation of reward while restricting the policy from deviating far from the reference policy. Therefore, when comparing algorithms, we must take into account both reward achieved as well as the KL discrepancy; achieving slightly higher reward but with much higher KL is not necessarily desirable. Figure~\\ref{fig:frontier-tldr-main} shows the reward-KL frontier for various algorithms in the sentiment setting. We execute multiple training runs for each algorithm, using a different hyperparameter for policy conservativeness in each run (target KL $\\in\\{3,6,9,12\\}$ for PPO, $\\beta \\in \\{0.05,0.1,1,5\\}$, $\\alpha\\in\\{0.05,0.1,0.5,1\\}$ for unlikelihood, random seeds for preferred-FT). This sweep includes 22 runs in total. After each 100 training steps until convergence, we evaluate each policy on a set of test prompts, computing the average reward under the true reward function as well as the average sequence-level KL\\footnote{That is, the sum of the per-timestep KL-divergences.} with the reference policy $\\text{KL}\\left(\\pi\\mid \\mid \\piref\\right)$. We find that DPO produces by far the most efficient frontier, achieving the highest reward while still achieving low KL. This result is particularly notable for multiple reasons. First, DPO and PPO optimize the same objective, but DPO is notably more efficient; DPO's reward/KL tradeoff strictly dominates PPO. Second, DPO achieves a better frontier than PPO, \\emph{even when PPO can access ground truth rewards} (PPO-GT).\n",
      "\n",
      "\\subsection{Can DPO scale to real preference datasets?}\n",
      "\\label{sec:dpo-real-datasets}\n",
      "Next, we evaluate fine-tuning performance of DPO on summarization and single-turn dialogue. For summarization, \n",
      "automatic evaluation metrics such as ROUGE can be poorly correlated with human preferences~\\citep{stiennon2022learning}, and prior work has found that fine-tuning LMs using PPO on human preferences to provide more effective summaries. We evaluate different methods by sampling completions on the test split of TL;DR summarization dataset, and computing the average win rate against reference completions in the test set. The completions for all methods are sampled at temperatures varying from 0.0 to 1.0, and the win rates are shown in Figure~\\ref{fig:frontier-tldr-main} (right). DPO, PPO and Preferred-FT all fine-tune the same GPT-J SFT model\\footnote{\\url{https://huggingface.co/CarperAI/openai_summarize_tldr_sft}}. We find that DPO has a win rate of approximately 61\\% at a temperature of 0.0, exceeding the performance of PPO at ~57\\% at its optimal sampling temperature of 0.0. DPO also achieves a higher maximum win rate compared to the best of $N$ baseline. We note that we did not meaningfully tune DPO's $\\beta$ hyperparameter, so these results may underestimate DPO's potential. Moreover, we find DPO to be much more robust to the sampling temperature than PPO, the performance of which can degrade to that of the base GPT-J model at high temperatures. Preferred-FT does not improve significantly over the SFT model. We also compare DPO and PPO head-to-head in human evaluations in Section~\\ref{sec:human-judgments}, where DPO samples at temperature 0.25 were preferred 58\\% times over PPO samples at temperature 0.\n",
      "\n",
      "On single-turn dialogue, we evaluate the different methods on the subset of the test split of the Anthropic HH dataset \\citep{bai2022training} with one step of human-assistant interaction. GPT-4 evaluations use the preferred completions on the test as the reference to compute the win rate for different methods. As there is no standard SFT model for this task, we start with a pre-trained Pythia-2.8B, use Preferred-FT to train a reference model on the chosen completions such that completions are within distribution of the model, and then train using DPO. We also compare against the best of 128 Preferred-FT completions (we found the Best of $N$ baseline plateaus at 128 completions for this task; see Appendix Figure~\\ref{fig:best-of-n}) and a 2-shot prompted version of the Pythia-2.8B base model, finding DPO performs as well or better for the best-performing temperatures for each method. We also evaluate an RLHF model trained with PPO on the Anthropic HH dataset \\footnote{\\url{https://huggingface.co/reciprocate/ppo_hh_pythia-6B}} from a well-known source \\footnote{\\url{https://github.com/CarperAI/trlx/tree/main/examples/hh}}, but are unable to find a prompt or sampling temperature that gives performance better than the base Pythia-2.8B model. Based on our results from TL;DR and the fact that both methods optimize the same reward function, we consider Best of 128 a rough proxy for PPO-level performance. Overall, DPO is the only computationally efficient method that improves over the preferred completions in the Anthropic HH dataset, and provides similar or better performance to the computationally demanding Best of 128 baseline. Finally, Figure~\\ref{fig:dialogue-main} shows that DPO converges to its best performance relatively quickly.\n",
      "\n",
      "\\subsection{Generalization to a new input distribution}\n",
      "\n",
      "\\begin{wraptable}{r}{0.375\\textwidth}\n",
      "    \\small\n",
      "    \\vspace{-10mm}\n",
      "    \\begin{tabular}{ccc}\n",
      "        \\toprule\n",
      "        & \\multicolumn{2}{c}{\\textbf{Win rate vs. ground truth}} \\\\\n",
      "        \\cmidrule(lr){2-3}\n",
      "        \\textbf{Alg.} & Temp $0$ & Temp $0.25$ \\\\\n",
      "        \\midrule\n",
      "        DPO & 0.36 & 0.31 \\\\\n",
      "        PPO & 0.26 & 0.23 \\\\\n",
      "        \\bottomrule\n",
      "    \\end{tabular}\n",
      "    \\caption{GPT-4 win rates vs. ground truth summaries for out-of-distribution CNN/DailyMail input articles.}\n",
      "    \\vspace{-3mm}\n",
      "    \\label{tab:ood}\n",
      "\\end{wraptable}\n",
      "\n",
      "To further compare the performance of PPO and DPO under distribution shifts, we evaluate the PPO and DPO policies from our Reddit TL;DR summarization experiment on a different distribution, news articles in the test split of the CNN/DailyMail dataset \\citep{nallapati-etal-2016-abstractive}, using the best sampling temperatures from TL;DR (0 and 0.25). The results are presented in Table~\\ref{tab:ood}. We computed the GPT-4 win rate against the ground-truth summaries in the datasets, using the same GPT-4 (C) prompt we used for Reddit TL;DR, but replacing the words ``forum post'' with ``news article''. For this new distribution, DPO continues to outperform the PPO policy by a significant margin. This experiment provides initial evidence that DPO policies can generalize similarly well to PPO policies, even though DPO does not use the additional unlabeled Reddit TL;DR prompts that PPO uses.\n",
      "\n",
      "\\subsection{Validating GPT-4 judgments with human judgments}\n",
      "\\label{sec:human-judgments}\n",
      "We conduct a human study to verify the reliability of GPT-4's judgments, using the results of the TL;DR summarization experiment and two different GPT-4 prompts. The \\textbf{GPT-4 (S)} (simple) prompt simply asks for which summary better-summarizes the important information in the post. The \\textbf{GPT-4 (C)} (concise) prompt also asks for which summary is more concise; we evaluate this prompt because we find that GPT-4 prefers longer, more repetitive summaries than humans do with the \\textbf{GPT-4 (S)} prompt. See Appendix~\\ref{app:prompts} for the complete prompts. We perform three comparisons, using the highest (DPO, temp. 0.25), the lowest (PPO, temp. 1.0), and a \\begin{wraptable}{r}{0.47\\textwidth}\n",
      "    \\centering\n",
      "    \\small\n",
      "    \\vspace{-1.5mm}\n",
      "    \\begin{tabular}{lccc}\n",
      "    \\toprule\n",
      "        & \\textbf{DPO} & \\textbf{SFT} & \\textbf{PPO-1} \\\\\n",
      "        \\cmidrule(lr){2-4}\n",
      "        N respondents & 272 & 122 & 199 \\\\\n",
      "        \\midrule\n",
      "        GPT-4 (S) win \\% & 47 & 27 & 13 \\\\\n",
      "        GPT-4 (C) win \\% & 54 & 32 & 12 \\\\\n",
      "        Human win \\% & 58 & 43 & 17 \\\\\n",
      "        \\midrule\n",
      "        GPT-4 (S)-H agree & 70 & 77 & 86 \\\\\n",
      "        GPT-4 (C)-H agree & 67 & 79 & 85 \\\\\n",
      "        H-H agree & 65 & - & 87 \\\\\n",
      "        \\bottomrule\n",
      "    \\end{tabular}\n",
      "    \\vspace{-1mm}\n",
      "    \\caption{Comparing human and GPT-4 win rates and per-judgment agreement on TL;DR summarization samples. \\textbf{Humans agree with GPT-4 about as much as they agree with each other.} Each experiment compares a summary from the stated method with a summary from PPO with temperature 0.}\n",
      "    \\vspace{-5mm}\n",
      "    \\label{tab:human_results}\n",
      "\\end{wraptable}middle-performing (SFT, temp. 0.25) method with the aim of covering a diversity of sample qualities; all three methods are compared against greedily-sampled PPO (its best-performing temperature). We find that with both prompts, GPT-4 tends to agree with humans about as often as humans agree with each other, suggesting that GPT-4 is a reasonable proxy for human evaluations (due to limited human raters, we only collect multiple human judgments for the DPO and PPO-1 comparisons). Overall, the \\textbf{GPT-4 (C)} prompt generally provides win rates more representative of humans; we therefore use this prompt for the main results in Section~\\ref{sec:dpo-real-datasets}. For additional details about the human study, including the web interface presented to raters and the list of human volunteers, see Appendix~\\ref{app:human-study}.\n",
      "\n",
      "\\section{Discussion}\n",
      "Learning from preferences is a powerful, scalable framework for training capable, aligned language models. We have introduced DPO, a simple training paradigm for training language models from preferences without reinforcement learning. Rather than coercing the preference learning problem into a standard RL setting in order to use off-the-shelf RL algorithms, DPO identifies a mapping between language model policies and reward functions that enables training a language model to satisfy human preferences \\textit{directly}, with a simple cross-entropy loss, without reinforcement learning or loss of generality. With virtually no tuning of hyperparameters, DPO performs similarly or better than existing RLHF algorithms, including those based on PPO; DPO thus meaningfully reduces the barrier to training more language models from human preferences.\n",
      "\n",
      "\\textbf{Limitations \\& Future Work.} Our results raise several important questions for future work. How does the DPO policy generalize out of distribution, compared with learning from an explicit reward function? Our initial results suggest that DPO policies can generalize similarly to PPO-based models, but more comprehensive study is needed. For example, can training with self-labeling from the DPO policy similarly make effective use of unlabeled prompts? On another front, how does reward over-optimization manifest in the direct preference optimization setting, and is the slight decrease in performance in Figure~\\ref{fig:dialogue-main}-right an instance of it? Additionally, while we evaluate models up to 6B parameters, exploration of scaling DPO to state-of-the-art models orders of magnitude larger is an exciting direction for future work. Regarding evaluations, we find that the win rates computed by GPT-4 are impacted by the prompt; future work may study the best way to elicit high-quality judgments from automated systems. Finally, many possible applications of DPO exist beyond training language models from human preferences, including training generative models in other modalities.\n",
      "\n",
      "\\end{document}\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploration; making sure the function works\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import utils.utils as utils\n",
    "\n",
    "part = cleaned_text.split('\\n\\n')[2]\n",
    "utils.query_llm(\n",
    "            prompt=f\"Paraphrase the following text, while keeping the latex formatting when it appears. Try to paraphrase the text as much as possible. Text: {part}\\n\\nParaphrased text: \",\n",
    "            model=\"gpt-4.1\", \n",
    "            temperature=1,\n",
    "            top_p=0.9,\n",
    "            system_prompt_included=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add .. path \n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import utils.utils as utils\n",
    "\n",
    "prompt = {}\n",
    "prompt['system'] = \"\"\"You are a helpful assistant that paraphrases text. \n",
    "# Instructions\n",
    "- Paraphrase while keeping latex formatting. \n",
    "- Paraphrase the text as much as possible, while keeping the meaning of the text.\n",
    "- Leave proper nouns, equations, and domain-specific terminology unchanged.\n",
    "- Only output the paraphrased text, no other text. \"\"\"\n",
    "\n",
    "def paraphrase_paragraph(part):\n",
    "    if len(part) < 200:\n",
    "        return part\n",
    "    else:\n",
    "        prompt['user'] = f\"Text: {part}\"\n",
    "        return utils.query_llm(\n",
    "            prompt=prompt,\n",
    "            model=\"gpt-4.1\", \n",
    "            temperature=1,\n",
    "            top_p=0.95,\n",
    "            system_prompt_included=True,\n",
    "        )\n",
    "\n",
    "def paraphrase_text(text):\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    # Parallelize the API calls\n",
    "    with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "        paraphrased_paragraphs = list(tqdm(\n",
    "            executor.map(paraphrase_paragraph, paragraphs),\n",
    "            total=len(paragraphs)\n",
    "        ))\n",
    "    return '\\n\\n'.join(paraphrased_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:16<00:00,  2.66it/s]\n",
      "100%|██████████| 45/45 [00:12<00:00,  3.49it/s]\n",
      "100%|██████████| 45/45 [00:18<00:00,  2.42it/s]\n",
      "100%|██████████| 45/45 [00:28<00:00,  1.59it/s]\n",
      "100%|██████████| 45/45 [00:21<00:00,  2.11it/s]\n",
      "100%|██████████| 45/45 [00:15<00:00,  2.96it/s]\n",
      "100%|██████████| 45/45 [00:18<00:00,  2.43it/s]\n",
      "100%|██████████| 45/45 [00:33<00:00,  1.33it/s]\n",
      "100%|██████████| 45/45 [00:30<00:00,  1.48it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    with open(f'../data/arxiv/cleaned_DPO_paraphrased_{i}.txt', 'w') as f:\n",
    "        paraphrased_text = paraphrase_text(cleaned_paper)\n",
    "        f.write(paraphrased_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the paraphrased texts as a list\n",
    "paraphrased_texts = []\n",
    "for i in range(9):\n",
    "    with open(f'../data/arxiv/cleaned_DPO_paraphrased_{i}.txt', 'r') as f:\n",
    "        paraphrased_texts.append(f.read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's paraphrase as much as we can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks.\n",
      "The empirical gains can be striking.\n",
      "For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.\n",
      "\\end{abstract}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Testing across three major language models demonstrates that chain-of-thought prompting enhances results on various tasks involving arithmetic, common sense, and symbolic reasoning. These improvements can be quite significant. For example, by supplying PaLM 540B with only eight chain-of-thought examples, the model attains top-tier accuracy on the GSM8K math word problem benchmark, outperforming even a finetuned GPT-3 that uses a verifier.'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Exploration; making sure the function works\n",
    "# For instance, if the original paragraph is 'Sally went to the store and bought a new dress because she was sad.', the paraphrased text should be 'Sally was sad. She went to the store and bought a new dress.'. \n",
    "part = cleaned_text.split('\\n\\n')[2]\n",
    "print(part)\n",
    "prompt = {}\n",
    "prompt['system'] = \"\"\"You are a helpful assistant that paraphrases text. \n",
    "# Instructions\n",
    "- Read the text to understand what it's trying to say.\n",
    "- Paraphrase the text as much as possible, while keeping the meaning of the text. \n",
    "- Paraphrase while keeping latex formatting. \n",
    "- Make sure that the syntax, sentence structure, and diction is completely different.\n",
    "- Ensure the writing is written well.\n",
    "- Only output the paraphrased text, no other text.\"\"\"\n",
    "prompt['user'] = f\"Text: {part}\\n\\n\"\n",
    "\n",
    "utils.query_llm(\n",
    "            prompt=prompt,\n",
    "            model=\"gpt-4.1\", \n",
    "            temperature=1,\n",
    "            top_p=0.95,\n",
    "            system_prompt_included=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\section{Chain-of-Thought Prompting}\n",
      "Consider one's own thought process when solving a complicated reasoning task such as a multi-step math word problem. \n",
      "It is typical to decompose the problem into intermediate steps and solve each before giving the final answer: \\textit{``After Jane gives 2 flowers to her mom she has 10 $\\ldots$ then after she gives 3 to her dad she will have 7 $\\ldots$ so the answer is 7.''}\n",
      "The goal of this paper is to endow language models with the ability to generate a similar \\textit{chain of thought}---a coherent series of intermediate reasoning steps that lead to the final answer for a problem.\n",
      "We will show that sufficiently large language models can generate chains of thought if demonstrations of chain-of-thought reasoning are provided in the exemplars for few-shot prompting.\n",
      "\n",
      "\\cref{fig:pull-figure} shows an example of a model producing a chain of thought to solve a math word problem that it would have otherwise gotten incorrect.\n",
      "The chain of thought in this case resembles a solution and can interpreted as one, but we still opt to call it a chain of thought to better capture the idea that it mimics a step-by-step thought process for arriving at the answer (and also, solutions/explanations typically come \\textit{after} the final answer \\citep[][\\textit{inter alia}]{narang2020wt5,wiegreffe2021reframing,lampinen2022can}).\n",
      "\n",
      "Chain-of-thought prompting has several attractive properties as an approach for facilitating reasoning in language models.\n",
      "\\begin{enumerate}[topsep=1pt,itemsep=0ex]%\n",
      "    \\item First, chain of thought, in principle, allows models to decompose multi-step problems into intermediate steps, which means that additional computation can be allocated to problems that require more reasoning steps.\n",
      "    \\item Second, a chain of thought provides an interpretable window into the behavior of the model, suggesting how it might have arrived at a particular answer and providing opportunities to debug where the reasoning path went wrong (although fully characterizing a model's computations that support an answer remains an open question).\n",
      "    \\item Third, chain-of-thought reasoning can be used for tasks such as math word problems, commonsense reasoning, and symbolic manipulation, and is potentially applicable (at least in principle) to any task that humans can solve via language.\n",
      "    \\item Finally, chain-of-thought reasoning can be readily elicited in sufficiently large off-the-shelf language models simply by including examples of chain of thought sequences into the exemplars of few-shot prompting.\n",
      "\\end{enumerate}\n",
      "\n",
      "In empirical experiments, we will observe the utility of chain-of-thought prompting for arithmetic reasoning (\\cref{sec:arithmetic-reasoning}), commonsense reasoning (\\cref{sec:commonsense-reasoning}), and symbolic reasoning (\\cref{sec:symbolic-reasoning}).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Exploration; making sure the function works\n",
    "# For instance, if the original paragraph is 'Sally went to the store and bought a new dress because she was sad.', the paraphrased text should be 'Sally was sad. She went to the store and bought a new dress.'. \n",
    "part = '\\section' + cleaned_text.split('\\section')[2]\n",
    "print(part)\n",
    "prompt = {}\n",
    "prompt['system'] = \"\"\"You are a helpful assistant that paraphrases text. \n",
    "# Instructions\n",
    "- Read the text to understand what it's trying to say.\n",
    "- Paraphrase the text as much as possible, while keeping the meaning of the text. \n",
    "- Don't just paraphrase sentence by sentence, but rather take the freedom to paraphrase the text as a whole. \n",
    "- Paraphrase while keeping latex formatting. \n",
    "- Leave proper nouns, equations, and domain-specific terminology unchanged.\n",
    "- Ensure the writing is clear and written well.\n",
    "- Only output the paraphrased text, no other text.\"\"\"\n",
    "prompt['user'] = f\"Text: {part}\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\section{Chain-of-Thought Prompting}\n",
      "When approaching complex reasoning tasks—such as multi-step math word problems—people often naturally break the problem down into smaller parts, working through each one before arriving at a final solution. For example: \\textit{``After Jane gives 2 flowers to her mom she has 10 $\\ldots$ then after she gives 3 to her dad she will have 7 $\\ldots$ so the answer is 7.''}\n",
      "This work aims to equip language models with the capacity to generate a similar \\textit{chain of thought}—that is, a logically connected sequence of intermediate reasoning steps that ultimately lead to the correct answer.\n",
      "\n",
      "We demonstrate that large language models are able to produce chains of thought when they are given examples that explicitly model this reasoning process in few-shot prompts.\n",
      "\n",
      "As illustrated in \\cref{fig:pull-figure}, providing a chain of thought enables a model to solve a math word problem that it might otherwise answer incorrectly. These reasoning steps closely resemble a worked-out solution, but we refer to them as chains of thought to emphasize that they capture a process of incremental reasoning toward an answer, rather than a post-hoc solution or explanation (which traditionally follows the answer \\citep[][\\textit{inter alia}]{narang2020wt5,wiegreffe2021reframing,lampinen2022can}).\n",
      "\n",
      "Chain-of-thought prompting offers several advantages in promoting reasoning within language models:\n",
      "\\begin{enumerate}[topsep=1pt,itemsep=0ex]%\n",
      "    \\item It enables the model to break down multi-step problems, dedicating more computation to cases that need extended reasoning.\n",
      "    \\item It makes the model's reasoning more transparent, revealing the intermediate steps that led to the answer and providing a pathway for diagnosing reasoning errors, even though fully understanding the model’s internal workings is still unresolved.\n",
      "    \\item This method applies broadly—not just to math problems, but also to commonsense reasoning, symbolic manipulation, and potentially to any language-based problem that humans solve through reasoning.\n",
      "    \\item Moreover, such reasoning sequences can be triggered in large pre-trained language models simply by including chain-of-thought examples in few-shot prompts.\n",
      "\\end{enumerate}\n",
      "\n",
      "Through empirical analysis, we examine the effectiveness of chain-of-thought prompting on tasks including arithmetic reasoning (\\cref{sec:arithmetic-reasoning}), commonsense reasoning (\\cref{sec:commonsense-reasoning}), and symbolic reasoning (\\cref{sec:symbolic-reasoning}).\n"
     ]
    }
   ],
   "source": [
    "print(utils.query_llm(\n",
    "            prompt=prompt,\n",
    "            model=\"gpt-4.1\", \n",
    "            temperature=1,\n",
    "            top_p=0.95,\n",
    "            system_prompt_included=True,\n",
    "        ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
