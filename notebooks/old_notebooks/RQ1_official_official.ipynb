{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "305602af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/unsloth/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjiosephlee\u001b[0m (\u001b[33mupenn-ml\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "2025-04-16 20:15:03,682 - INFO - Loading base model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.254 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:07<00:00,  1.23s/it]\n",
      "2025-04-16 20:15:17,652 - INFO - Base model loaded.\n",
      "2025-04-16 20:15:17,653 - INFO - Adding LoRA adapters...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading input_embeddings to disk to save VRAM\n",
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.19 patched 80 layers with 80 QKV layers, 80 O layers and 80 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Training embed_tokens in mixed precision to save VRAM\n",
      "Unsloth: Training lm_head in mixed precision to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-16 20:16:02,445 - INFO - LoRA adapters added.\n",
      "2025-04-16 20:16:02,446 - INFO - Loading dataset...\n",
      "2025-04-16 20:16:03,675 - INFO - Dataset loaded with 25 examples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['pubid', 'question', 'context', 'long_answer', 'final_decision'],\n",
      "    num_rows: 211269\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel, UnslothTrainer, UnslothTrainingArguments, is_bfloat16_supported\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy # Needed for deep copying state dict\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from importlib import reload\n",
    "import utils.utils as utils\n",
    "import utils.prompts as prompts\n",
    "from utils.keys import WANDB_API_KEY\n",
    "reload(utils)\n",
    "reload(prompts)\n",
    "\n",
    "# Track experiment\n",
    "import wandb\n",
    "wandb.login(key=WANDB_API_KEY) \n",
    "os.environ[\"WANDB_PROJECT\"]=\"Fine-Tuning-or-Retrieval\"\n",
    "\n",
    "# --- Logging Setup ---\n",
    "LOGS_DIR = \"logs\"\n",
    "LOG_FILE = os.path.join(LOGS_DIR, \"experiment.log\")\n",
    "\n",
    "# Create logs directory if it doesn't exist\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)\n",
    "\n",
    "# Configure logging\n",
    "import logging # Add logging import\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG, # Capture debug messages and above\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_FILE, mode='w'), # Write to file (overwrite mode)\n",
    "        logging.StreamHandler() # Write to console\n",
    "    ]\n",
    ")\n",
    "# Set console handler level to INFO to reduce console verbosity\n",
    "logging.getLogger().handlers[1].setLevel(logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Model Configuration ---\n",
    "max_seq_length = 2048\n",
    "dtype = None\n",
    "load_in_4bit = True\n",
    "model_name = \"unsloth/Meta-Llama-3.1-70B\" # Or your preferred model\n",
    "original_seed = 3407 # Define the base seed\n",
    "\n",
    "logger.info(\"Loading base model...\") # Replace print\n",
    "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "logger.info(\"Base model loaded.\") # Replace print\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "if tokenizer.pad_token is None:\n",
    "    logger.info(\"Setting pad token to EOS token.\") # Replace print\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "logger.info(\"Adding LoRA adapters...\") # Replace print\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    base_model,\n",
    "    r=128,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\", \"embed_tokens\", \"lm_head\",],\n",
    "    lora_alpha=128,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=original_seed, # Use base seed here\n",
    "    use_rslora=True,\n",
    "    loftq_config=None,\n",
    ")\n",
    "logger.info(\"LoRA adapters added.\") # Replace print\n",
    "\n",
    "# --- Dataset Loading ---\n",
    "logger.info(\"Loading dataset...\") # Replace print\n",
    "dataset = utils.load_dataset('PubMedQA', split='train', start_index=0, end_index=25) # Use 'train', smaller subset first\n",
    "logger.info(f\"Dataset loaded with {len(dataset)} examples.\") # Replace print\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def format_pretraining_text(context_list):\n",
    "    \"\"\"Formats context list into a single string for pre-training.\"\"\"\n",
    "    return \"\\n\".join(context_list) + EOS_TOKEN\n",
    "\n",
    "def format_qa_prompt(background, question):\n",
    "    \"\"\"Formats background and question into the Yes/No prompt.\"\"\"\n",
    "    return f\"Background: {background}\\n\\nQuestion: {question}\\n\\nPlease answer with Yes or No.\" # EOS is handled by generation\n",
    "\n",
    "def parse_yes_no(text):\n",
    "    \"\"\"Parses generated text to extract 'yes' or 'no'.\"\"\"\n",
    "    text_lower = text.lower().strip()\n",
    "    # More robust parsing\n",
    "    if re.search(r\"^\\s*yes\", text_lower):\n",
    "        return \"yes\"\n",
    "    elif re.search(r\"^\\s*no\", text_lower):\n",
    "        return \"no\"\n",
    "    # Fallback if not at the beginning\n",
    "    elif \"yes\" in text_lower:\n",
    "        return \"yes\"\n",
    "    elif \"no\" in text_lower:\n",
    "        return \"no\"\n",
    "    return \"unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c054247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_question(model, tokenizer, qa_prompt_text):\n",
    "    \"\"\"Generates an answer for the QA prompt and parses Yes/No.\"\"\"\n",
    "    logger.debug(f\"Evaluating QA prompt: {qa_prompt_text[:100]}...\") # Debug log\n",
    "    FastLanguageModel.for_inference(model) # <<< Enable fast inference\n",
    "    # model.eval() # Trainer should handle this\n",
    "    # Note: No EOS token added in format_qa_prompt now, let generation handle it\n",
    "    inputs = tokenizer(qa_prompt_text,\n",
    "                       return_tensors=\"pt\",\n",
    "    ).to('cuda')\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        output_scores=True,                # â† ask for logits/scores\n",
    "        use_cache=True,\n",
    "        return_dict_in_generate=True       # â† get a dict, not just a tensor\n",
    "    )\n",
    "        # The generated token IDs\n",
    "    generated_ids = outputs.sequences\n",
    "\n",
    "    # outputs.scores is a list of length num_generated_tokens,\n",
    "    # each element is a (batch_size, vocab_size) tensor of logits for that step.\n",
    "    # e.g. logits_step1 = outputs.scores[0]\n",
    "    step_logits = torch.stack(outputs.scores, dim=1)  # shape: (batch, seq_len, vocab)\n",
    "    # Decode only the generated part\n",
    "    # Input length: inputs['input_ids'].shape[1]\n",
    "    prediction_text = tokenizer.batch_decode(generated_ids)[0]\n",
    "    logger.debug(f\"Raw prediction: '{prediction_text}'\") # Debug log raw output\n",
    "    parsed_answer = parse_yes_no(prediction_text)\n",
    "    logger.debug(f\"Parsed answer: '{parsed_answer}'\") # Debug log parsed output\n",
    "    # Optional: Put model back into training mode if needed outside this function\n",
    "    # model.train() # Typically trainer handles this before training step\n",
    "    return parsed_answer, step_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5477f695",
   "metadata": {},
   "outputs": [],
   "source": [
    "results= []\n",
    "# --- Dataset Loading ---\n",
    "logger.info(\"Loading dataset...\") # Replace print\n",
    "dataset = utils.load_dataset('PubMedQA', split='test', start_index=0, end_index=1000) # Use 'train', smaller subset first\n",
    "logger.info(f\"Dataset loaded with {len(dataset)} examples.\") # Replace print\n",
    "\n",
    "for idx, example in enumerate(tqdm(dataset, desc=\"Processing Questions\")):\n",
    "    question_id = example.get('id', f'idx_{idx}')\n",
    "    true_answer = example['final_decision'].lower()\n",
    "    question_text = example['question']\n",
    "    contexts = example['context']['contexts'] # Ensure 'contexts' exists\n",
    "    background_text = \"\\n\".join(contexts)\n",
    "\n",
    "    logger.debug(f\"Processing Question ID: {question_id}\") # Debug log\n",
    "\n",
    "    result = {\n",
    "        'id': question_id,\n",
    "        'question': question_text,\n",
    "        'true_answer': true_answer,\n",
    "        'prediction': None,\n",
    "        'logits': None,\n",
    "    }\n",
    "\n",
    "    qa_prompt_text = format_qa_prompt(background_text, question_text)\n",
    "    result['prediction'], result['logits'] = evaluate_question(base_model, tokenizer, qa_prompt_text)\n",
    "    results.append(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3f6c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Extract true answers and predictions from the results list\n",
    "true_answers = [result['true_answer'] for result in results]\n",
    "predictions = [result['prediction'] for result in results]\n",
    "\n",
    "# Compute the accuracy score\n",
    "accuracy = accuracy_score(true_answers, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d08a9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the indices where the prediction does NOT match the true answer.\n",
    "wrong_indices = [i for i, result in enumerate(results) \n",
    "                 if result['prediction'] != result['true_answer']]\n",
    "\n",
    "# Log the number of wrong predictions\n",
    "print(f\"Number of mispredicted examples: {len(wrong_indices)}\")\n",
    "\n",
    "# Create a list of examples from the original dataset using the filtered indices.\n",
    "# Here we assume `dataset` is your original Hugging Face dataset.\n",
    "wrong_examples = [dataset[i] for i in wrong_indices]\n",
    "\n",
    "# Create a new Hugging Face dataset from the filtered list.\n",
    "wrong_dataset = Dataset.from_list(wrong_examples)\n",
    "\n",
    "# Display the dataset (it should have the same features as the original)\n",
    "print(wrong_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdaa94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_examples = [dataset[i] for i in wrong_indices[:50]]\n",
    "\n",
    "# Create a new Hugging Face dataset from the filtered list.\n",
    "wrong_dataset = Dataset.from_list(wrong_examples)\n",
    "\n",
    "# Display the dataset (it should have the same features as the original)\n",
    "print(wrong_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5454f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Experiment Setup ---\n",
    "num_finetune_epochs_per_question = 3\n",
    "results = []\n",
    "\n",
    "# --- Main Experiment Loop ---\n",
    "logger.info(\"Starting experiment loop...\") # Replace print\n",
    "for idx, example in enumerate(tqdm(wrong_dataset, desc=\"Processing Questions\")):\n",
    "    question_id = example.get('id', f'idx_{idx}')\n",
    "    true_answer = example['final_decision'].lower()\n",
    "    question_text = example['question']\n",
    "    contexts = example['context']['contexts'] # Ensure 'contexts' exists\n",
    "    background_text = \"\\n\".join(contexts)\n",
    "\n",
    "    logger.debug(f\"Processing Question ID: {question_id}\") # Debug log\n",
    "\n",
    "    current_results = {\n",
    "        'id': question_id,\n",
    "        'question': question_text,\n",
    "        'true_answer': true_answer,\n",
    "        'predictions': {}\n",
    "    }\n",
    "\n",
    "    qa_prompt_text = format_qa_prompt(background_text, question_text)\n",
    "\n",
    "    # 1. Pre-Tune Evaluation\n",
    "    logger.debug(f\"[{question_id}] Resetting model and performing pre-tune evaluation.\") # Debug log\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        base_model,\n",
    "        r=128,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                        \"gate_proj\", \"up_proj\", \"down_proj\", \"embed_tokens\", \"lm_head\",],\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "        random_state=original_seed, # Use base seed here\n",
    "        use_rslora=True,\n",
    "        loftq_config=None,\n",
    "        ).to('cuda')\n",
    "    pre_train_pred = evaluate_question(model, tokenizer, qa_prompt_text)\n",
    "    current_results['predictions']['pre_train'] = pre_train_pred\n",
    "    logger.debug(f\"[{question_id}] Pre-train Prediction: {pre_train_pred} (True: {true_answer})\") # Debug log\n",
    "\n",
    "    # 2. Prepare Context Data\n",
    "    logger.debug(f\"[{question_id}] Preparing context data for tuning.\") # Debug log\n",
    "    context_for_tuning = format_pretraining_text(contexts)\n",
    "    tuning_data = Dataset.from_dict({\"text\": [context_for_tuning]})\n",
    "    # Configure trainer - Use original args where possible\n",
    "    temp_output_dir = f\"./outputs_temp_{question_id}\"\n",
    "\n",
    "    # Define arguments, keeping originals where feasible\n",
    "    args = UnslothTrainingArguments(\n",
    "        # --- Args to keep from original (potentially) ---\n",
    "        warmup_ratio = 0.1,           # Original: 0.1\n",
    "        learning_rate = 5e-5,         # Original: 5e-5\n",
    "        embedding_learning_rate = 5e-6, # Original: 5e-6 (can include if needed)\n",
    "        optim = \"adamw_8bit\",         # Original: adamw_8bit\n",
    "        weight_decay = 0.00,          # Original: 0.00\n",
    "        lr_scheduler_type = \"cosine\", # Original: cosine (though effect minimal for 1 step)\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        # --- Args specific to this step ---\n",
    "        per_device_train_batch_size = 1, # MUST be 1 for single example\n",
    "        gradient_accumulation_steps = 1, # MUST be 1 for single step update\n",
    "        num_train_epochs = 4,          # MUST be 1 for single step update\n",
    "        logging_steps = 1,            # Adjust logging frequency if desired (original was 1)\n",
    "        seed = original_seed ,  # Vary seed per step\n",
    "        output_dir = temp_output_dir,  # Temporary output\n",
    "        report_to = \"none\" if \"WANDB_PROJECT\" in os.environ else \"none\", # Report to wandb if configured\n",
    "        save_strategy = \"no\",          # Disable saving checkpoints\n",
    "        # save_steps = 5,\n",
    "    )\n",
    "\n",
    "    trainer = UnslothTrainer(\n",
    "        model=model, # Pass current model state\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=tuning_data,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        dataset_num_proc=1,\n",
    "        args=args, # Use the defined args\n",
    "    )\n",
    "    # 3. Iterative Fine-Tuning & Evaluation Loop\n",
    "    for epoch in range(1, num_finetune_epochs_per_question + 1):\n",
    "        logger.debug(f\"[{question_id}] Starting fine-tuning epoch {epoch}/{num_finetune_epochs_per_question}.\") # Debug log\n",
    "        # Fine-tune for one step\n",
    "        logger.debug(f\"[{question_id}][Epoch {epoch}] Starting trainer.train()\") # Debug log\n",
    "        # Trainer should handle model.train() / model.eval() transitions\n",
    "        if epoch > 1:\n",
    "            print(f\"Starting epoch {epoch}....\")\n",
    "            args = UnslothTrainingArguments(\n",
    "                    # --- Args to keep from original (potentially) ---\n",
    "                    warmup_ratio = 0.0,           # Original: 0.1\n",
    "                    learning_rate = 5e-5,         # Original: 5e-5\n",
    "                    embedding_learning_rate = 5e-6, # Original: 5e-6 (can include if needed)\n",
    "                    optim = \"adamw_8bit\",         # Original: adamw_8bit\n",
    "                    weight_decay = 0.00,          # Original: 0.00\n",
    "                    lr_scheduler_type = \"cosine\", # Original: cosine (though effect minimal for 1 step)\n",
    "                    fp16 = not is_bfloat16_supported(),\n",
    "                    bf16 = is_bfloat16_supported(),\n",
    "                    # --- Args specific to this step ---\n",
    "                    per_device_train_batch_size = 1, # MUST be 1 for single example\n",
    "                    gradient_accumulation_steps = 1, # MUST be 1 for single step update\n",
    "                    num_train_epochs = 3,          # MUST be 1 for single step update\n",
    "                    logging_steps = 1,            # Adjust logging frequency if desired (original was 1)\n",
    "                    seed = original_seed ,  # Vary seed per step\n",
    "                    output_dir = temp_output_dir,  # Temporary output\n",
    "                    report_to = \"none\" if \"WANDB_PROJECT\" in os.environ else \"none\", # Report to wandb if configured\n",
    "                    save_strategy = \"no\",          # Disable saving checkpoints\n",
    "                    # save_steps = 5,\n",
    "                )\n",
    "\n",
    "            trainer = UnslothTrainer(\n",
    "                model=model, # Pass current model state\n",
    "                tokenizer=tokenizer,\n",
    "                train_dataset=tuning_data,\n",
    "                dataset_text_field=\"text\",\n",
    "                max_seq_length=max_seq_length,\n",
    "                dataset_num_proc=1,\n",
    "                args=args, # Use the defined args\n",
    "            )\n",
    "            trainer.train()\n",
    "        else:\n",
    "            train_result = trainer.train()\n",
    "\n",
    "        # Check if training_loss is available\n",
    "        training_loss = train_result.training_loss if hasattr(train_result, 'training_loss') else \"N/A\"\n",
    "        logger.debug(f\"[{question_id}][Epoch {epoch}] Training finished. Loss: {training_loss}\") # Debug log loss\n",
    "\n",
    "        # Evaluate on the question *after* this epoch\n",
    "        logger.debug(f\"[{question_id}][Epoch {epoch}] Evaluating question post-tuning.\") # Debug log\n",
    "        epoch_pred = evaluate_question(model, tokenizer, qa_prompt_text)\n",
    "        current_results['predictions'][f'epoch_{epoch}'] = epoch_pred\n",
    "        logger.debug(f\"[{question_id}][Epoch {epoch}] Prediction: {epoch_pred} (True: {true_answer})\") # Debug log\n",
    "\n",
    "    # Optional cleanup\n",
    "    import shutil\n",
    "    if os.path.exists(temp_output_dir):\n",
    "            logger.debug(f\"[{question_id}][Epoch {epoch}] Cleaning up temporary directory: {temp_output_dir}\") # Debug log\n",
    "            shutil.rmtree(temp_output_dir)\n",
    "\n",
    "    results.append(current_results)\n",
    "    logger.debug(f\"Finished processing Question ID: {question_id}\") # Debug log\n",
    "\n",
    "logger.info(\"Experiment loop finished.\") # Replace print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4776e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dca961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analysis ---\n",
    "logger.info(\"--- Analyzing Results ---\") # Replace print\n",
    "if not results:\n",
    "    logger.warning(\"No results collected.\") # Use warning level\n",
    "else:\n",
    "    df = pd.DataFrame(results)\n",
    "    try:\n",
    "        predictions_df = pd.json_normalize(df['predictions'])\n",
    "        analysis_df = pd.concat([df[['id', 'question', 'true_answer']], predictions_df], axis=1)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing results into DataFrame: {e}\") # Log error\n",
    "        analysis_df = pd.DataFrame() # Create empty df to avoid further errors\n",
    "\n",
    "    if not analysis_df.empty:\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        accuracies = {}\n",
    "        stages = ['pre_train'] + [f'epoch_{e}' for e in range(1, num_finetune_epochs_per_question + 1)]\n",
    "\n",
    "        for stage in stages:\n",
    "            if stage in analysis_df.columns:\n",
    "                valid_preds_mask = analysis_df[stage] != 'unknown'\n",
    "                # Ensure true_answer column exists and has data before calculating accuracy\n",
    "                if 'true_answer' in analysis_df.columns and not analysis_df['true_answer'].isnull().all():\n",
    "                    accuracy = accuracy_score(\n",
    "                        analysis_df.loc[valid_preds_mask, 'true_answer'],\n",
    "                        analysis_df.loc[valid_preds_mask, stage]\n",
    "                    ) if valid_preds_mask.sum() > 0 else 0.0 # Handle case with zero valid preds\n",
    "                else:\n",
    "                    accuracy = 0.0 # Cannot calculate accuracy if true answers are missing\n",
    "                    logger.warning(f\"Cannot calculate accuracy for stage '{stage}' due to missing true answers.\")\n",
    "\n",
    "                num_unknown = len(analysis_df) - valid_preds_mask.sum()\n",
    "                accuracies[stage] = (accuracy, num_unknown)\n",
    "            else:\n",
    "                logger.warning(f\"Stage '{stage}' not found in results columns.\") # Log warning\n",
    "                accuracies[stage] = (0.0, len(analysis_df))\n",
    "\n",
    "        logger.info(f\"Processed {len(df)} questions.\") # Replace print\n",
    "        logger.info(\"Accuracies (Ignoring 'unknown' predictions):\") # Replace print\n",
    "        for stage, (acc, unknown_count) in accuracies.items():\n",
    "            total_count = len(analysis_df)\n",
    "            valid_count = total_count - unknown_count\n",
    "            logger.info(f\"- {stage}: {acc:.4f} ({valid_count}/{total_count} valid predictions, {unknown_count} unknown)\") # Replace print\n",
    "\n",
    "        output_filename = \"rq1_experiment_results.csv\"\n",
    "        try:\n",
    "            analysis_df.to_csv(output_filename, index=False)\n",
    "            logger.info(f\"Detailed results saved to {output_filename}\") # Replace print\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save results to CSV: {e}\") # Log error\n",
    "    else:\n",
    "        logger.error(\"Analysis DataFrame is empty, skipping accuracy calculation and saving.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
