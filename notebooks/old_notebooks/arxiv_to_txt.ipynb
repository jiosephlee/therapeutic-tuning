{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load arxiv text and clean it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Introduction\n",
      "\n",
      "Large unsupervised language models (LMs) trained on very large datasets\n",
      "acquire surprising\n",
      "capabilities [@chowdhery2022palm; @brown2020language; @touvron2023llama; @bubeck2023sparks].\n",
      "However, these models are trained on data generated by humans with a\n",
      "wide variety of goals, priorities, and skillsets. Some of these goals\n",
      "and skillsets may not be desirable to imitate; for example, while we may\n",
      "want our AI coding assistant to *understand* common programming mistakes\n",
      "in order to correct them, nevertheless, when generating code, we would\n",
      "like to bias our model toward the (potentially rare) high-quality coding\n",
      "ability present in its training data. Similarly, we might want our\n",
      "language model to be *aware* of a common misconception believed by 50%\n",
      "of people, but we certainly do not want the model to claim this\n",
      "misconception to be true in 50% of queries about it! In other words,\n",
      "selecting the model's *desired responses and behavior* from its very\n",
      "wide *knowledge and abilities* is crucial to building AI systems that\n",
      "are safe, performant, and controllable [@ouyang2022training]. While\n",
      "existing methods typically steer LMs to match human preferences using\n",
      "reinforcement learning (RL), we will show that the RL-based objective\n",
      "used by existing methods can be optimized exactly with a simple binary\n",
      "cross-entropy objective, greatly simplifying the preference learning\n",
      "pipeline.\n",
      "\n",
      "![ **optimizes for human preferences while avoiding reinforcement\n",
      "learning.** Existing methods for fine-tuning language models with human\n",
      "feedback first fit a reward model to a dataset of prompts and human\n",
      "preferences over pairs of responses, and then use RL to find a policy\n",
      "that maximizes the learned reward. In contrast, directly optimizes for\n",
      "the policy best satisfying the preferences with a simple classification\n",
      "objective, fitting an *implicit* reward model whose corresponding\n",
      "optimal policy can be extracted in closed\n",
      "form.](figures/diagrams/teaser.png){#fig:teaser width=\"99.9%\"}\n",
      "\n",
      "[]{#fig:teaser label=\"fig:teaser\"}\n",
      "\n",
      "At a high level, existing methods instill the desired behaviors into a\n",
      "language model using curated sets of human preferences representing the\n",
      "types of behaviors that humans find safe and helpful. This preference\n",
      "learning stage occurs after an initial stage of large-scale unsupervised\n",
      "pre-training on a large text dataset. While the most straightforward\n",
      "approach to preference learning is supervised fine-tuning on human\n",
      "demonstrations of high quality responses, the most successful class of\n",
      "methods is reinforcement learning from human (or AI) feedback\n",
      "(RLHF/RLAIF; [@christiano2017deep; @bai2022constitutional]). RLHF\n",
      "methods fit a reward model to a dataset of human preferences and then\n",
      "use RL to optimize a language model policy to produce responses assigned\n",
      "high reward without drifting excessively far from the original model.\n",
      "While RLHF produces models with impressive conversational and coding\n",
      "abilities, the RLHF pipeline is considerably more complex than\n",
      "supervised learning, involving training multiple LMs and sampling from\n",
      "the LM policy in the loop of training, incurring significant\n",
      "computational costs.\n",
      "\n",
      "In this paper, we show how to directly optimize a language model to\n",
      "adhere to human preferences, without explicit reward modeling or\n",
      "reinforcement learning. We propose *()*, an algorithm that implicitly\n",
      "optimizes the same objective as existing RLHF algorithms (reward\n",
      "maximization with a KL-divergence constraint) but is simple to implement\n",
      "and straightforward to train. Intuitively, the update increases the\n",
      "relative log probability of preferred to dispreferred responses, but it\n",
      "incorporates a dynamic, per-example importance weight that prevents the\n",
      "model degeneration that we find occurs with a naive probability ratio\n",
      "objective. Like existing algorithms, relies on a theoretical preference\n",
      "model (such as the Bradley-Terry model; [@bradley1952rankanalysis]) that\n",
      "measures how well a given reward function aligns with empirical\n",
      "preference data. However, while existing methods use the preference\n",
      "model to define a preference loss to train a reward model and then train\n",
      "a policy that optimizes the learned reward model, uses a change of\n",
      "variables to define the preference loss as a function of the policy\n",
      "directly. Given a dataset of human preferences over model responses, can\n",
      "therefore optimize a policy using a simple binary cross entropy\n",
      "objective, producing the optimal policy to an implicit reward function\n",
      "fit to the preference data.\n",
      "\n",
      "Our main contribution is (), a simple RL-free algorithm for training\n",
      "language models from preferences. Our experiments show that is at least\n",
      "as effective as existing methods, including PPO-based RLHF, for learning\n",
      "from preferences in tasks such as sentiment modulation, summarization,\n",
      "and dialogue, using language models with up to 6B parameters.\n",
      "\n",
      "# Related Work\n",
      "\n",
      "Self-supervised language models of increasing scale learn to complete\n",
      "some tasks zero-shot [@radford2019language] or with few-shot prompts\n",
      "[@gpt3; @megatron; @chowdhery2022palm]. However, their performance on\n",
      "downstream tasks and alignment with user intent can be significantly\n",
      "improved by fine-tuning on datasets of instructions and human-written\n",
      "completions\n",
      "[@mishra-etal-2022-cross; @sanh2022multitask; @chung2022scaling; @thoppilan2022lamda].\n",
      "This 'instruction-tuning' procedure enables LLMs to generalize to\n",
      "instructions outside of the instruction-tuning set and generally\n",
      "increase their usability [@chung2022scaling]. Despite the success of\n",
      "instruction tuning, *relative* human judgments of response quality are\n",
      "often easier to collect than expert demonstrations, and thus subsequent\n",
      "works have fine-tuned LLMs with datasets of human preferences, improving\n",
      "proficiency in translation [@kreutzer-etal-2018-reliability],\n",
      "summarization [@stiennon2022learning; @ziegler2020finetuning],\n",
      "story-telling [@ziegler2020finetuning], and instruction-following\n",
      "[@ouyang2022training; @ramamurthy2023is]. These methods first optimize a\n",
      "neural network reward function for compatibility with the dataset of\n",
      "preferences under a preference model such as the Bradley-Terry model\n",
      "[@bradley1952rankanalysis], then fine-tune a language model to maximize\n",
      "the given reward using reinforcement learning algorithms, commonly\n",
      "REINFORCE [@williams1992reinforce], proximal policy optimization (PPO;\n",
      "[@schulman2017proximal]), or variants [@ramamurthy2023is]. A\n",
      "closely-related line of work leverages LLMs fine-tuned for instruction\n",
      "following with human feedback to generate additional synthetic\n",
      "preference data for targeted attributes such as safety or harmlessness\n",
      "[@bai2022constitutional], using only weak supervision from humans in the\n",
      "form of a text rubric for the LLM's annotations. These methods represent\n",
      "a convergence of two bodies of work: one body of work on training\n",
      "language models with reinforcement learning for a variety of\n",
      "objectives [@Ranzato2015SequenceLT; @paulus2018a; @wu2018learning] and\n",
      "another body of work on general methods for learning from human\n",
      "preferences [@christiano2017deep; @kupcsik2018learning]. Despite the\n",
      "appeal of using relative human preferences, fine-tuning large language\n",
      "models with reinforcement learning remains a major practical challenge;\n",
      "this work provides a theoretically-justified approach to optimizing\n",
      "relative preferences without RL.\n",
      "\n",
      "Outside of the context of language, learning policies from preferences\n",
      "has been studied in both bandit and reinforcement learning settings, and\n",
      "several approaches have been proposed. Contextual bandit learning using\n",
      "preferences or rankings of actions, rather than rewards, is known as a\n",
      "contextual dueling bandit (CDB; [@yue2012karmed; @dudik2015contextual]).\n",
      "In the absence of absolute rewards, theoretical analysis of CDBs\n",
      "substitutes the notion of an optimal policy with a *von Neumann winner*,\n",
      "a policy whose expected win rate against *any* other policy is at least\n",
      "50% [@dudik2015contextual]. However, in the CDB setting, preference\n",
      "labels are given online, while in learning from human preferences, we\n",
      "typically learn from a fixed batch of offline preference-annotated\n",
      "action pairs [@yan2022human]. Similarly, *preference-based RL* (PbRL)\n",
      "learns from binary preferences generated by an *unknown* 'scoring'\n",
      "function rather than rewards [@BusaFekete2014; @ruiz2023dueling].\n",
      "Various algorithms for PbRL exist, including methods that can reuse\n",
      "off-policy preference data, but generally involve first explicitly\n",
      "estimating the latent scoring function (i.e. the reward model) and\n",
      "subsequently optimizing it\n",
      "[@jain2013learning; @BusaFekete2014; @christiano2017deep; @sadigh2017active; @kupcsik2018learning].\n",
      "We instead present a single stage policy learning approach that directly\n",
      "optimizes a policy to satisfy preferences.\n",
      "\n",
      "# Preliminaries {#section:prelims}\n",
      "\n",
      "We review the RLHF pipeline in @ziegler2020finetuning (and later\n",
      "[@stiennon2022learning; @bai2022training; @ouyang2022training]). It\n",
      "usually includes three phases: 1) supervised fine-tuning (SFT); 2)\n",
      "preference sampling and reward learning and 3) RL optimization.\n",
      "\n",
      "**SFT**: RLHF typically begins by fine-tuning a pre-trained LM with\n",
      "supervised learning on high-quality data for the downstream task(s) of\n",
      "interest (dialogue, summarization, etc.), to obtain a model\n",
      "$\\pi^\\text{SFT}$.\n",
      "\n",
      "**Reward Modelling Phase**: In the second phase the SFT model is\n",
      "prompted with prompts $x$ to produce pairs of answers\n",
      "$(y_1, y_2)\\sim \\pi^\\text{SFT}(y \\mid x)$. These are then presented to\n",
      "human labelers who express preferences for one answer, denoted as\n",
      "$y_w\\succ y_l \\mid x$ where $y_w$ and $y_l$ denotes the preferred and\n",
      "dispreferred completion amongst $(y_1, y_2)$ respectively. The\n",
      "preferences are assumed to be generated by some latent reward model\n",
      "$r^*(y, x)$, which we do not have access to. There are a number of\n",
      "approaches used to model preferences, the Bradley-Terry (BT)\n",
      "[@bradley1952rankanalysis] model being a popular choice (although more\n",
      "general Plackett-Luce ranking models\n",
      "[@plackett1975analysis; @luce2012individual] are also compatible with\n",
      "the framework if we have access to several ranked answers). The BT model\n",
      "stipulates that the human preference distribution $p^*$ can be written\n",
      "as: $$\\label{eq:bradley-terry}\n",
      "    p^*(y_1\\succ y_2 \\mid x)=\\frac{\\exp\\left(r^*(x, y_1)\\right)}{\\exp\\left(r^*(x, y_1)\\right) + \\exp\\left(r^*(x, y_2)\\right)}.$$\n",
      "Assuming access to a static dataset of comparisons\n",
      "$\\mathcal{D}=\\bigl\\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\\bigr\\}_{i=1}^N$\n",
      "sampled from $p^*$, we can parametrize a reward model $r_{\\phi}(x, y)$\n",
      "and estimate the parameters via maximum likelihood. Framing the problem\n",
      "as a binary classification we have the negative log-likelihood loss:\n",
      "$$\\label{eq:reward_model}\n",
      "    \\mathcal{L}_R(r_{\\phi}, \\mathcal{D}) = -\\mathbb{E}_{(x, y_w, y_l)\\sim \\mathcal{D}}\\bigl[\\log \\sigma(r_{\\phi}(x, y_w)- r_{\\phi}(x, y_l))\\bigr]$$\n",
      "where $\\sigma$ is the logistic function. In the context of LMs, the\n",
      "network $r_{\\phi}(x, y)$ is often initialized from the SFT model\n",
      "$\\pi^\\text{SFT}(y \\mid x)$ with the addition of a linear layer on top of\n",
      "the final transformer layer that produces a single scalar prediction for\n",
      "the reward value [@ziegler2020finetuning]. To ensure a reward function\n",
      "with lower variance, prior works normalize the rewards, such that\n",
      "$\\mathbb{E}_{x,y\\sim \\mathcal{D}}\\left[r_\\phi(x, y)\\right] = 0$ for all\n",
      "$x$.\n",
      "\n",
      "**RL Fine-Tuning Phase**: During the RL phase, the learned reward\n",
      "function is used to provide feedback to the language model. Following\n",
      "prior works [@jaques2017sequence; @jaques2020human], the optimization is\n",
      "formulated as $$\\label{eq:RL}\n",
      "\\max_{\\pi_{\\theta}}  \\mathbb{E}_{x\\sim \\mathcal{D}, y\\sim \\pi_{\\theta}(y \\mid x)}\\bigl[r_{\\phi}(x, y)\\bigr] - \\beta\\mathbb{D}_{\\textrm{KL}}\\bigl[\\pi_{\\theta}(y\\mid x)\\mid \\mid \\pi_\\text{ref}(y\\mid x)\\bigr],$$\n",
      "where $\\beta$ is a parameter controlling the deviation from the base\n",
      "reference policy $\\pi_\\text{ref}$, namely the initial SFT model\n",
      "$\\pi^\\text{SFT}$. In practice, the language model policy $\\pi_\\theta$ is\n",
      "also initialized to $\\pi^\\text{SFT}$. The added constraint is important,\n",
      "as it prevents the model from deviating too far from the distribution on\n",
      "which the reward model is accurate, as well as maintaining the\n",
      "generation diversity and preventing mode-collapse to single high-reward\n",
      "answers. Due to the discrete nature of language generation, this\n",
      "objective is not differentiable and is typically optimized with\n",
      "reinforcement learning. The standard approach\n",
      "[@ziegler2020finetuning; @stiennon2022learning; @bai2022training; @ouyang2022training]\n",
      "has been to construct the reward function\n",
      "${r(x, y) = r_{\\phi}(x, y) -\\beta (\\log \\pi_{\\theta}(y\\mid x) - \\log \\pi_\\text{ref}(y\\mid x))}$,\n",
      "and maximize using PPO [@schulman2017proximal].\n",
      "\n",
      "# Direct Preference Optimization {#sec:DPO}\n",
      "\n",
      "Motivated by the challenges of applying reinforcement learning\n",
      "algorithms on large-scale problems such as fine-tuning language models,\n",
      "our goal is to derive a simple approach for policy optimization using\n",
      "preferences directly. Unlike prior RLHF methods, which learn a reward\n",
      "and then optimize it via RL, our approach leverages a particular choice\n",
      "of reward model parameterization that enables extraction of its optimal\n",
      "policy in closed form, without an RL training loop. As we will describe\n",
      "next in detail, our key insight is to leverage an analytical mapping\n",
      "from reward functions to optimal policies, which enables us to transform\n",
      "a loss function over reward functions into a loss function over\n",
      "policies. This change-of-variables approach avoids fitting an explicit,\n",
      "standalone reward model, while still optimizing under existing models of\n",
      "human preferences, such as the Bradley-Terry model. In essence, the\n",
      "policy network represents both the language model and the (implicit)\n",
      "reward.\n",
      "\n",
      "**Deriving the DPO objective.** We start with the same RL objective as\n",
      "prior work, Eq. [\\[eq:RL\\]](#eq:RL){reference-type=\"ref\"\n",
      "reference=\"eq:RL\"}, under a general reward function $r$. Following prior\n",
      "work [@peters2007reinforcement; @peng2019advantage; @korbak2022reinforcement; @go2023aligning],\n",
      "it is straightforward to show that the optimal solution to the\n",
      "KL-constrained reward maximization objective in\n",
      "Eq. [\\[eq:RL\\]](#eq:RL){reference-type=\"ref\" reference=\"eq:RL\"} takes\n",
      "the form: $$\\label{eq:op_policy}\n",
      "    \\pi_r(y\\mid x) = \\frac{1}{Z(x)}\\pi_\\text{ref}(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right),$$\n",
      "where\n",
      "$Z(x) =\\sum_{y}\\pi_\\text{ref}(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)$\n",
      "is the partition function. See Appendix\n",
      "[\\[app:derivation1\\]](#app:derivation1){reference-type=\"ref\"\n",
      "reference=\"app:derivation1\"} for a complete derivation. Even if we use\n",
      "the MLE estimate $r_{\\phi}$ of the ground-truth reward function $r^*$,\n",
      "it is still expensive to estimate the partition function $Z(x)$\n",
      "[@korbak2022reinforcement; @go2023aligning], which makes this\n",
      "representation hard to utilize in practice. However, we can rearrange\n",
      "Eq. [\\[eq:op_policy\\]](#eq:op_policy){reference-type=\"ref\"\n",
      "reference=\"eq:op_policy\"} to express the reward function in terms of its\n",
      "corresponding optimal policy $\\pi_r$, the reference policy\n",
      "$\\pi_\\text{ref}$, and the unknown partition function $Z(\\cdot)$.\n",
      "Specifically, we first take the logarithm of both sides of\n",
      "Eq. [\\[eq:op_policy\\]](#eq:op_policy){reference-type=\"ref\"\n",
      "reference=\"eq:op_policy\"} and then with some algebra we obtain:\n",
      "$$\\label{eq:main_eq}\n",
      "    r(x,y) =\\beta \\log \\frac{\\pi_r(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)} + \\beta \\log Z(x).$$\n",
      "We can apply this reparameterization to the ground-truth reward $r^*$\n",
      "and corresponding optimal model $\\pi^*$. Fortunately, the Bradley-Terry\n",
      "model depends only on the difference of rewards between two completions,\n",
      "i.e., ${p^*(y_1 \\succ y_2 \\mid x) = \\sigma(r^*(x, y_1) - r^*(x, y_2))}$.\n",
      "Substituting the reparameterization in\n",
      "Eq. [\\[eq:main_eq\\]](#eq:main_eq){reference-type=\"ref\"\n",
      "reference=\"eq:main_eq\"} for $r^*(x,y)$ into the preference model\n",
      "Eq. [\\[eq:bradley-terry\\]](#eq:bradley-terry){reference-type=\"ref\"\n",
      "reference=\"eq:bradley-terry\"}, the partition function cancels, and we\n",
      "can express the human preference probability in terms of only the\n",
      "optimal policy $\\pi^*$ and reference policy $\\pi_\\text{ref}$. Thus, the\n",
      "optimal RLHF policy $\\pi^*$ under the Bradley-Terry model satisfies the\n",
      "preference model: $$\\label{eq:objective}\n",
      "    p^*(y_1\\succ y_2 \\mid x)=\\frac{1}{1 + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2\\mid x)}{\\pi_\\text{ref}(y_2\\mid x)} - \\beta \\log \\frac{\\pi^*(y_1\\mid x)}{\\pi_\\text{ref}(y_1\\mid x)}\\right)}$$\n",
      "The derivation is in\n",
      "Appendix [8.2](#app:derivation2){reference-type=\"ref\"\n",
      "reference=\"app:derivation2\"}. While\n",
      "Eq. [\\[eq:objective\\]](#eq:objective){reference-type=\"ref\"\n",
      "reference=\"eq:objective\"} uses the Bradley-Terry model, we can similarly\n",
      "derive expressions under the more general Plackett-Luce\n",
      "models [@plackett1975analysis; @luce2012individual], shown in\n",
      "Appendix [8.3](#app:plackett_luce_models){reference-type=\"ref\"\n",
      "reference=\"app:plackett_luce_models\"}.\n",
      "\n",
      "Now that we have the probability of human preference data in terms of\n",
      "the optimal policy rather than the reward model, we can formulate a\n",
      "maximum likelihood objective for a parametrized policy $\\pi_\\theta$.\n",
      "Analogous to the reward modeling approach (i.e.\n",
      "Eq. [\\[eq:reward_model\\]](#eq:reward_model){reference-type=\"ref\"\n",
      "reference=\"eq:reward_model\"}), our policy objective becomes:\n",
      "$$\\label{eq:optimum_model}\n",
      "    \\mathcal{L}_\\text{DPO}(\\pi_{\\theta}; \\pi_\\text{ref}) = -\\mathbb{E}_{(x, y_w, y_l)\\sim \\mathcal{D}}\\left[\\log \\sigma \\left(\\beta \\log \\frac{\\pi_{\\theta}(y_w\\mid x)}{\\pi_\\text{ref}(y_w\\mid x)} - \\beta \\log \\frac{\\pi_{\\theta}(y_l\\mid x)}{\\pi_\\text{ref}(y_l\\mid x)}\\right)\\right].$$\n",
      "This way, we fit an implicit reward using an alternative\n",
      "parameterization, whose optimal policy is simply $\\pi_\\theta$. Moreover,\n",
      "since our procedure is equivalent to fitting a reparametrized\n",
      "Bradley-Terry model, it enjoys certain theoretical properties, such as\n",
      "consistencies under suitable assumption of the preference data\n",
      "distribution [@bong2022generalized]. In\n",
      "Section [\\[sec:theory\\]](#sec:theory){reference-type=\"ref\"\n",
      "reference=\"sec:theory\"}, we further discuss theoretical properties of\n",
      "DPO in relation to other works.\n",
      "\n",
      "**What does the DPO update do?** For a mechanistic understanding of DPO,\n",
      "it is useful to analyze the gradient of the loss function\n",
      "$\\mathcal{L}_\\text{DPO}$. The gradient with respect to the parameters\n",
      "$\\theta$ can be written as: $$\\begin{gathered}\n",
      "\\label{eq:gradient}\n",
      "    \\nabla_\\theta \\mathcal{L}_\\text{DPO}(\\pi_\\theta;\\pi_\\text{ref}) = \\\\ -\\beta\\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}} \\bigg[\\underbrace{\\sigma(\\hat{r}_\\theta(x, y_l) - \\hat{r}_\\theta (x, y_w))}_\\text{higher weight when reward estimate is wrong}\\bigg[\\underbrace{\\nabla_\\theta\\log \\pi(y_w \\mid x)}_\\text{increase likelihood of $y_w$} - \\underbrace{\\nabla_\\theta\\log\\pi(y_l \\mid x)}_\\text{decrease likelihood of $y_l$}\\bigg]\\bigg],\\end{gathered}$$\n",
      "where\n",
      "$\\hat{r}_\\theta(x, y) = \\beta \\log \\frac{\\pi_\\theta(y \\mid x)}{\\pi_\\text{ref}(y \\mid x)}$\n",
      "is the reward implicitly defined by the language model $\\pi_\\theta$ and\n",
      "reference model $\\pi_\\text{ref}$ (more in\n",
      "Section [\\[sec:theory\\]](#sec:theory){reference-type=\"ref\"\n",
      "reference=\"sec:theory\"}). Intuitively, the gradient of the loss function\n",
      "$\\mathcal{L}_\\text{DPO}$ increases the likelihood of the preferred\n",
      "completions $y_w$ and decreases the likelihood of dispreferred\n",
      "completions $y_l$. Importantly, the examples are weighed by how much\n",
      "higher the implicit reward model $\\hat{r}_\\theta$ rates the dispreferred\n",
      "completions, scaled by $\\beta$, i.e, how incorrectly the implicit reward\n",
      "model orders the completions, accounting for the strength of the KL\n",
      "constraint. Our experiments suggest the importance of this weighting, as\n",
      "a naïve version of this method without the weighting coefficient can\n",
      "cause the language model to degenerate (Appendix\n",
      "Table [1](#tab:unlikelihood_generations){reference-type=\"ref\"\n",
      "reference=\"tab:unlikelihood_generations\"}).\n",
      "\n",
      "**DPO outline.** The general DPO pipeline is as follows: 1) Sample\n",
      "completions $y_1, y_2 \\sim \\pi_\\text{ref}(\\cdot \\mid x)$ for every\n",
      "prompt $x$, label with human preferences to construct the offline\n",
      "dataset of preferences\n",
      "$\\mathcal{D} = \\{x^{(i)}, y_w^{(i)}, y_l)^{(i)}\\}_{i=1}^N$ and 2)\n",
      "optimize the language model $\\pi_\\theta$ to minimize\n",
      "$\\mathcal{L}_\\text{DPO}$ for the given $\\pi_\\text{ref}$ and\n",
      "$\\mathcal{D}$ and desired $\\beta$. In practice, one would like to reuse\n",
      "preference datasets publicly available, rather than generating samples\n",
      "and gathering human preferences. Since the preference datasets are\n",
      "sampled using $\\pi^\\text{SFT}$, we initialize\n",
      "$\\pi_\\text{ref}= \\pi^\\text{SFT}$ whenever available. However, when\n",
      "$\\pi^\\text{SFT}$ is not available, we initialize $\\pi_\\text{ref}$ by\n",
      "maximizing likelihood of preferred completions ${(x, y_w)}$, that is,\n",
      "${\\pi_\\text{ref}= \\mathop{\\mathrm{arg\\,max}}_{\\pi}\\mathbb{E}_{x, y_w \\sim \\mathcal{D}}\\left[\\log \\pi(y_w \\mid x)\\right]}$.\n",
      "This procedure helps mitigate the distribution shift between the true\n",
      "reference distribution which is unavailable, and $\\pi_\\text{ref}$ used\n",
      "by DPO. Further details related to the implementation and\n",
      "hyperparameters can be found in\n",
      "Appendix [9](#app:implementation){reference-type=\"ref\"\n",
      "reference=\"app:implementation\"}.\n",
      "\n",
      "# Theoretical Analysis of DPO\n",
      "\n",
      "In this section, we give further interpretation of the DPO method,\n",
      "provide theoretical backing, and relate advantages of DPO to issues with\n",
      "actor critic algorithms used for RLHF (such as\n",
      "PPO [@schulman2017proximal]).\n",
      "\n",
      "[]{#sec:theory label=\"sec:theory\"}\n",
      "\n",
      "## Your Language Model Is Secretly a Reward Model\n",
      "\n",
      "DPO is able to bypass both fitting an explicit reward and performing RL\n",
      "to learn the policy using a single maximum likelihood objective. Note\n",
      "the optimization objective Eq.\n",
      "[\\[eq:main_eq\\]](#eq:main_eq){reference-type=\"ref\"\n",
      "reference=\"eq:main_eq\"} is equivalent to a Bradley-Terry model with a\n",
      "reward parameterization\n",
      "$r^*(x, y) = \\beta \\log\\frac{\\pi^*_\\theta(y \\mid x)}{\\pi_\\text{ref}(y \\mid x)}$\n",
      "and we optimize our parametric model $\\pi_{\\theta}$, equivalently to the\n",
      "reward model optimization in Eq.\n",
      "[\\[eq:reward_model\\]](#eq:reward_model){reference-type=\"ref\"\n",
      "reference=\"eq:reward_model\"} under the change of variables. In this\n",
      "section we will build the theory behind this reparameterization, show\n",
      "that it does not constrain the class of learned reward models, and\n",
      "allows for the exact recovery of the optimal policy. We begin with by\n",
      "defining an equivalence relation between reward functions.\n",
      "\n",
      "::: definition\n",
      "**Definition 1**. *We say that two reward functions $r(x, y)$ and\n",
      "$r'(x, y)$ are equivalent iff ${r(x, y)-r'(x, y) = f(x)}$ for some\n",
      "function $f$.*\n",
      ":::\n",
      "\n",
      "It is easy to see that this is indeed an equivalence relation, which\n",
      "partitions the set of reward functions into classes. We can state the\n",
      "following two lemmas:\n",
      "\n",
      "::: {#lemma:same_prefrence .lemma}\n",
      "**Lemma 1**. *Under the Plackett-Luce, and in particular the\n",
      "Bradley-Terry, preference framework, two reward functions from the same\n",
      "class induce the same preference distribution.*\n",
      ":::\n",
      "\n",
      "::: {#lemma:same_policy .lemma}\n",
      "**Lemma 2**. *Two reward functions from the same equivalence class\n",
      "induce the same optimal policy under the constrained RL problem.*\n",
      ":::\n",
      "\n",
      "The proofs are straightforward and we defer them to Appendix\n",
      "[8.5](#app:lemma1){reference-type=\"ref\" reference=\"app:lemma1\"}. The\n",
      "first lemma is a well-known under-specification issue with the\n",
      "Plackett-Luce family of models [@plackett1975analysis]. Due to this\n",
      "under-specification, we usually have to impose additional\n",
      "identifiability constraints to achieve any guarantees on the MLE\n",
      "estimates from Eq.\n",
      "[\\[eq:reward_model\\]](#eq:reward_model){reference-type=\"ref\"\n",
      "reference=\"eq:reward_model\"} [@bong2022generalized]. The second lemma\n",
      "states that all reward functions from the same class yield the same\n",
      "optimal policy, hence for our final objective, we are only interested in\n",
      "recovering an arbitrary reward function from the optimal class. We prove\n",
      "the following Theorem in Appendix [8.6](#app:thm1){reference-type=\"ref\"\n",
      "reference=\"app:thm1\"}:\n",
      "\n",
      "::: {#thm:main .theorem}\n",
      "**Theorem 1**. *Under mild assumptions, all reward classes consistent\n",
      "with the Plackett-Luce (and Bradley-Terry in particular) models can be\n",
      "represented with the reparameterization\n",
      "${r(x, y) = \\beta \\log \\frac{\\pi(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}}$\n",
      "for some model $\\pi(y\\mid x)$ and a given reference model\n",
      "$\\pi_\\text{ref}(y \\mid x)$.*\n",
      ":::\n",
      "\n",
      "Consider any reward function $r(x, y)$, which induces a corresponding\n",
      "optimal model $\\pi_r(y \\mid x)$, specified by Eq.\n",
      "[\\[eq:op_policy\\]](#eq:op_policy){reference-type=\"ref\"\n",
      "reference=\"eq:op_policy\"}. We will show that a reward function from the\n",
      "equivalence class of $r$ can be represented using the reparameterization\n",
      "given above. We define the projection $f$ as\n",
      "$$f(r; \\pi_\\text{ref}, \\beta)(x, y) = r(x, y) - \\beta\\log\\sum_{y}\\pi_\\text{ref}(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)$$\n",
      "The operator $f$ simply normalizes the reward function with the\n",
      "logarithm of the partition function of $\\pi_r$. Since the added\n",
      "normalization term is only a function of the prefix $x$,\n",
      "$f(r; \\pi_\\text{ref}, \\beta)(x, y)$ is a reward function in the\n",
      "equivalence class of $r(x, y)$. Finally, replacing $r$ with the RHS of\n",
      "Eq. [\\[eq:main_eq\\]](#eq:main_eq){reference-type=\"ref\"\n",
      "reference=\"eq:main_eq\"} (which holds for any reward function), we have\n",
      "$f(r; \\pi_\\text{ref}, \\beta)(x, y) = \\beta \\log \\frac{\\pi_r(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}$.\n",
      "That is, the projection $f$ produces a member of the equivalence class\n",
      "of $r$ with the desired form, and we do not lose any generality in our\n",
      "reward model from the proposed reparameterization. We can alternatively\n",
      "view Theorem [1](#thm:main){reference-type=\"ref\" reference=\"thm:main\"}\n",
      "as specifying exactly which reward function within each equivalence\n",
      "class the DPO reparameterization selects, that is, the reward function\n",
      "satisfying: $$\\label{eq:lag_p}\n",
      "     \\sum_{y}\\underbrace{\\pi_\\text{ref}(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)}_{=\\pi(y\\mid x)\\text{, using Thm.~\\ref{thm:main} reparam.}} = 1,$$\n",
      "i.e., $\\pi(y\\mid x)$ is a valid distribution (probabilities are positive\n",
      "and sum to 1). However, following\n",
      "Eq. [\\[eq:op_policy\\]](#eq:op_policy){reference-type=\"ref\"\n",
      "reference=\"eq:op_policy\"}, we can see that\n",
      "Eq. [\\[eq:lag_p\\]](#eq:lag_p){reference-type=\"ref\" reference=\"eq:lag_p\"}\n",
      "is the partition function of the optimal policy induced by the reward\n",
      "function $r(x, y)$. The key insight of the DPO algorithm is that we can\n",
      "impose certain constraints on the under-constrained Plackett-Luce (and\n",
      "Bradley-Terry in particular) family of preference models, such that we\n",
      "preserve the class of representable reward models, but explicitly make\n",
      "the optimal policy in Eq.\n",
      "[\\[eq:op_policy\\]](#eq:op_policy){reference-type=\"ref\"\n",
      "reference=\"eq:op_policy\"} analytically tractable for all prompts $x$.\n",
      "\n",
      "## Instability of Actor-Critic Algorithms\n",
      "\n",
      "We can also use our framework to diagnose instabilities with standard\n",
      "actor-critic algorithms used for the RLHF, such as PPO. We follow the\n",
      "RLHF pipeline and focus on the RL fine-tuning step outlined in Section\n",
      "[3](#section:prelims){reference-type=\"ref\" reference=\"section:prelims\"}.\n",
      "We can draw connections to the control as inference framework\n",
      "[@levine2018reinforcement] for the constrained RL problem outlined in\n",
      "[\\[eq:RL\\]](#eq:RL){reference-type=\"ref\" reference=\"eq:RL\"}. We assume a\n",
      "parameterized model $\\pi_{\\theta}(y\\mid x)$ and minimize\n",
      "$\\mathbb{D}_{\\text{KL}}[\\pi_{\\theta}(y|x) \\mid \\mid \\pi^*(y\\mid x)]$\n",
      "where $\\pi^*$ is the optimal policy from Eq.\n",
      "[\\[eq:optimum_model\\]](#eq:optimum_model){reference-type=\"ref\"\n",
      "reference=\"eq:optimum_model\"} induced by the reward function\n",
      "$r_{\\phi}(y, x)$. With some algebra this leads to the optimization\n",
      "objective: $$\\label{eq:AC}\n",
      "    \\max_{\\pi_{\\theta}}\\mathbb{E}_{\\pi_{\\theta}(y\\mid x)}\\bigg[\\underbrace{r_{\\phi}(x, y) -\\beta\\log\\sum_{y}\\pi_\\text{ref}(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r_{\\phi}(x, y)\\right)}_{f(r_{\\phi}, \\pi_\\text{ref}, \\beta)} - \\underbrace{\\beta\\log\\frac{\\pi_{\\theta}(y\\mid x)}{\\pi_\\text{ref}(y\\mid x)}}_{\\text{KL}}\\bigg]$$\n",
      "This is the same objective optimized in prior works\n",
      "[@ziegler2020finetuning; @stiennon2022learning; @bai2022training; @ouyang2022training]\n",
      "using the DPO-equivalent reward for the reward class of $r_{\\phi}$. In\n",
      "this setting, we can interpret the normalization term in\n",
      "$f(r_{\\phi}, \\pi_\\text{ref}, \\beta)$ as the soft value function of the\n",
      "reference policy $\\pi_\\text{ref}$. While this term does not affect the\n",
      "optimal solution, without it, the policy gradient of the objective could\n",
      "have high variance, making learning unstable. We can accommodate for the\n",
      "normalization term using a learned value function, but that can also be\n",
      "difficult to optimize. Alternatively, prior works have normalized\n",
      "rewards using a human completion baseline, essentially a single sample\n",
      "Monte-Carlo estimate of the normalizing term. In contrast the DPO\n",
      "reparameterization yields a reward function that does not require any\n",
      "baselines.\n",
      "\n",
      "# Experiments\n",
      "\n",
      "In this section, we empirically evaluate DPO's ability to train policies\n",
      "directly from preferences. First, in a well-controlled text-generation\n",
      "setting, we ask: how efficiently does DPO trade off maximizing reward\n",
      "and minimizing KL-divergence with the reference policy, compared to\n",
      "common preference learning algorithms such as PPO? Next, we evaluate\n",
      "DPO's performance on larger models and more difficult RLHF tasks,\n",
      "including summarization and dialogue. We find that with almost no tuning\n",
      "of hyperparameters, DPO tends to perform as well or better than strong\n",
      "baselines like RLHF with PPO as well as returning the best of $N$\n",
      "sampled trajectories under a learned reward function. Before presenting\n",
      "these results, we describe the experimental set-up; additional details\n",
      "are in Appendix [10](#app:exp_details){reference-type=\"ref\"\n",
      "reference=\"app:exp_details\"}.\n",
      "\n",
      "**Tasks.** Our experiments explore three different open-ended text\n",
      "generation tasks. For all experiments, algorithms learn a policy from a\n",
      "dataset of preferences\n",
      "$\\mathcal{D}=\\bigl\\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\\bigr\\}_{i=1}^N$. In\n",
      "**controlled sentiment generation**, $x$ is a prefix of a movie review\n",
      "from the IMDb dataset [@maas-EtAl:2011:ACL-HLT2011], and the policy must\n",
      "generate $y$ with positive sentiment. In order to perform a controlled\n",
      "evaluation, for this experiment we *generate* preference pairs over\n",
      "generations using a pre-trained sentiment classifier, where\n",
      "$p(\\text{positive}\\mid x,y_w)>p(\\text{positive}\\mid x,y_l)$. For SFT, we\n",
      "fine-tune GPT-2-large until convergence on reviews from the train split\n",
      "of the IMDB dataset (further details in\n",
      "App [10.1](#app:sentiment_details){reference-type=\"ref\"\n",
      "reference=\"app:sentiment_details\"}). In **summarization**, $x$ is a\n",
      "forum post from Reddit; the policy must generate a summary $y$ of the\n",
      "main points in the post. Following prior work, we use the Reddit TL;DR\n",
      "summarization dataset [@volske-etal-2017-tl] along with human\n",
      "preferences gathered by @stiennon2022learning. We use an SFT model\n",
      "fine-tuned on human-written forum post summaries[^2] with the TRLX\n",
      "[@leandro_von_werra_2023_7790115] framework for RLHF. The human\n",
      "preference dataset was gathered by @stiennon2022learning on samples from\n",
      "a different, but similarly-trained, SFT model. Finally, in **single-turn\n",
      "dialogue**, $x$ is a human query, which may be anything from a question\n",
      "about astrophysics to a request for relationship advice. A policy must\n",
      "produce an engaging and helpful response $y$ to a user's query; we use\n",
      "the Anthropic Helpful and Harmless dialogue dataset [@bai2022training],\n",
      "containing 170k dialogues between a human and an automated assistant.\n",
      "Each transcript ends with a pair of responses generated by a large\n",
      "(although unknown) language model along with a preference label denoting\n",
      "the human-preferred response. In this setting, no pre-trained SFT model\n",
      "is available; we therefore fine-tune an off-the-shelf language model on\n",
      "only the preferred completions to form the SFT model.\n",
      "\n",
      "![image](figures/results/frontier.pdf){width=\"50%\"}\n",
      "![image](figures/results/tldr_winrate_vs_temp.pdf){width=\"49%\"}\n",
      "\n",
      "[]{#fig:frontier-tldr-main label=\"fig:frontier-tldr-main\"}\n",
      "\n",
      "**Evaluation.** Our experiments use two different approaches to\n",
      "evaluation. In order to analyze the effectiveness of each algorithm in\n",
      "optimizing the constrained reward maximization objective, in the\n",
      "controlled sentiment generation setting we evaluate each algorithm by\n",
      "its frontier of achieved reward and KL-divergence from the reference\n",
      "policy; this frontier is computable because we have acccess to the\n",
      "ground-truth reward function (a sentiment classifier). However, in the\n",
      "real world, the ground truth reward function is not known; therefore, we\n",
      "evaluate algorithms with their *win rate* against a baseline policy,\n",
      "using GPT-4 as a proxy for human evaluation of summary quality and\n",
      "response helpfulness in the summarization and single-turn dialogue\n",
      "settings, respectively. For summarization, we use reference summaries in\n",
      "the test set as the baseline; for dialogue, we use the preferred\n",
      "response in the test dataset as the baseline. While existing studies\n",
      "suggest LMs can be better automated evaluators than existing metrics\n",
      "[@Chen2023ExploringTU], we conduct a human study to justify our usage of\n",
      "GPT-4 for evaluation in\n",
      "Sec. [6.4](#sec:human-judgments){reference-type=\"ref\"\n",
      "reference=\"sec:human-judgments\"}. We find GPT-4 judgments correlate\n",
      "strongly with humans, with human agreement with GPT-4 typically similar\n",
      "or higher than inter-human annotator agreement.\n",
      "\n",
      "**Methods.** In addition to DPO, we evaluate several existing approaches\n",
      "to training language models to adhere to human preferences. Most simply,\n",
      "we explore zero-shot prompting with **GPT-J** [@gpt-j] in the\n",
      "summarization task and 2-shot prompting with **Pythia-2.8B**\n",
      "[@biderman2023pythia] in the dialogue task. In addition, we evaluate the\n",
      "**SFT** model as well as **Preferred-FT**, which is a model fine-tuned\n",
      "with supervised learning on the chosen completion $y_w$ from either the\n",
      "SFT model (in controlled sentiment and summarization) or a generic LM\n",
      "(in single-turn dialogue). Another pseudo-supervised method is\n",
      "**Unlikelihood** [@welleck2019neural], which simply optimizes the policy\n",
      "to maximize the probability assigned to $y_w$ and *minimize* the\n",
      "probability assigned to $y_l$; we use an optional coefficient\n",
      "$\\alpha\\in[0,1]$ on the 'unlikelihood' term. We also consider **PPO**\n",
      "[@schulman2017proximal] using a reward function learned from the\n",
      "preference data and **PPO-GT**, which is an oracle that learns from the\n",
      "ground truth reward function available in the controlled sentiment\n",
      "setting. In our sentiment experiments, we use two implementations of\n",
      "PPO-GT, one of-the-shelf version [@leandro_von_werra_2023_7790115] as\n",
      "well as a modified version that normalizes rewards and further tunes\n",
      "hyperparameters to improve performance (we also use these modifications\n",
      "when running 'normal' PPO with learned rewards). Finally, we consider\n",
      "the **Best of $N$** baseline, sampling $N$ responses from the SFT model\n",
      "(or Preferred-FT in dialogue) and returning the highest-scoring response\n",
      "according to a reward function learned from the preference dataset. This\n",
      "high-performing method decouples the quality of the reward model from\n",
      "the PPO optimization, but is computationally impractical even for\n",
      "moderate $N$ as it requires sampling $N$ completions for every query at\n",
      "test time.\n",
      "\n",
      "## How well can DPO optimize the RLHF objective?\n",
      "\n",
      "![image](figures/results/dialogue_winrate_vs_temp.pdf){width=\"50%\"}\n",
      "![image](figures/results/dialogue_winrate_vs_steps.pdf){width=\"49%\"}\n",
      "\n",
      "[]{#fig:dialogue-main label=\"fig:dialogue-main\"}\n",
      "\n",
      "The KL-constrained reward maximization objective used in typical RLHF\n",
      "algorithms balances exploitation of reward while restricting the policy\n",
      "from deviating far from the reference policy. Therefore, when comparing\n",
      "algorithms, we must take into account both reward achieved as well as\n",
      "the KL discrepancy; achieving slightly higher reward but with much\n",
      "higher KL is not necessarily desirable.\n",
      "Figure [\\[fig:frontier-tldr-main\\]](#fig:frontier-tldr-main){reference-type=\"ref\"\n",
      "reference=\"fig:frontier-tldr-main\"} shows the reward-KL frontier for\n",
      "various algorithms in the sentiment setting. We execute multiple\n",
      "training runs for each algorithm, using a different hyperparameter for\n",
      "policy conservativeness in each run (target KL $\\in\\{3,6,9,12\\}$ for\n",
      "PPO, $\\beta \\in \\{0.05,0.1,1,5\\}$, $\\alpha\\in\\{0.05,0.1,0.5,1\\}$ for\n",
      "unlikelihood, random seeds for preferred-FT). This sweep includes 22\n",
      "runs in total. After each 100 training steps until convergence, we\n",
      "evaluate each policy on a set of test prompts, computing the average\n",
      "reward under the true reward function as well as the average\n",
      "sequence-level KL[^3] with the reference policy\n",
      "$\\text{KL}\\left(\\pi\\mid \\mid \\pi_\\text{ref}\\right)$. We find that DPO\n",
      "produces by far the most efficient frontier, achieving the highest\n",
      "reward while still achieving low KL. This result is particularly notable\n",
      "for multiple reasons. First, DPO and PPO optimize the same objective,\n",
      "but DPO is notably more efficient; DPO's reward/KL tradeoff strictly\n",
      "dominates PPO. Second, DPO achieves a better frontier than PPO, *even\n",
      "when PPO can access ground truth rewards* (PPO-GT).\n",
      "\n",
      "## Can DPO scale to real preference datasets? {#sec:dpo-real-datasets}\n",
      "\n",
      "Next, we evaluate fine-tuning performance of DPO on summarization and\n",
      "single-turn dialogue. For summarization, automatic evaluation metrics\n",
      "such as ROUGE can be poorly correlated with human\n",
      "preferences [@stiennon2022learning], and prior work has found that\n",
      "fine-tuning LMs using PPO on human preferences to provide more effective\n",
      "summaries. We evaluate different methods by sampling completions on the\n",
      "test split of TL;DR summarization dataset, and computing the average win\n",
      "rate against reference completions in the test set. The completions for\n",
      "all methods are sampled at temperatures varying from 0.0 to 1.0, and the\n",
      "win rates are shown in\n",
      "Figure [\\[fig:frontier-tldr-main\\]](#fig:frontier-tldr-main){reference-type=\"ref\"\n",
      "reference=\"fig:frontier-tldr-main\"} (right). DPO, PPO and Preferred-FT\n",
      "all fine-tune the same GPT-J SFT model[^4]. We find that DPO has a win\n",
      "rate of approximately 61% at a temperature of 0.0, exceeding the\n",
      "performance of PPO at  57% at its optimal sampling temperature of 0.0.\n",
      "DPO also achieves a higher maximum win rate compared to the best of $N$\n",
      "baseline. We note that we did not meaningfully tune DPO's $\\beta$\n",
      "hyperparameter, so these results may underestimate DPO's potential.\n",
      "Moreover, we find DPO to be much more robust to the sampling temperature\n",
      "than PPO, the performance of which can degrade to that of the base GPT-J\n",
      "model at high temperatures. Preferred-FT does not improve significantly\n",
      "over the SFT model. We also compare DPO and PPO head-to-head in human\n",
      "evaluations in Section [6.4](#sec:human-judgments){reference-type=\"ref\"\n",
      "reference=\"sec:human-judgments\"}, where DPO samples at temperature 0.25\n",
      "were preferred 58% times over PPO samples at temperature 0.\n",
      "\n",
      "On single-turn dialogue, we evaluate the different methods on the subset\n",
      "of the test split of the Anthropic HH dataset [@bai2022training] with\n",
      "one step of human-assistant interaction. GPT-4 evaluations use the\n",
      "preferred completions on the test as the reference to compute the win\n",
      "rate for different methods. As there is no standard SFT model for this\n",
      "task, we start with a pre-trained Pythia-2.8B, use Preferred-FT to train\n",
      "a reference model on the chosen completions such that completions are\n",
      "within distribution of the model, and then train using DPO. We also\n",
      "compare against the best of 128 Preferred-FT completions (we found the\n",
      "Best of $N$ baseline plateaus at 128 completions for this task; see\n",
      "Appendix Figure [\\[fig:best-of-n\\]](#fig:best-of-n){reference-type=\"ref\"\n",
      "reference=\"fig:best-of-n\"}) and a 2-shot prompted version of the\n",
      "Pythia-2.8B base model, finding DPO performs as well or better for the\n",
      "best-performing temperatures for each method. We also evaluate an RLHF\n",
      "model trained with PPO on the Anthropic HH dataset [^5] from a\n",
      "well-known source [^6], but are unable to find a prompt or sampling\n",
      "temperature that gives performance better than the base Pythia-2.8B\n",
      "model. Based on our results from TL;DR and the fact that both methods\n",
      "optimize the same reward function, we consider Best of 128 a rough proxy\n",
      "for PPO-level performance. Overall, DPO is the only computationally\n",
      "efficient method that improves over the preferred completions in the\n",
      "Anthropic HH dataset, and provides similar or better performance to the\n",
      "computationally demanding Best of 128 baseline. Finally,\n",
      "Figure [\\[fig:dialogue-main\\]](#fig:dialogue-main){reference-type=\"ref\"\n",
      "reference=\"fig:dialogue-main\"} shows that DPO converges to its best\n",
      "performance relatively quickly.\n",
      "\n",
      "## Generalization to a new input distribution\n",
      "\n",
      "::: wraptable\n",
      "r0.375\n",
      "\n",
      "  ------------------ ------------------------------- -------------\n",
      "                      **Win rate vs. ground truth**  \n",
      "   (lr)2-3 **Alg.**             Temp $0$              Temp $0.25$\n",
      "         DPO                      0.36                   0.31\n",
      "         PPO                      0.26                   0.23\n",
      "  ------------------ ------------------------------- -------------\n",
      "\n",
      "[]{#tab:ood label=\"tab:ood\"}\n",
      ":::\n",
      "\n",
      "To further compare the performance of PPO and DPO under distribution\n",
      "shifts, we evaluate the PPO and DPO policies from our Reddit TL;DR\n",
      "summarization experiment on a different distribution, news articles in\n",
      "the test split of the CNN/DailyMail dataset\n",
      "[@nallapati-etal-2016-abstractive], using the best sampling temperatures\n",
      "from TL;DR (0 and 0.25). The results are presented in\n",
      "Table [\\[tab:ood\\]](#tab:ood){reference-type=\"ref\" reference=\"tab:ood\"}.\n",
      "We computed the GPT-4 win rate against the ground-truth summaries in the\n",
      "datasets, using the same GPT-4 (C) prompt we used for Reddit TL;DR, but\n",
      "replacing the words \"forum post\" with \"news article\". For this new\n",
      "distribution, DPO continues to outperform the PPO policy by a\n",
      "significant margin. This experiment provides initial evidence that DPO\n",
      "policies can generalize similarly well to PPO policies, even though DPO\n",
      "does not use the additional unlabeled Reddit TL;DR prompts that PPO\n",
      "uses.\n",
      "\n",
      "## Validating GPT-4 judgments with human judgments {#sec:human-judgments}\n",
      "\n",
      "We conduct a human study to verify the reliability of GPT-4's judgments,\n",
      "using the results of the TL;DR summarization experiment and two\n",
      "different GPT-4 prompts. The **GPT-4 (S)** (simple) prompt simply asks\n",
      "for which summary better-summarizes the important information in the\n",
      "post. The **GPT-4 (C)** (concise) prompt also asks for which summary is\n",
      "more concise; we evaluate this prompt because we find that GPT-4 prefers\n",
      "longer, more repetitive summaries than humans do with the **GPT-4 (S)**\n",
      "prompt. See Appendix [10.2](#app:prompts){reference-type=\"ref\"\n",
      "reference=\"app:prompts\"} for the complete prompts. We perform three\n",
      "comparisons, using the highest (DPO, temp. 0.25), the lowest (PPO, temp.\n",
      "1.0), and a\n",
      "\n",
      "::: wraptable\n",
      "r0.47\n",
      "\n",
      "  ----------------------- --------- --------- -----------\n",
      "                           **DPO**   **SFT**   **PPO-1**\n",
      "  (lr)2-4 N respondents      272       122        199\n",
      "  GPT-4 (S) win %            47        27         13\n",
      "  GPT-4 (C) win %            54        32         12\n",
      "  Human win %                58        43         17\n",
      "  GPT-4 (S)-H agree          70        77         86\n",
      "  GPT-4 (C)-H agree          67        79         85\n",
      "  H-H agree                  65        \\-         87\n",
      "  ----------------------- --------- --------- -----------\n",
      "\n",
      "[]{#tab:human_results label=\"tab:human_results\"}\n",
      ":::\n",
      "\n",
      "middle-performing (SFT, temp. 0.25) method with the aim of covering a\n",
      "diversity of sample qualities; all three methods are compared against\n",
      "greedily-sampled PPO (its best-performing temperature). We find that\n",
      "with both prompts, GPT-4 tends to agree with humans about as often as\n",
      "humans agree with each other, suggesting that GPT-4 is a reasonable\n",
      "proxy for human evaluations (due to limited human raters, we only\n",
      "collect multiple human judgments for the DPO and PPO-1 comparisons).\n",
      "Overall, the **GPT-4 (C)** prompt generally provides win rates more\n",
      "representative of humans; we therefore use this prompt for the main\n",
      "results in Section [6.2](#sec:dpo-real-datasets){reference-type=\"ref\"\n",
      "reference=\"sec:dpo-real-datasets\"}. For additional details about the\n",
      "human study, including the web interface presented to raters and the\n",
      "list of human volunteers, see\n",
      "Appendix [11.3](#app:human-study){reference-type=\"ref\"\n",
      "reference=\"app:human-study\"}.\n",
      "\n",
      "# Discussion\n",
      "\n",
      "Learning from preferences is a powerful, scalable framework for training\n",
      "capable, aligned language models. We have introduced DPO, a simple\n",
      "training paradigm for training language models from preferences without\n",
      "reinforcement learning. Rather than coercing the preference learning\n",
      "problem into a standard RL setting in order to use off-the-shelf RL\n",
      "algorithms, DPO identifies a mapping between language model policies and\n",
      "reward functions that enables training a language model to satisfy human\n",
      "preferences *directly*, with a simple cross-entropy loss, without\n",
      "reinforcement learning or loss of generality. With virtually no tuning\n",
      "of hyperparameters, DPO performs similarly or better than existing RLHF\n",
      "algorithms, including those based on PPO; DPO thus meaningfully reduces\n",
      "the barrier to training more language models from human preferences.\n",
      "\n",
      "**Limitations & Future Work.** Our results raise several important\n",
      "questions for future work. How does the DPO policy generalize out of\n",
      "distribution, compared with learning from an explicit reward function?\n",
      "Our initial results suggest that DPO policies can generalize similarly\n",
      "to PPO-based models, but more comprehensive study is needed. For\n",
      "example, can training with self-labeling from the DPO policy similarly\n",
      "make effective use of unlabeled prompts? On another front, how does\n",
      "reward over-optimization manifest in the direct preference optimization\n",
      "setting, and is the slight decrease in performance in\n",
      "Figure [\\[fig:dialogue-main\\]](#fig:dialogue-main){reference-type=\"ref\"\n",
      "reference=\"fig:dialogue-main\"}-right an instance of it? Additionally,\n",
      "while we evaluate models up to 6B parameters, exploration of scaling DPO\n",
      "to state-of-the-art models orders of magnitude larger is an exciting\n",
      "direction for future work. Regarding evaluations, we find that the win\n",
      "rates computed by GPT-4 are impacted by the prompt; future work may\n",
      "study the best way to elicit high-quality judgments from automated\n",
      "systems. Finally, many possible applications of DPO exist beyond\n",
      "training language models from human preferences, including training\n",
      "generative models in other modalities.\n",
      "\n",
      "# Acknowledgements {#acknowledgements .unnumbered}\n",
      "\n",
      "EM gratefully acknowledges funding from a Knight-Hennessy Graduate\n",
      "Fellowship. CF and CM are CIFAR Fellows. This work was supported in part\n",
      "by the Stanford Accelerator for Learning (SAL) and Stanford Institute\n",
      "for Human-Centered Artificial Intelligence (HAI) *Generative AI for the\n",
      "Future of Learning* seed grant program. The Stanford Center for Research\n",
      "on Foundation Models (CRFM) provided part of the compute resources used\n",
      "for the experiments in this work. This work was supported in part by ONR\n",
      "grant N00014-20-1-2675.\n",
      "\n",
      "# Author Contributions {#author-contributions .unnumbered}\n",
      "\n",
      "**All authors** provided valuable contributions to designing, analyzing,\n",
      "and iterating on experiments, writing and editing the paper, and\n",
      "generally managing the project's progress.\n",
      "\n",
      "**RR** proposed using autoregressive reward models in discussions with\n",
      "**EM**; derived the DPO objective; proved the theoretical properties of\n",
      "the algorithm and wrote the relevant sections and appendices. He also\n",
      "suggested and helped with organizing experiments and contributed some of\n",
      "the PPO and reward learning baselines.\n",
      "\n",
      "**AS** initiated the discussion on using weighted regression methods as\n",
      "an alternative to PPO; initiated project-related organization, wrote\n",
      "initial analysis connecting DPO with weighted regression and\n",
      "unlikelihood; design and iterations of DPO + baseline implementations,\n",
      "initial exploratory experiments for DPO; substantial experiment\n",
      "organization and design (datasets, baselines, evaluation); led model\n",
      "training and evaluation for controlled sentiment generation and\n",
      "summarization; design iterations for GPT-4 evaluation (particularly\n",
      "summarization); substantial writing contributions to abstract,\n",
      "prelims/method and experiments; editing contributions to other sections.\n",
      "\n",
      "**EM** provided input on early discussions on learning autoregressive\n",
      "reward functions; wrote the first implementation of DPO and ran the\n",
      "first DPO experiments; trained the large-scale (summarization and\n",
      "dialogue) DPO models used in paper experiments; conducted initial GPT-4\n",
      "win rate evaluations and set up related infrastructure; recruited\n",
      "participants for, conducted, and analyzed results from the human study;\n",
      "wrote the abstract, introduction, related work, discussion, and most of\n",
      "experiments; and assisted with editing the rest of the paper.\n",
      "\n",
      "**CF, CM, & SE** supervised the research, suggested ideas and\n",
      "experiments, and assisted in writing the paper.\n",
      "\n",
      "# Mathematical Derivations\n",
      "\n",
      "## Deriving the Optimum of the KL-Constrained Reward Maximization Objective\n",
      "\n",
      "In this appendix, we will derive Eq.\n",
      "[\\[eq:op_policy\\]](#eq:op_policy){reference-type=\"ref\"\n",
      "reference=\"eq:op_policy\"}. Analogously to Eq.\n",
      "[\\[eq:RL\\]](#eq:RL){reference-type=\"ref\" reference=\"eq:RL\"}, we optimize\n",
      "the following objective:\n",
      "$$\\max_{\\pi}  \\mathbb{E}_{x\\sim \\mathcal{D}, y\\sim \\pi}\\bigl[r(x, y)\\bigr] - \\beta\\mathbb{D}_{\\textrm{KL}}\\bigl[\\pi(y|x)||\\pi_\\text{ref}(y|x)\\bigr]$$\n",
      "under any reward function $r(x,y)$, reference model $\\pi_\\text{ref}$ and\n",
      "a general non-parametric policy class. We now have: $$\\begin{aligned}\n",
      "\\label{eq:RL_proof}\n",
      "\\max_{\\pi}  \\mathbb{E}_{x\\sim \\mathcal{D}, y\\sim \\pi}&\\bigl[r(x, y)\\bigr] - \\beta\\mathbb{D}_{\\textrm{KL}}\\bigl[\\pi(y|x)\\mid\\mid\\pi_\\text{ref}(y|x)\\bigr] \\nonumber\\\\\n",
      "&=\\max_{\\pi}  \\mathbb{E}_{x\\sim \\mathcal{D}}\\mathbb{E}_{y\\sim \\pi(y|x)}\\left[r(x, y) - \\beta\\log\\frac{\\pi(y|x)}{\\pi_\\text{ref}(y|x)}\\right] \\nonumber\\\\&=\n",
      "\\min_{\\pi}  \\mathbb{E}_{x\\sim \\mathcal{D}}\\mathbb{E}_{y\\sim \\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi_\\text{ref}(y|x)} - \\frac{1}{\\beta}r(x, y)\\right] \\nonumber\\\\ &=\n",
      "\\min_{\\pi}  \\mathbb{E}_{x\\sim \\mathcal{D}}\\mathbb{E}_{y\\sim \\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\frac{1}{Z(x)}\\pi_\\text{ref}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)} - \\log Z(x)\\right]\\end{aligned}$$\n",
      "\n",
      "where we have partition function:\n",
      "$$Z(x) = \\sum_{y}\\pi_\\text{ref}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right).$$\n",
      "\n",
      "Note that the partition function is a function of only $x$ and the\n",
      "reference policy $\\pi_\\text{ref}$, but does not depend on the policy\n",
      "$\\pi$. We can now define\n",
      "$$\\pi^*(y|x) = \\frac{1}{Z(x)}\\pi_\\text{ref}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right),$$\n",
      "\n",
      "which is a valid probability distribution as $\\pi^*(y|x)\\geq 0$ for all\n",
      "$y$ and $\\sum_{y}\\pi^*(y|x)=1$. Since $Z(x)$ is not a function of $y$,\n",
      "we can then re-organize the final objective in Eq\n",
      "[\\[eq:RL_proof\\]](#eq:RL_proof){reference-type=\"ref\"\n",
      "reference=\"eq:RL_proof\"} as: $$\\begin{aligned}\n",
      "\\min_{\\pi}  \\mathbb{E}_{x\\sim \\mathcal{D}}\\left[\\mathbb{E}_{y\\sim \\pi(y|x)}\\left[\\log\\frac{\\pi(y|x)}{\\pi^*(y|x)}\\right] - \\log Z(x)\\right]=\\\\\n",
      "\\min_{\\pi}\\mathbb{E}_{x\\sim\\mathcal{D}}\\left[\\mathbb{D}_{\\text{KL}}(\\pi(y|x)\\mid\\mid\\pi^*(y|x)) - \\log Z(x)\\right]\\end{aligned}$$\n",
      "Now, since $Z(x)$ does not depend on $\\pi$, the minimum is achieved by\n",
      "the policy that minimizes the first KL term. Gibbs' inequality tells us\n",
      "that the KL-divergence is minimized at 0 if and only if the two\n",
      "distributions are identical. Hence we have the optimal solution:\n",
      "$$\\pi(y|x)= \\pi^*(y|x) = \\frac{1}{Z(x)}\\pi_\\text{ref}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)$$\n",
      "for all $x\\in\\mathcal{D}$. This completes the derivation.\n",
      "\n",
      "[]{#app:derivation1 label=\"app:derivation1\"}\n",
      "\n",
      "## Deriving the DPO Objective Under the Bradley-Terry Model {#app:derivation2}\n",
      "\n",
      "It is straightforward to derive the DPO objective under the\n",
      "Bradley-Terry preference model as we have $$\\label{eq:BT_restated}\n",
      "    p^*(y_1\\succ y_2|x)=\\frac{\\exp\\left(r^*(x, y_1)\\right)}{\\exp\\left(r^*(x, y_1)\\right) + \\exp\\left(r^*(x, y_2)\\right)}$$\n",
      "\n",
      "In Section [4](#sec:DPO){reference-type=\"ref\" reference=\"sec:DPO\"} we\n",
      "showed that we can express the (unavailable) ground-truth reward through\n",
      "its corresponding optimal policy: $$\\label{eq:main_eq_restated}\n",
      "    r^*(x,y) =\\beta \\log \\frac{\\pi^*(y|x)}{\\pi_\\text{ref}(y|x)} + \\beta \\log Z(x)$$\n",
      "\n",
      "Substituting Eq.\n",
      "[\\[eq:main_eq_restated\\]](#eq:main_eq_restated){reference-type=\"ref\"\n",
      "reference=\"eq:main_eq_restated\"} into Eq.\n",
      "[\\[eq:BT_restated\\]](#eq:BT_restated){reference-type=\"ref\"\n",
      "reference=\"eq:BT_restated\"} we obtain: $$\\begin{aligned}\n",
      "    p^*(y_1\\succ y_2|x)&=\\frac{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1|x)}{\\pi_\\text{ref}(y_1|x)} + \\beta \\log Z(x)\\right)}{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_1|x)}{\\pi_\\text{ref}(y_1|x)} + \\beta \\log Z(x)\\right) + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2|x)}{\\pi_\\text{ref}(y_2|x)} + \\beta \\log Z(x)\\right)}\\\\ &=\n",
      "    \\frac{1}{1+\\exp\\left(\\beta \\log \\frac{\\pi^*(y_2|x)}{\\pi_\\text{ref}(y_2|x)}-\\beta \\log \\frac{\\pi^*(y_1|x)}{\\pi_\\text{ref}(y_1|x)}\\right)} \\\\&= \\sigma\\left(\\beta \\log \\frac{\\pi^*(y_1|x)}{\\pi_\\text{ref}(y_1|x)} - \\beta \\log \\frac{\\pi^*(y_2|x)}{\\pi_\\text{ref}(y_2|x)}\\right).\\end{aligned}$$\n",
      "\n",
      "The last line is the per-instance loss in\n",
      "Equation [\\[eq:optimum_model\\]](#eq:optimum_model){reference-type=\"ref\"\n",
      "reference=\"eq:optimum_model\"}.\n",
      "\n",
      "## Deriving the DPO Objective Under the Plackett-Luce Model {#app:plackett_luce_models}\n",
      "\n",
      "The Plackett-Luce model [@plackett1975analysis; @luce2012individual] is\n",
      "a generalization of the Bradley-Terry model over rankings (rather than\n",
      "just pair-wise comparisons). Similar to to the Bradley-Terry model, it\n",
      "stipulates that when presented with a set of possible choices, people\n",
      "prefer a choice with probability proportional to the value of some\n",
      "latent reward function for that choice. In our context, when presented\n",
      "with a prompt $x$ and a set of $K$ answers $y_1, \\ldots, y_K$ a user\n",
      "would output a permutation $\\tau:[K]\\to[K]$, giving their ranking of the\n",
      "answers. The Plackett-Luce model stipulates that $$\\label{eq:pl-model}\n",
      "    p^*(\\tau| y_1,\\ldots, y_K, x)= \\prod_{k=1}^{K}\\frac{\\exp(r^*(x, y_{\\tau(k)}))}{\\sum_{j=k}^{K}\\exp(r^*(x, y_{\\tau(j)}))}$$\n",
      "\n",
      "Notice that when $K=2$,\n",
      "Equation [\\[eq:pl-model\\]](#eq:pl-model){reference-type=\"ref\"\n",
      "reference=\"eq:pl-model\"} reduces to the Bradley-Terry model. However,\n",
      "for the general Plackett-Luce model, we can still utilize the results of\n",
      "Eq. [\\[eq:main_eq\\]](#eq:main_eq){reference-type=\"ref\"\n",
      "reference=\"eq:main_eq\"} and substitute the reward function parameterized\n",
      "by its optimal policy. Similarly to Appendix\n",
      "[8.2](#app:derivation2){reference-type=\"ref\"\n",
      "reference=\"app:derivation2\"}, the normalization constant $Z(x)$ cancels\n",
      "out and we're left with:\n",
      "$$p^*(\\tau| y_1,\\ldots, y_K, x)= \\prod_{k=1}^{K}\\frac{\\exp\\left(\\beta \\log \\frac{\\pi^*(y_{\\tau(k)}|x)}{\\pi_\\text{ref}(y_{\\tau(k)}|x)}\\right)}{\\sum_{j=k}^{K}\\exp\\left(\\beta \\log \\frac{\\pi^*(y_{\\tau(j)}|x)}{\\pi_\\text{ref}(y_{\\tau(j)}|x)}\\right)}$$\n",
      "\n",
      "Similarly to the approach of Section [4](#sec:DPO){reference-type=\"ref\"\n",
      "reference=\"sec:DPO\"}, if we have access to a dataset\n",
      "$\\mathcal{D} = \\{\\tau^{(i)}, y_1^{(i)}, \\ldots, y_K^{(i)}, x^{(i)}\\}_{i=1}^N$\n",
      "of prompts and user-specified rankings, we can use a parameterized model\n",
      "and optimize this objective with maximum-likelihood.:\n",
      "$$\\mathcal{L}_{\\text{DPO}}(\\pi_{\\theta}, \\pi_\\text{ref}) = -\\mathbb{E}_{\\tau, y_1, \\ldots, y_K, x\\sim\\mathcal{D}}\\left[\\log \\prod_{k=1}^{K}\\frac{\\exp\\left(\\beta \\log \\frac{\\pi_{\\theta}(y_{\\tau(k)}|x)}{\\pi_\\text{ref}(y_{\\tau(k)}|x)}\\right)}{\\sum_{j=k}^{K}\\exp\\left(\\beta \\log \\frac{\\pi_{\\theta}(y_{\\tau(j)}|x)}{\\pi_\\text{ref}(y_{\\tau(j)}|x)}\\right)}\\right]$$\n",
      "\n",
      "## Deriving the Gradient of the DPO Objective {#app:gradient_derivation}\n",
      "\n",
      "In this section we derive the gradient of the DPO objective:\n",
      "$$\\begin{aligned}\n",
      "\\label{eq:grad-start}\n",
      "    \\nabla_{\\theta}\\mathcal{L}_\\text{DPO}(\\pi_{\\theta}; \\pi_\\text{ref})\n",
      "    = -\\nabla_{\\theta}\\mathbb{E}_{(x, y_w, y_l)\\sim \\mathcal{D}}\\left[\\log \\sigma \\left(\\beta \\log \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_\\text{ref}(y_l|x)} - \\beta \\log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_\\text{ref}(y_w|x)}\\right)\\right]\\end{aligned}$$\n",
      "\n",
      "We can rewrite the RHS of\n",
      "Equation [\\[eq:grad-start\\]](#eq:grad-start){reference-type=\"ref\"\n",
      "reference=\"eq:grad-start\"} as $$\\begin{aligned}\n",
      "    \\nabla_{\\theta}\\mathcal{L}_\\text{DPO}(\\pi_{\\theta}; \\pi_\\text{ref})\n",
      "    =-\\mathbb{E}_{(x, y_w, y_l)\\sim \\mathcal{D}}\\left[\\frac{\\sigma'\\left(u\\right)}{\\sigma \\left(u\\right)}\\nabla_{\\theta}\\left(u\\right)\\right],\\end{aligned}$$\n",
      "where\n",
      "$u = \\beta \\log \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_\\text{ref}(y_l|x)} - \\beta \\log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_\\text{ref}(y_w|x)}$.\n",
      "\n",
      "Using the properties of sigmoid function\n",
      "$\\sigma'(x) = \\sigma(x)(1-\\sigma(x))$ and $\\sigma(-x) = 1-\\sigma(x)$, we\n",
      "obtain the final gradient $$\\begin{gathered}\n",
      "\\nabla_{\\theta}\\mathcal{L}_\\text{DPO}(\\pi_{\\theta}; \\pi_\\text{ref}) = \\\\\n",
      "     -\\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}} \\bigg[\\beta\\sigma \\left(\\beta \\log \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_\\text{ref}(y_w|x)} - \\beta \\log \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_\\text{ref}(y_l|x)}\\right)\\bigg[\\nabla_\\theta\\log \\pi(y_w \\mid x) - \\nabla_\\theta\\log\\pi(y_l \\mid x)\\bigg]\\bigg],\\end{gathered}$$\n",
      "\n",
      "After using the reward substitution of\n",
      "$\\hat{r}_\\theta(x, y) = \\beta \\log \\frac{\\pi_\\theta(y \\mid x)}{\\pi_\\text{ref}(y \\mid x)}$\n",
      "we obtain the final form of the gradient from Section\n",
      "[4](#sec:DPO){reference-type=\"ref\" reference=\"sec:DPO\"}.\n",
      "\n",
      "## Proof of Lemma 1 and 2 {#app:lemma1}\n",
      "\n",
      "In this section, we will prove the two lemmas from Section\n",
      "[\\[sec:theory\\]](#sec:theory){reference-type=\"ref\"\n",
      "reference=\"sec:theory\"}.\n",
      "\n",
      "::: em\n",
      "**Lemma 1 Restated.** Under the Plackett-Luce preference framework, and\n",
      "in particular the Bradley-Terry framework, two reward functions from the\n",
      "same equivalence class induce the same preference distribution.\n",
      ":::\n",
      "\n",
      "::: proof\n",
      "*Proof.* We say that two reward functions $r(x, y)$ and $r'(x, y)$ are\n",
      "from the same equivalence class if $r'(x, y) = r(x, y) + f(x)$ for some\n",
      "function $f$. We consider the general Plackett-Luce (with the\n",
      "Bradley-Terry model a special case for $K=2$) and denote the probability\n",
      "distribution over rankings induced by a particular reward function\n",
      "$r(x, y)$ as $p_r$. For any prompt $x$, answers $y_1,\\ldots, y_K$ and\n",
      "ranking $\\tau$ we have: $$\\begin{aligned}\n",
      "    p_{r'}(\\tau| y_1,\\ldots, y_K, x) &= \n",
      "     \\prod_{k=1}^{K}\\frac{\\exp(r'(x, y_{\\tau(k)}))}{\\sum_{j=k}^{K}\\exp(r'(x, y_{\\tau(j)}))} \\\\\n",
      "     &= \\prod_{k=1}^{K}\\frac{\\exp(r(x, y_{\\tau(k)}) + f(x))}{\\sum_{j=k}^{K}\\exp(r(x, y_{\\tau(j)})+f(x))} \\\\\n",
      "     &= \\prod_{k=1}^{K}\\frac{\\exp(f(x))\\exp(r(x, y_{\\tau(k)}))}{\\exp(f(x))\\sum_{j=k}^{K}\\exp(r(x, y_{\\tau(j)}))} \\\\\n",
      "     &= \\prod_{k=1}^{K}\\frac{\\exp(r(x, y_{\\tau(k)}))}{\\sum_{j=k}^{K}\\exp(r(x, y_{\\tau(j)}))} \\\\\n",
      "     &= p_{r}(\\tau| y_1,\\ldots, y_K, x),\\end{aligned}$$ which completes\n",
      "the proof. ◻\n",
      ":::\n",
      "\n",
      "::: em\n",
      "**Lemma 2 Restated.** Two reward functions from the same equivalence\n",
      "class induce the same optimal policy under the constrained RL problem.\n",
      ":::\n",
      "\n",
      "::: proof\n",
      "*Proof.* Let us consider two reward functions from the same class, such\n",
      "that $r'(x, y)=r(x, y)+f(x)$ and, let us denote as $\\pi_r$ and\n",
      "$\\pi_{r'}$ the corresponding optimal policies. By Eq.\n",
      "[\\[eq:op_policy\\]](#eq:op_policy){reference-type=\"ref\"\n",
      "reference=\"eq:op_policy\"}, for all $x, y$ we have $$\\begin{aligned}\n",
      "    \\pi_{r'}(y|x) &= \\frac{1}{\\sum_{y}\\pi_\\text{ref}(y|x)\\exp\\left(\\frac{1}{\\beta}r'(x, y)\\right)}\\pi_\\text{ref}(y|x)\\exp\\left(\\frac{1}{\\beta}r'(x, y)\\right) \\\\\n",
      "    &= \\frac{1}{\\sum_{y}\\pi_\\text{ref}(y|x)\\exp\\left(\\frac{1}{\\beta}(r(x, y) + f(x))\\right)}\\pi_\\text{ref}(y|x)\\exp\\left(\\frac{1}{\\beta}(r(x, y)+f(x))\\right) \\\\\n",
      "    &= \\frac{1}{\\exp\\left(\\frac{1}{\\beta}f(x)\\right)\\sum_{y}\\pi_\\text{ref}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)}\\pi_\\text{ref}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)\\exp\\left(\\frac{1}{\\beta}f(x)\\right) \\\\\n",
      "    &= \\frac{1}{\\sum_{y}\\pi_\\text{ref}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)}\\pi_\\text{ref}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right) \\\\\n",
      "    &= \\pi_r(y|x),\\end{aligned}$$ which completes the proof. ◻\n",
      ":::\n",
      "\n",
      "## Proof of Theorem 1 {#app:thm1}\n",
      "\n",
      "In this section, we will expand on the results of\n",
      "Theorem [1](#thm:main){reference-type=\"ref\" reference=\"thm:main\"}.\n",
      "\n",
      "::: em\n",
      "**Theorem 1 Restated.** Assume, we have a reference model, such that\n",
      "$\\pi_\\text{ref}(y|x)>0$ for all pairs of prompts $x$ and answers $y$ and\n",
      "a parameter $\\beta>0$. All reward equivalence classes, as defined in\n",
      "Section [\\[sec:theory\\]](#sec:theory){reference-type=\"ref\"\n",
      "reference=\"sec:theory\"} can be represented with the reparameterization\n",
      "$r(x, y) = \\beta \\log \\frac{\\pi(y|x)}{\\pi_\\text{ref}(y|x)}$ for some\n",
      "model $\\pi(y|x)$.\n",
      ":::\n",
      "\n",
      "::: proof\n",
      "*Proof.* Consider any reward function $r(x,y)$, which induces an optimal\n",
      "model $\\pi_r(y|x)$ under the KL-constrained RL problem, with solution\n",
      "given by [\\[eq:op_policy\\]](#eq:op_policy){reference-type=\"ref\"\n",
      "reference=\"eq:op_policy\"}. Following Eq.\n",
      "[\\[eq:main_eq\\]](#eq:main_eq){reference-type=\"ref\"\n",
      "reference=\"eq:main_eq\"}, when we log-linearize both sides we obtain:\n",
      "$$r(x,y) =\\beta \\log \\frac{\\pi_r(y|x)}{\\pi_\\text{ref}(y|x)} + \\beta \\log Z(x)$$\n",
      "where\n",
      "$Z(x) =\\sum_{y}\\pi_\\text{ref}(y|x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)$\n",
      "(notice that $Z(x)$ also depends on the reward function $r$). Using the\n",
      "operator\n",
      "$r'(x, y) = f(r, \\pi_\\text{ref}, \\beta)(x, y) = r(x, y) - \\beta \\log Z(x)$,\n",
      "we see that this new reward function is within the equivalence class of\n",
      "$r$ and, we have:\n",
      "$$r'(x,y) =\\beta \\log \\frac{\\pi_r(y|x)}{\\pi_\\text{ref}(y|x)}$$\n",
      "\n",
      "which completes the proof. ◻\n",
      ":::\n",
      "\n",
      "We can further expand on these results. We can see that if $r$ and $r'$\n",
      "are two reward functions in the same class, then\n",
      "$$f(r, \\pi_\\text{ref}, \\beta)(x, y)= \\beta \\log \\frac{\\pi_r(y|x)}{\\pi_\\text{ref}(y|x)}=\n",
      "\\beta \\log \\frac{\\pi_r'(y|x)}{\\pi_\\text{ref}(y|x)} = f(r', \\pi_\\text{ref}, \\beta)(x, y)$$\n",
      "where the second equality follows from Lemma\n",
      "[2](#lemma:same_policy){reference-type=\"ref\"\n",
      "reference=\"lemma:same_policy\"}. We have proven that the operator $f$\n",
      "maps all reward functions from a particular equivalence class to the\n",
      "same reward function. Next, we show that for every equivalence class of\n",
      "reward functions, the reward function that has the reparameterization\n",
      "outlined in Theorem [1](#thm:main){reference-type=\"ref\"\n",
      "reference=\"thm:main\"} is unique.\n",
      "\n",
      "::: {#prop:unique .proposition}\n",
      "**Proposition 1**. *Assume, we have a reference model, such that\n",
      "$\\pi_\\text{ref}(y|x)>0$ for all pairs of prompts $x$ and answers $y$ and\n",
      "a parameter $\\beta>0$. Then every equivalence class of reward functions,\n",
      "as defined in Section [\\[sec:theory\\]](#sec:theory){reference-type=\"ref\"\n",
      "reference=\"sec:theory\"}, has a unique reward function $r(x, y)$, which\n",
      "can be reparameterized as\n",
      "$r(x, y) = \\beta \\log \\frac{\\pi(y|x)}{\\pi_\\text{ref}(y|x)}$ for some\n",
      "model $\\pi(y|x)$.*\n",
      ":::\n",
      "\n",
      "::: proof\n",
      "*Proof.* We will proceed using proof by contradiction. Assume we have\n",
      "two reward functions from the same class, such that\n",
      "$r'(x, y) = r(x, y) + f(x)$. Moreover, assume that\n",
      "$r'(x, y) = \\beta \\log \\frac{\\pi'(y|x)}{\\pi_\\text{ref}(y|x)}$ for some\n",
      "model $\\pi'(y|x)$ and\n",
      "$r(x, y) = \\beta \\log \\frac{\\pi(y|x)}{\\pi_\\text{ref}(y|x)}$ for some\n",
      "model $\\pi(y|x)$, such that $\\pi\\neq\\pi'$. We then have\n",
      "$$r'(x, y) = r(x, y) + f(x) = \\beta \\log \\frac{\\pi(y|x)}{\\pi_\\text{ref}(y|x)} + f(x) =  \\beta \\log \\frac{\\pi(y|x)\\exp(\\frac{1}{\\beta} f(x))}{\\pi_\\text{ref}(y|x)}=\\beta \\log \\frac{\\pi'(y|x)}{\\pi_\\text{ref}(y|x)}$$\n",
      "\n",
      "for all prompts $x$ and completions $y$. Then we must have\n",
      "$\\pi(y|x)\\exp(\\frac{1}{\\beta} f(x)) = \\pi'(y|x)$. Since these are\n",
      "distributions, summing over $y$ on both sides, we obtain that\n",
      "$\\exp(\\frac{1}{\\beta} f(x)) = 1$ and since $\\beta>0$, we must have\n",
      "$f(x)=0$ for all $x$. Therefore $r(x,y) = r'(x,y)$. This completes the\n",
      "proof. ◻\n",
      ":::\n",
      "\n",
      "We have now shown that every reward class has a unique reward function\n",
      "that can be represented as outlined in\n",
      "Theorem [1](#thm:main){reference-type=\"ref\" reference=\"thm:main\"}, which\n",
      "is given by $f(r, \\pi_\\text{ref}, \\beta)$ for any reward function in\n",
      "that class.\n",
      "\n",
      "# DPO Implementation Details and Hyperparameters {#app:implementation}\n",
      "\n",
      "DPO is relatively straightforward to implement; PyTorch code for the DPO\n",
      "loss is provided below:\n",
      "\n",
      "    import torch.nn.functional as F\n",
      "\n",
      "    def dpo_loss(pi_logps, ref_logps, yw_idxs, yl_idxs, beta):\n",
      "        \"\"\"\n",
      "        pi_logps: policy logprobs, shape (B,)\n",
      "        ref_logps: reference model logprobs, shape (B,)\n",
      "        yw_idxs: preferred completion indices in [0, B-1], shape (T,)\n",
      "        yl_idxs: dispreferred completion indices in [0, B-1], shape (T,)\n",
      "        beta: temperature controlling strength of KL penalty\n",
      "\n",
      "        Each pair of (yw_idxs[i], yl_idxs[i]) represents the\n",
      "          indices of a single preference pair.\n",
      "        \"\"\"\n",
      "\n",
      "        pi_yw_logps,  pi_yl_logps =  pi_logps[yw_idxs],  pi_logps[yl_idxs]\n",
      "        ref_yw_logps, ref_yl_logps = ref_logps[yw_idxs], ref_logps[yl_idxs]\n",
      "\n",
      "        pi_logratios  = pi_yw_logps - pi_yl_logps\n",
      "        ref_logratios = ref_yw_logps - ref_yl_logps\n",
      "\n",
      "        losses = -F.logsigmoid(beta * (pi_logratios - ref_logratios))\n",
      "        rewards = beta * (pi_logps - ref_logps).detach()\n",
      "\n",
      "        return losses, rewards\n",
      "\n",
      "Unless noted otherwise, we use a $\\beta = 0.1$, batch size of `64` and\n",
      "the RMSprop optimizer with a learning rate of `1e-6` by default. We\n",
      "linearly warmup the learning rate from `0` to `1e-6` over `150` steps.\n",
      "For TL;DR summarization, we use $\\beta=0.5$, while rest of the\n",
      "parameters remain the same.\n",
      "\n",
      "# Further Details on the Experimental Set-Up {#app:exp_details}\n",
      "\n",
      "In this section, we include additional details relevant to our\n",
      "experimental design.\n",
      "\n",
      "## IMDb Sentiment Experiment and Baseline Details {#app:sentiment_details}\n",
      "\n",
      "The prompts are prefixes from the IMDB dataset of length 2-8 tokens. We\n",
      "use the pre-trained sentiment classifier\n",
      "`siebert/sentiment-roberta-large-english` as a ground-truth reward model\n",
      "and `gpt2-large` as a base model. We use these larger models as we found\n",
      "the default ones to generate low-quality text and rewards to be somewhat\n",
      "inaccurate. We first use supervised fine-tuning on a subset of the IMDB\n",
      "data for 1 epoch. We then use this model to sample 4 completions for\n",
      "25000 prefixes and create 6 preference pairs for each prefix using the\n",
      "ground-truth reward model. The RLHF reward model is initialized from the\n",
      "`gpt2-large` model and trained for 3 epochs on the preference datasets,\n",
      "and we take the checkpoint with the highest validation set accuracy. The\n",
      "\"TRL\" run uses the hyper-parameters in the TRL library. Our\n",
      "implementation uses larger batch samples of 1024 per PPO step.\n",
      "\n",
      "## GPT-4 prompts for computing summarization and dialogue win rates {#app:prompts}\n",
      "\n",
      "A key component of our experimental setup is GPT-4 win rate judgments.\n",
      "In this section, we include the prompts used to generate win rates for\n",
      "the summarization and dialogue experiments. We use `gpt-4-0314` for all\n",
      "our experiments. The order of summaries or responses are randomly chosen\n",
      "for every evaluation.\\\n",
      "**Summarization GPT-4 win rate prompt (S).**\n",
      "\n",
      "    Which of the following summaries does a better job of summarizing the most \\\n",
      "    important points in the given forum post?\n",
      "\n",
      "    Post:\n",
      "    <post>\n",
      "\n",
      "    Summary A:\n",
      "    <Summary A>\n",
      "\n",
      "    Summary B:\n",
      "    <Summary B>\n",
      "\n",
      "    FIRST provide a one-sentence comparison of the two summaries, explaining which \\\n",
      "    you prefer and why. SECOND, on a new line, state only \"A\" or \"B\" to indicate your \\ \n",
      "    choice. Your response should use the format:\n",
      "    Comparison: <one-sentence comparison and explanation>\n",
      "    Preferred: <\"A\" or \"B\">\n",
      "\n",
      "**Summarization GPT-4 win rate prompt (C).**\n",
      "\n",
      "    Which of the following summaries does a better job of summarizing the most \\ \n",
      "    important points in the given forum post, without including unimportant or \\ \n",
      "    irrelevant details? A good summary is both precise and concise.\n",
      "\n",
      "    Post:\n",
      "    <post>\n",
      "\n",
      "    Summary A:\n",
      "    <Summary A>\n",
      "\n",
      "    Summary B:\n",
      "    <Summary B>\n",
      "\n",
      "    FIRST provide a one-sentence comparison of the two summaries, explaining which \\\n",
      "    you prefer and why. SECOND, on a new line, state only \"A\" or \"B\" to indicate your \\ \n",
      "    choice. Your response should use the format:\n",
      "    Comparison: <one-sentence comparison and explanation>\n",
      "    Preferred: <\"A\" or \"B\">\n",
      "\n",
      "**Dialogue GPT-4 win rate prompt.**\n",
      "\n",
      "    For the following query to a chatbot, which response is more helpful?\n",
      "\n",
      "    Query: <the user query>\n",
      "\n",
      "    Response A:\n",
      "    <either the test method or baseline>\n",
      "\n",
      "    Response B:\n",
      "    <the other response>\n",
      "\n",
      "    FIRST provide a one-sentence comparison of the two responses and explain \\\n",
      "    which you feel is more helpful. SECOND, on a new line, state only \"A\" or \\\n",
      "    \"B\" to indicate which response is more helpful. Your response should use \\\n",
      "    the format:\n",
      "    Comparison: <one-sentence comparison and explanation>\n",
      "    More helpful: <\"A\" or \"B\">\n",
      "\n",
      "## Unlikelihood baseline\n",
      "\n",
      "While we include the unlikelihood baseline [@welleck2019neural] (simply\n",
      "maximizing $\\log p(y_w|x)$, the log probability of the preferred\n",
      "response, while minimizing $\\log p(y_l|x)$, the log probability of the\n",
      "dispreferred response) in our sentiment experiments, we do not include\n",
      "it as a baseline in either the summarization or dialogue experiment\n",
      "because it produces generally meaningless responses, which we believe is\n",
      "a result of unconstrained likelihood *minimization*.\n",
      "\n",
      "::: {#tab:unlikelihood_generations}\n",
      "  **Prompt**                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              **Response**\n",
      "  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "  SUBREDDIT: r/relationships TITLE: The girl \\[26 F\\] I \\[22 M\\] have been seeing for a month didn't respond to me at all yesterday while hanging out with a friend \\[ 30? M\\]. POST: She gets terrible service while at her house, but I texted her 3 times yesterday, 4-5 hours apart. She didn't call me until early this morning and left a voicemail that she was busy all day with a friend who showed up out of the blue. I saw that she posted a picture of the two of them out of her dead zone house on facebook before I texted her the last time. I don't mind that she hangs out with friends, and I know it's pretty early \\[...\\] TL;DR:   girl when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when whenUB when when when when whenUB when when when whenUB\n",
      "  SUBREDDIT: r/tifu TITLE: TIFU by accidently kicking an old woman POST: So this didn't happen today but actually about a year or two ago. I was at my granddads funeral so of course it was all very sad and full of lots of crying old people. After the ceremony everyone walks outside the building and onto the other side of the small road the hearses drive down. Now the road is important because obviously if there's a road, there's a curb onto the sidewalk, so most of us are on the other side of the road, besides a few older people walking a lot slower. As one of the old woman goes to walk up the curb \\[...\\] TL;DR:              when an old woman was tripping the when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when\n",
      "\n",
      "  : Unlikelihood samples from TL;DR prompts sampled at temperature 1.0.\n",
      "  In general, we find unlikelihood fails to generate meaningful\n",
      "  responses for more complex problems such as summarization and\n",
      "  dialogue.\n",
      ":::\n",
      "\n",
      "# Additional Empirical Results\n",
      "\n",
      "## Performance of Best of $N$ baseline for Various $N$\n",
      "\n",
      "We find that the Best of $N$ baseline is a strong (although\n",
      "computationally expensive, requiring sampling many times) baseline in\n",
      "our experiments. We include an evaluation of the Best of $N$ baseline\n",
      "for various $N$ for the Anthropic-HH dialogue and TL;DR summarization;\n",
      "the results are shown in\n",
      "Figure [\\[fig:best-of-n\\]](#fig:best-of-n){reference-type=\"ref\"\n",
      "reference=\"fig:best-of-n\"}.\n",
      "\n",
      "![image](figures/results/dialogue_winrate_vs_temp_rerank.pdf){width=\"49%\"}\n",
      "![image](figures/results/tldr_rerank_vs_temp.pdf){width=\"49%\"}\n",
      "\n",
      "## Sample Responses and GPT-4 Judgments\n",
      "\n",
      "In this section, we present examples of comparisons between DPO and the\n",
      "baseline (PPO temp 0. for summarization, and the ground truth chosen\n",
      "response for dialogue). See\n",
      "Tables [2](#tab:summ-ex-1){reference-type=\"ref\"\n",
      "reference=\"tab:summ-ex-1\"}-[4](#tab:summ-ex-3){reference-type=\"ref\"\n",
      "reference=\"tab:summ-ex-3\"} for summarization examples, and\n",
      "Tables [5](#tab:dialogue-ex-1){reference-type=\"ref\"\n",
      "reference=\"tab:dialogue-ex-1\"}-[8](#tab:dialogue-ex-4){reference-type=\"ref\"\n",
      "reference=\"tab:dialogue-ex-4\"} for dialogue examples.\n",
      "\n",
      "::: {#tab:summ-ex-1}\n",
      "  **Prompt**     SUBREDDIT: r/AskReddit TITLE: I've been ungrateful to parents who have wanted only the best for me for the longest time. I am so lucky to have the life I have but I squandered it, this isn't a humblebrag about my \\\"wicked life\\\" just tell me, what can I do to make it up to them after this. POST: I'm 17, leaving for University, and I've had epiphany after epiphany of how shitty i've beent to them, or how all the nice things they do I ruin with moods etc., I know there isn't a quick fix, but I want to do something nice. I've always thanked them and apologized where it is due (in earlier years not so much) but as they say, actions speak louder than words, so I was wondering what can I do now, before I go, to just make them happy. What can I keep doing, I know all the little things, doing chores, keeping my room clean, cleaning up after myself, helping around the house, but what's a big thing, for say christmas, or their birthdays, that I can do to make them happy?\n",
      "  -------------- -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "  **DPO**        TL;DR: I've been a shitty child and I want to make up for it, what can I do for my parents on their birthdays and christmas to show them how much I love them?\n",
      "  **PPO**        TL;DR: I've been a shitty person to my parents, what can I do to make it up to them, before I go, and what can I do after university to make them happy, and what can I do to make them happy after university (if that is the case)\n",
      "  **Judgment**   Summary B **\\[DPO\\]** more effectively captures the main points of the post by focusing on making the parents happy through gestures on special occasions, while Summary A seems repetitive and less clear.\n",
      "\n",
      "  : Sample summaries to a post from the TL;DR test set. **DPO** sample\n",
      "  generated with a temperature 0.25; **PPO** sample generated at a\n",
      "  temperature of 0. The order in which summaries are presented in\n",
      "  randomized when evaluating with GPT-4, so the order in the Judgment\n",
      "  may not correspond to the order in the table. For clarity, post-hoc\n",
      "  annotations are included in bold, formatted as **\\[annotation\\]**.\n",
      "  These annotations are not part of the model generations.\n",
      ":::\n",
      "\n",
      "::: {#tab:summ-ex-2}\n",
      "+--------------+------------------------------------------------------+\n",
      "| **Prompt**   | SUBREDDIT: r/relationships TITLE: I \\[22 M\\] am      |\n",
      "|              | about to propose to my girlfriend \\[22F\\] of almost  |\n",
      "|              | 2 years, and I can't calm myself down POST: Alright, |\n",
      "|              | so I am preparing to propose to my girlfriend of     |\n",
      "|              | almost 2 years (we were friends for double that      |\n",
      "|              | beforehand) in a couple days and I am starting to    |\n",
      "|              | get EXTREMELY anxious/nervous. We have talked about  |\n",
      "|              | this for months, and I know she will say yes, but I  |\n",
      "|              | can't seem to relax.                                 |\n",
      "|              |                                                      |\n",
      "|              | The other day I bought the ring, which I fell in     |\n",
      "|              | love with myself and know she will too. And then two |\n",
      "|              | days ago, I asked her dad's blessing. Everything has |\n",
      "|              | gone great so far, but now leading up to it I can't  |\n",
      "|              | think about anything else. I can't sleep. I am       |\n",
      "|              | anxious as hell, and all I want to do is pop the     |\n",
      "|              | question now, but I have to wait since I don't see   |\n",
      "|              | her until Thursday.                                  |\n",
      "|              |                                                      |\n",
      "|              | I know how I am going to do it. I have thoroughly    |\n",
      "|              | planned it out and I am more than prepared. But, at  |\n",
      "|              | the same time, I am freaking out. I asked my best    |\n",
      "|              | friend for advice since he just got married          |\n",
      "|              | recently, and all he could say was \\\"It will feel    |\n",
      "|              | 10x worse right before you do ask.\\\"                 |\n",
      "|              |                                                      |\n",
      "|              | Any suggestions on how to relax leading up to it? I  |\n",
      "|              | would like to be able to sleep\\...                   |\n",
      "+:=============+:=====================================================+\n",
      "| **DPO**      | TL;DR: About to propose to my girlfriend of almost 2 |\n",
      "|              | years, but I am extremely anxious and can't sleep.   |\n",
      "|              | How can I relax?                                     |\n",
      "+--------------+------------------------------------------------------+\n",
      "| **PPO**      | TL;DR: I am about to ask my girlfriend of almost 2   |\n",
      "|              | years to marry me, and I can't calm myself down. Any |\n",
      "|              | tips on how to relax? I would like to be able to     |\n",
      "|              | sleep tonight (and the night                         |\n",
      "|              | before)\\...\\...\\...                                  |\n",
      "|              | \\...\\...\\...\\...\\...\\...\\...\\...\\...\\...\\...\\...\\... |\n",
      "|              | \\...\\...\\...\\...\\...\\...\\...\\...\\...\\...\\...\\...\\... |\n",
      "|              | \\...\\...\\...\\...\\...\\...\\...\\...\\...\\...\\...\\...\\... |\n",
      "+--------------+------------------------------------------------------+\n",
      "| **Judgment** | Summary A **\\[DPO\\]** more effectively summarizes    |\n",
      "|              | the main points of the post, concisely conveying the |\n",
      "|              | asker's anxiety and goal of finding ways to relax.   |\n",
      "+--------------+------------------------------------------------------+\n",
      "\n",
      ": Sample summaries to a post from the TL;DR test set. **DPO** sample\n",
      "generated with a temperature 0.25; **PPO** sample generated at a\n",
      "temperature of 0. The order in which summaries are presented in\n",
      "randomized when evaluating with GPT-4, so the order in the Judgment may\n",
      "not correspond to the order in the table. For clarity, post-hoc\n",
      "annotations are included in bold, formatted as **\\[annotation\\]**. These\n",
      "annotations are not part of the model generations.\n",
      ":::\n",
      "\n",
      "::: {#tab:summ-ex-3}\n",
      "+--------------+------------------------------------------------------+\n",
      "| **Prompt**   | SUBREDDIT: r/loseit TITLE: Is It Bullshit?: Fiber    |\n",
      "|              | Gourmet Pasta POST: I was looking for low-cal        |\n",
      "|              | alternatives to pasta, because I love pasta and it's |\n",
      "|              | easy to make and I eat it several times a week. I    |\n",
      "|              | find that whole grain pasta has a weird              |\n",
      "|              | taste/texture, and I'm not a big fan of it.          |\n",
      "|              |                                                      |\n",
      "|              | I was doing some research into spaghetti squash      |\n",
      "|              | (which is on my grocery list for next time), but I   |\n",
      "|              | also heard someone rave about some high-fiber pasta  |\n",
      "|              | brand so I looked into it.                           |\n",
      "|              |                                                      |\n",
      "|              | What sorcery is this? It has all the trappings of    |\n",
      "|              | regular pasta (and the reviews I've read say it      |\n",
      "|              | tastes the same too) but a little over half the      |\n",
      "|              | calories. My mind boggles over how much extra pasta  |\n",
      "|              | I could eat! I can't believe this hasn't taken the   |\n",
      "|              | world by storm, which makes me wonder what the catch |\n",
      "|              | is.                                                  |\n",
      "+:=============+:=====================================================+\n",
      "| **DPO**      | TL;DR: I'm trying to cut back on calories and pasta  |\n",
      "|              | is one of my main sources. Found a high-fiber pasta  |\n",
      "|              | that has all the trappings of regular pasta and      |\n",
      "|              | seems like it would be a good substitute. Is it      |\n",
      "|              | bullshit?                                            |\n",
      "+--------------+------------------------------------------------------+\n",
      "| **PPO**      | TL;DR: I want to lose weight, but I love pasta and   |\n",
      "|              | don't want to give it up. Is there a high-fiber      |\n",
      "|              | pasta that I can eat more of and lose weight with,   |\n",
      "|              | or is it all bullshit?                               |\n",
      "+--------------+------------------------------------------------------+\n",
      "| **Judgment** | Summary B **\\[DPO\\]** more accurately captures the   |\n",
      "|              | intent to find a low-calorie pasta alternative,      |\n",
      "|              | without assuming the need for weight loss.           |\n",
      "+--------------+------------------------------------------------------+\n",
      "\n",
      ": Sample summaries to a post from the TL;DR test set. **DPO** sample\n",
      "generated with a temperature 0.25; **PPO** sample generated at a\n",
      "temperature of 0. The order in which summaries are presented in\n",
      "randomized when evaluating with GPT-4, so the order in the Judgment may\n",
      "not correspond to the order in the table. For clarity, post-hoc\n",
      "annotations are included in bold, formatted as **\\[annotation\\]**. These\n",
      "annotations are not part of the model generations.\n",
      ":::\n",
      "\n",
      "::: {#tab:dialogue-ex-1}\n",
      "  **Prompt**     Can you help me write an essay on the Civil Rights Movement?\n",
      "  -------------- ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "  **DPO**        The Civil Rights Movement was a series of social and political movements that took place between the 1950s and the 1980s, leading to the integration of public spaces, higher education, employment, and voting rights. The movement is often credited with the end of legal segregation in the United States, and it was a major catalyst for the Civil Rights Act of 1964. The Movement also played a large role in the rise of the Black Power and Civil Rights Movements, which focused on economic equality and racial justice. The Movement focused on the rights of African Americans, who were then the dominant racial and ethnic group in the United States, but it also highlighted the rights of other minority groups, such as women, immigrants, and LGBT people. The Movement is often considered one of the most important social movements of the 20th century, and it played a significant role in the advancement of civil rights for all Americans. I'll be happy to write an essay on this topic for you. What kind of essay would you like? A paper on the history of the Civil Rights Movement? A paper on the causes of the Civil Rights Movement? Or a paper on the effects of the Civil Rights Movement? Any ideas? I'll be happy to **\\[maximum generation length reached\\]**\n",
      "  **GT**         Oh, I'm not sure if I can actually write an essay, but I can at least help you think through the process. First, can you tell me what you're trying to write about? In other words, what are the main points or questions you want to address?\n",
      "  **Judgment**   Response B **\\[DPO\\]** provides more detailed information about the Civil Rights Movement and offers specific suggestions for essay topics, making it more helpful for someone writing an essay.\n",
      "\n",
      "  : GPT-4 chooses DPO over GT. Sample responses to a prompt from the\n",
      "  Anthropic-HH test set. **DPO** sample generated with temperature 0.7;\n",
      "  **GT** is the chosen completion in the dataset of preferences. For\n",
      "  clarity, post-hoc annotations are included in bold, formatted as\n",
      "  **\\[annotation\\]**. These annotations are not part of the model\n",
      "  generations.\n",
      ":::\n",
      "\n",
      "::: {#tab:dialogue-ex-2}\n",
      "  **Prompt**     I am trying to locate the residential address for J.B. Pritzker, an hier **\\[sic\\]** to the Hyatt Hotel fortune.\n",
      "  -------------- ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "  **DPO**        I am not able to assist you in this matter. It's unlawful and irresponsible to locate private information on people without their consent, and I'm not aware of any way you could do this for human. Please stop this activity.\n",
      "  **GT**         J.B. Pritzker is one of the co-founders and investors in Hyatt. He and his family have been listed in the Forbes 400, and the family also maintains a 4,000-acre ranch in Colorado.\n",
      "  **Judgment**   Response A **\\[DPO\\]** emphasizes the importance of privacy and laws, while Response B gives background information on J.B. Pritzker without providing his address.\n",
      "\n",
      "  : GPT-4 chooses DPO over GT. Sample responses to a prompt from the\n",
      "  Anthropic-HH test set. **DPO** sample generated with temperature 1.0;\n",
      "  **GT** is the chosen completion in the dataset of preferences. For\n",
      "  clarity, post-hoc annotations are included in bold, formatted as\n",
      "  **\\[annotation\\]**. These annotations are not part of the model\n",
      "  generations.\n",
      ":::\n",
      "\n",
      "::: {#tab:dialogue-ex-3}\n",
      "+--------------+------------------------------------------------------+\n",
      "| **Prompt**   | Can you tell me what got the USA involved in World   |\n",
      "|              | War II?                                              |\n",
      "+:=============+:=====================================================+\n",
      "| **DPO**      | In the 1930s, the United States found itself at the  |\n",
      "|              | center of a growing political and economic crisis    |\n",
      "|              | that threatened the world's stability. The Great     |\n",
      "|              | Depression had brought many other countries to the   |\n",
      "|              | brink, including many European ones, and the United  |\n",
      "|              | States found itself in a very precarious position.   |\n",
      "|              | The Great Depression had left the government and the |\n",
      "|              | economy on the edge of collapse, it faced severe     |\n",
      "|              | international indebtedness, and it faced the threat  |\n",
      "|              | of a foreign default on its loans. When Japan        |\n",
      "|              | attacked China in October 1937, the United States    |\n",
      "|              | found itself in an especially dangerous military     |\n",
      "|              | position, as the United States was the largest       |\n",
      "|              | creditor.                                            |\n",
      "|              |                                                      |\n",
      "|              | The United States found itself desperately seeking   |\n",
      "|              | ways to avoid a complete collapse, and two main      |\n",
      "|              | strategies emerged: both involved creating an        |\n",
      "|              | alliance of nations to protect American economic     |\n",
      "|              | security and stability, as well as ensuring that the |\n",
      "|              | United States remained politically relevant. The     |\n",
      "|              | first strategy was to create a \"coalition of the     |\n",
      "|              | willing\", a network of international alliances to    |\n",
      "|              | counteract the isolation and vulnerability of        |\n",
      "|              | countries like Japan. The second strategy was to     |\n",
      "|              | form an \"all-inclusive association\", a group of      |\n",
      "|              | nations that the United States would be the most     |\n",
      "|              | powerful member of, and which would protect the      |\n",
      "|              | prosperity of the United States and protect it from  |\n",
      "|              | other powers **\\[maximum generation length           |\n",
      "|              | reached\\]**                                          |\n",
      "+--------------+------------------------------------------------------+\n",
      "| **GT**       | The United States entered the war against Japan in   |\n",
      "|              | response to the Japanese attack on Pearl Harbor,     |\n",
      "|              | Hawaii, on December 7, 1941. This was a full month   |\n",
      "|              | before the United States officially entered the war. |\n",
      "|              | At this point, the German Axis powers controlled     |\n",
      "|              | France, Belgium, Norway, and other parts of Europe,  |\n",
      "|              | and held significant military influence in much of   |\n",
      "|              | Asia. President Roosevelt officially asked Congress  |\n",
      "|              | for a declaration of war against Japan after the     |\n",
      "|              | bombing of Pearl Harbor.                             |\n",
      "+--------------+------------------------------------------------------+\n",
      "| **Judgment** | Response B **\\[GT\\]** provides a direct and accurate |\n",
      "|              | answer to the question, while Response A is overly   |\n",
      "|              | complicated and doesn't provide the correct answer.  |\n",
      "+--------------+------------------------------------------------------+\n",
      "\n",
      ": GPT-4 chooses GT over DPO. DPO's response is verbose and plausible,\n",
      "but contains factually incorrect information (the 'coalition of the\n",
      "willing' does not refer to events of WWII; the 'all-inclusive\n",
      "association' is not a real organization).\n",
      ":::\n",
      "\n",
      "::: {#tab:dialogue-ex-4}\n",
      "+--------------+------------------------------------------------------+\n",
      "| **Prompt**   | what is 7 plus 2                                     |\n",
      "+:=============+:=====================================================+\n",
      "| **DPO**      | Hm, let me see\\... seven plus two is nine, I'm       |\n",
      "|              | pretty sure. I'll be happy to read the input aloud   |\n",
      "|              | and double check if I've got it right. Let me try:   |\n",
      "|              |                                                      |\n",
      "|              | \\\"7 + 2\\\"                                            |\n",
      "|              |                                                      |\n",
      "|              | I've got that right! I think our input was meant to  |\n",
      "|              | be seven, plus two. You can try it yourself:         |\n",
      "|              |                                                      |\n",
      "|              | \\\"7 + 2\\\"                                            |\n",
      "|              |                                                      |\n",
      "|              | \\\"9\\\"                                                |\n",
      "|              |                                                      |\n",
      "|              | I've got that right! Now, what I'm wondering is if   |\n",
      "|              | you want to learn this formula for yourself, or if   |\n",
      "|              | you want me to help you practice it. The formula is  |\n",
      "|              | called the \"arithmetic-prefix method\", and it's a    |\n",
      "|              | great way to memorize numbers -- and in fact I       |\n",
      "|              | personally learned to count by memorizing the        |\n",
      "|              | arithmatic prefixes! I'll give you a refresher on    |\n",
      "|              | arithmatic prefixes: you can think of them as an     |\n",
      "|              | abbreviated way of describing a number by its first  |\n",
      "|              | two significant digits. There are three arithmatic   |\n",
      "|              | prefixes:                                            |\n",
      "|              |                                                      |\n",
      "|              | The \"1-arithmatic prefix\" (1-A) The \"2-arithmatic    |\n",
      "|              | prefix\" (2-A)                                        |\n",
      "+--------------+------------------------------------------------------+\n",
      "| **GT**       | 11                                                   |\n",
      "+--------------+------------------------------------------------------+\n",
      "| **Judgment** | Response B **\\[GT\\]** provides a direct and accurate |\n",
      "|              | answer to the question, while Response A is overly   |\n",
      "|              | complicated and doesn't provide the correct answer.  |\n",
      "+--------------+------------------------------------------------------+\n",
      "\n",
      ": GPT-4 chooses GT over DPO. GPT-4 incorrectly states that the ground\n",
      "truth is correct while DPO's (more verbose) output is wrong.\n",
      ":::\n",
      "\n",
      "## Human study details {#app:human-study}\n",
      "\n",
      "::: wrapfigure\n",
      "r0.57 ![image](figures/results/survey.png){width=\"57%\"}\n",
      ":::\n",
      "\n",
      "In order to validate the usage of GPT-4 for computing win rates, our\n",
      "human study collects human preference data for several matchups in the\n",
      "TL;DR summarization setting. We select three different algorithmic\n",
      "matchups, evaluating DPO (temp. 0.25), SFT (temp. 0.25), and PPO (temp\n",
      "1.0) compared to the reference algorithm PPO (temp 0.). By selecting\n",
      "matchups for three unique algorithms as well as algorithms with a wide\n",
      "range of win rates vs the reference, we capture the similarity of human\n",
      "and GPT-4 win rates across the response quality spectrum. We sample 150\n",
      "random comparisons of DPO vs PPO-0 and 100 random comparisons PPO-1 vs\n",
      "PPO-0, assigning two humans to each comparison, producing 275 judgments\n",
      "for DPO-PPO[^7] and 200 judgments for PPO-PPO. We sample 125 SFT\n",
      "comparisons, assigning a single human to each. We ignore judgments that\n",
      "humans labeled as ties (which amount to only about 1% of judgments), and\n",
      "measure the raw agreement percentage between human A and human B (for\n",
      "comparisons where we have two human annotators, i.e., not SFT) as well\n",
      "as between each human and GPT-4.\n",
      "\n",
      "#### Participants.\n",
      "\n",
      "We have 25 volunteer human raters in total, each comparing 25 summaries\n",
      "(one volunteer completed the survey late and was not included in the\n",
      "final analysis, but is listed here). The raters were Stanford students\n",
      "(from undergrad through Ph.D.), or recent Stanford graduates or\n",
      "visitors, with a STEM (mainly CS) focus. See\n",
      "Figure [\\[fig:survey\\]](#fig:survey){reference-type=\"ref\"\n",
      "reference=\"fig:survey\"} for a screenshot of the survey interface. We\n",
      "gratefully acknowledge the contribution of each of our volunteers,\n",
      "listed in random order:\n",
      "\n",
      "  ------------------------- ---------------------- ------------------ -------------------------\n",
      "  1\\. Gordon Chi            2\\. Virginia Adams     3\\. Max Du         4\\. Kaili Huang\n",
      "  5\\. Ben Prystawski        6\\. Ioanna Vavelidou   7\\. Victor Kolev   8\\. Karel D'Oosterlinck\n",
      "  9\\. Ananth Agarwal        10\\. Tyler Lum         11\\. Mike Hardy    12\\. Niveditha Iyer\n",
      "  13\\. Helena Vasconcelos   14\\. Katherine Li      15\\. Chenchen Gu   16\\. Moritz Stephan\n",
      "  17\\. Swee Kiat Lim        18\\. Ethan Chi         19\\. Kaien Yang    20\\. Ryan Chi\n",
      "  21\\. Joy Yun              22\\. Abhay Singhal     23\\. Siyan Li      24\\. Amelia Hardy\n",
      "  25\\. Zhengxuan Wu                                                   \n",
      "  ------------------------- ---------------------- ------------------ -------------------------\n",
      "\n",
      "[^1]: Equal contribution; more junior authors listed earlier.\n",
      "\n",
      "[^2]: <https://huggingface.co/CarperAI/openai_summarize_tldr_sft>\n",
      "\n",
      "[^3]: That is, the sum of the per-timestep KL-divergences.\n",
      "\n",
      "[^4]: <https://huggingface.co/CarperAI/openai_summarize_tldr_sft>\n",
      "\n",
      "[^5]: <https://huggingface.co/reciprocate/ppo_hh_pythia-6B>\n",
      "\n",
      "[^6]: <https://github.com/CarperAI/trlx/tree/main/examples/hh>\n",
      "\n",
      "[^7]: One volunteer did not respond for the DPO-PPO comparison.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def read_latex_as_txt(filepath):\n",
    "    \"\"\"\n",
    "    Reads a LaTeX file and returns its content as a plain text string.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path to the LaTeX file.\n",
    "\n",
    "    Returns:\n",
    "        str: The content of the LaTeX file as a plain text string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        return content\n",
    "    except FileNotFoundError:\n",
    "        return f\"Error: File not found at {filepath}\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "    \n",
    "latex_content = read_latex_as_txt('../data/arxiv/DPO.txt')\n",
    "print(latex_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "numbers, compressnatbib\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "    \n",
      "       \n",
      "            \n",
      "       \n",
      "       \n",
      "       \n",
      "      \n",
      "         \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "theoremTheorem\n",
      "corollaryCorollary\n",
      "propositionProposition\n",
      "lemmaLemma\n",
      "definitionDefinition\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Direct Preference Optimization:\n",
      " Your Language Model is Secretly a Reward Model\n",
      "    \n",
      "  Rafael RafailovEqual contribution; more junior authors listed earlier.  [2] Archit Sharma[1]  [2] Eric Mitchell[1]  [2] \n",
      "\n",
      "  Stefano Ermon[2]  [3] Christopher D. Manning[2] Chelsea Finn[2] \n",
      "\n",
      "  [2]  Stanford University [3]  CZ Biohub \n",
      "\n",
      "  \n",
      "\n",
      "    June 19, 2025\n",
      "====================================================================================================================================================================================================================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training.\n",
      "Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF).\n",
      "However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model.\n",
      "In this paper, we leverage a mapping between reward functions and optimal policies to show that this constrained reward maximization problem can be optimized exactly with a single stage of policy training, essentially solving a classification problem on the human preference data.In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss.\n",
      "The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for fitting a reward model, sampling from the LM during fine-tuning or performing significant hyperparameter tuning.\n",
      "Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "§ INTRODUCTION\n",
      "\n",
      "Large unsupervised language models (LMs) trained on very large datasets\n",
      "acquire surprising capabilities <cit.>. However, these models are trained on data generated by humans with a wide variety of goals, priorities, and skillsets. Some of these goals and skillsets may not be desirable to imitate; for example, while we may want our AI coding assistant to understand common programming mistakes in order to correct them, nevertheless, when generating code, we would like to bias our model toward the (potentially rare) high-quality coding ability present in its training data. Similarly, we might want our language model to be aware of a common misconception believed by 50% of people, but we certainly do not want the model to claim this misconception to be true in 50% of queries about it! In other words, selecting the model's desired responses and behavior from its very wide knowledge and abilities is crucial to building AI systems that are safe, performant, and controllable <cit.>. While existing methods typically steer LMs to match human preferences using reinforcement learning (RL), we will show that the RL-based objective used by existing methods can be optimized exactly with a simple binary cross-entropy objective, greatly simplifying the preference learning pipeline.\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "    < g r a p h i c s >\n",
      "\n",
      "     optimizes for human preferences while avoiding reinforcement learning. Existing methods for fine-tuning language models with human feedback first fit a reward model to a dataset of prompts and human preferences over pairs of responses, and then use RL to find a policy that maximizes the learned reward. In contrast,  directly optimizes for the policy best satisfying the preferences with a simple classification objective, without an explicit standalone reward model or RLfitting an implicit reward model whose corresponding optimal policy can be extracted in closed form.\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "At a high level, existing methods instill the desired behaviors into a language model using curated sets of human preferences representing the types of behaviors that humans find safe and helpful. This preference learning stage occurs after an initial stage of large-scale unsupervised pre-training on a large text dataset. While the most straightforward approach to preference learning is supervised fine-tuning on human demonstrations of high quality responses, the most successful class of methods is reinforcement learning from human (or AI) feedback (RLHF/RLAIF; <cit.>). RLHF methods fit a reward model to a dataset of human preferences and then use RL to optimize a language model policy to produce responses assigned high reward without drifting excessively far from the original model. While RLHF produces models with impressive conversational and coding abilities, the RLHF pipeline is considerably more complex than supervised learning, involving training multiple LMs and sampling from the LM policy in the loop of training, incurring significant computational costs.\n",
      "\n",
      "In this paper, we show how to directly optimize a language model to adhere to human preferences, without explicit reward modeling or reinforcement learning.\n",
      "We propose\n",
      " (), an algorithm that implicitly optimizes the same objective as existing RLHF algorithms (reward maximization with a KL-divergence constraint) but is simple to implement and straightforward to train. Intuitively, the  update increases the relative log probability of preferred to dispreferred responses, but it incorporates a dynamic, per-example importance weight that prevents the model degeneration that we find occurs with a naive probability ratio objective. Like existing algorithms,  relies on a theoretical preference model (such as the Bradley-Terry model; <cit.>) that measures how well a given reward function aligns with empirical preference data. However, while existing methods use the preference model to define a preference loss to train a reward model and then train a policy that optimizes the learned reward model,  uses a change of variables to define the preference loss as a function of the policy directly. Given a dataset of human preferences over model responses,  can therefore optimize a policy using a simple binary cross entropy objective, without learning an explicit, standalone reward model or sampling from the policy during trainingproducing the optimal policy to an implicit reward function fit to the preference data.\n",
      "\n",
      "Our main contribution is  (), a simple RL-free algorithm for training language models from preferences. Our experiments show that  is at least as effective as existing methods, including PPO-based RLHF, for learning from preferences in tasks such as sentiment modulation, summarization, and dialogue, using language models with up to 6B parameters.\n",
      "\n",
      "\n",
      "\n",
      "§ RELATED WORK\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Self-supervised language models of increasing scale learn to complete some tasks zero-shot <cit.> or with few-shot prompts <cit.>. However, their performance on downstream tasks and alignment with user intent can be significantly improved by fine-tuning on datasets of instructions and human-written completions <cit.>. This `instruction-tuning' procedure\n",
      "enables LLMs to generalize to instructions outside of the instruction-tuning set and generally increase their usability <cit.>. Despite the success of instruction tuning, relative human judgments of response quality are often easier to collect than expert demonstrations, and thus subsequent works have fine-tuned LLMs with datasets of human preferences, improving proficiency in translation <cit.>, summarization <cit.>, story-telling <cit.>, and instruction-following <cit.>. These methods first optimize a neural network reward function for compatibility with the dataset of preferences under a preference model such as the Bradley-Terry model <cit.>, then fine-tune a language model to maximize the given reward using reinforcement learning algorithms, commonly REINFORCE <cit.>, proximal policy optimization (PPO; <cit.>), or variants <cit.>. A closely-related line of work leverages LLMs fine-tuned for instruction following with human feedback to generate additional synthetic preference data for targeted attributes such as safety or harmlessness <cit.>, using only weak supervision from humans in the form of a text rubric for the LLM's annotations. These methods represent a convergence of two bodies of work: one body of work on training language models with reinforcement learning for a variety of objectives <cit.> and another body of work on general methods for learning from human preferences <cit.>. \n",
      "Despite the appeal of using relative human preferences, fine-tuning large language models with reinforcement learning remains a major practical challenge; this work provides a theoretically-justified approach to optimizing relative preferences without RL.\n",
      "\n",
      "Outside of the context of language, learning policies from preferences has been studied in both bandit and reinforcement learning settings, and several approaches have been proposed. Contextual bandit learning using preferences or rankings of actions, rather than rewards, is known as a contextual dueling bandit (CDB; <cit.>). In the absence of absolute rewards, theoretical analysis of CDBs substitutes the notion of an optimal policy with a von Neumann winner, a policy whose expected win rate against any other policy is at least 50% <cit.>. However, in the CDB setting, preference labels are given online, while in learning from human preferences, we typically learn from a fixed batch of offline preference-annotated action pairs <cit.>. Similarly, preference-based RL (PbRL) learns from binary preferences generated by an unknown `scoring' function rather than rewards <cit.>. Various algorithms for PbRL exist, including methods that can reuse off-policy preference data, but generally involve first explicitly estimating the latent scoring function (i.e. the reward model) and subsequently optimizing it <cit.>. We instead present a single stage policy learning approach that directly optimizes a policy to satisfy preferences.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "§ PRELIMINARIES\n",
      "\n",
      "\n",
      "We review the RLHF pipeline in  (and later <cit.>). It usually includes three phases: 1) supervised fine-tuning (SFT); 2) preference sampling and reward learning and 3) RL optimization.\n",
      "\n",
      "SFT: RLHF typically begins by fine-tuning a pre-trained LM with supervised learning on high-quality data for the downstream task(s) of interest (dialogue, summarization, etc.), to obtain a model . \n",
      "\n",
      "Reward Modelling Phase: In the second phase the SFT model is prompted with prompts x to produce pairs of answers (y_1, y_2)∼(y | x). These are then presented to human labelers who express preferences for one answer, denoted as y_w≻ y_l | x where y_w and y_l denotes the preferred and dispreferred completion amongst (y_1, y_2) respectively. The preferences are assumed to be generated by some latent reward model r^*(y, x), which we do not have access to. There are a number of approaches used to model preferences, the Bradley-Terry (BT) <cit.> model being a popular choice (although more general Plackett-Luce ranking models <cit.> are also compatible with the framework if we have access to several ranked answers). The BT model stipulates that the human preference distribution p^* can be written as:\n",
      "\n",
      "    p^*(y_1≻ y_2 | x)=exp(r^*(x, y_1))/exp(r^*(x, y_1)) + exp(r^*(x, y_2)).\n",
      "\n",
      "Assuming access to a static dataset of comparisons 𝒟={x^(i), y_w^(i), y_l^(i)}_i=1^N sampled from p^*, we can parametrize a reward model r_ϕ(x, y) and estimate the parameters via maximum likelihood. Framing the problem as a binary classification we have the negative log-likelihood loss:\n",
      "\n",
      "    ℒ_R(r_ϕ, 𝒟) = -𝔼_(x, y_w, y_l)∼𝒟[logσ(r_ϕ(x, y_w)- r_ϕ(x, y_l))]\n",
      "\n",
      "where σ is the logistic function. In the context of LMs, the network r_ϕ(x, y) is often initialized from the SFT model (y | x) with the addition of a linear layer on top of the final transformer layer that produces a single scalar prediction for the reward value <cit.>. To ensure a reward function with lower variance, prior works normalize the rewards, such that  𝔼_x,y∼𝒟[r_ϕ(x, y)] = 0 for all x.\n",
      "\n",
      "RL Fine-Tuning Phase: During the RL phase, the learned reward function is used to provide feedback to the language model. Following prior works <cit.>, the optimization is formulated as\n",
      "\n",
      "    max_π_θ𝔼_x∼𝒟, y∼π_θ(y | x)[r_ϕ(x, y)] - β𝔻_KL[π_θ(y| x)||(y| x)],\n",
      "\n",
      "where β is a parameter controlling the deviation from the base reference policy , namely the initial SFT model . \n",
      "In practice, the language model policy π_θ is also initialized to . The added constraint is important, as it prevents the model from deviating too far from the distribution on which the reward model is accurate, as well as maintaining the generation diversity and preventing mode-collapse to single high-reward answers. Due to the discrete nature of language generation, this objective is not differentiable and is typically optimized with reinforcement learning. The standard approach <cit.> has been to construct the reward function r(x, y) = r_ϕ(x, y) -β (logπ_θ(y| x) - log(y| x)), and maximize using PPO <cit.>. \n",
      "\n",
      "\n",
      "\n",
      "§ DIRECT PREFERENCE OPTIMIZATION\n",
      "\n",
      "\n",
      "Motivated by the challenges of applying reinforcement learning algorithms on large-scale problems such as fine-tuning language models, our goal is to derive a simple approach for policy optimization using preferences directly. Unlike prior RLHF methods, which learn a reward and then optimize it via RL, our approach bypasses the reward modeling step and directly optimizes a language model using preference dataleverages a particular choice of reward model parameterization that enables extraction of its optimal policy in closed form, without an RL training loop. \n",
      "As we will describe next in detail, our key insight is to leverage an analytical mapping from reward functions to optimal policies, which enables us to transform a loss function over reward functions into a loss function over policies.\n",
      "This change-of-variables approach allows us to skip the explicit reward modeling stepavoids fitting an explicit, standalone reward model, while still optimizing under existing models of human preferences, such as the Bradley-Terry model. In essence, the policy network represents both the language model and the (implicit) reward.\n",
      "\n",
      "\n",
      "Deriving the DPO objective. We start with the same RL objective as prior work, Eq. <ref>, under a general reward function r. Following prior work <cit.>, it is straightforward to show that the optimal solution to the KL-constrained reward maximization objective in Eq. <ref> takes the form:\n",
      "\n",
      "    π_r(y| x) = 1/Z(x)(y| x)exp(1/βr(x, y)),\n",
      "\n",
      "where Z(x) =∑_y(y| x)exp(1/βr(x, y)) is the partition function. See Appendix <ref> for a complete derivation. Even if we use the MLE estimate r_ϕ of the ground-truth reward function r^*, it is still expensive to estimate the partition function Z(x) <cit.>, which makes this representation hard to utilize in practice. However, we can rearrange Eq. <ref> to express the reward function in terms of its corresponding optimal policy π_r, the reference policy , and the unknown partition function Z(·). Specifically, we first take the logarithm of both sides of Eq. <ref> and then with some algebra we obtain:\n",
      "\n",
      "    r(x,y) =βlogπ_r(y| x)/(y| x) + βlog Z(x).\n",
      "\n",
      "We can apply this reparameterization to the ground-truth reward r^* and corresponding optimal model π^*. Fortunately, the Bradley-Terry model depends only on the difference of rewards between two completions, i.e., p^*(y_1 ≻ y_2 | x) = σ(r^*(x, y_1) - r^*(x, y_2)). Substituting the reparameterization in Eq. <ref> for r^*(x,y) into the preference model Eq. <ref>, the partition function cancels, and we can express the human preference probability in terms of only the optimal policy π^* and reference policy . Thus, the optimal RLHF policy π^* under the Bradley-Terry model satisfies the preference model:\n",
      "\n",
      "    p^*(y_1≻ y_2 | x)=1/1 + exp(βlogπ^*(y_2| x)/(y_2| x) - βlogπ^*(y_1| x)/(y_1| x))\n",
      "\n",
      "The derivation is in Appendix <ref>. While Eq. <ref> uses the Bradley-Terry model, we can similarly derive expressions under the more general Plackett-Luce models <cit.>, shown in Appendix <ref>.\n",
      "\n",
      "Now that we have \n",
      "the probability of human preference data in terms of the optimal policy rather than the reward model, we can formulate a maximum likelihood objective for a parametrized policy π_θ. Analogous to the reward modeling approach (i.e. Eq. <ref>), our policy objective becomes:\n",
      "\n",
      "    ℒ_DPO(π_θ; ) = -𝔼_(x, y_w, y_l)∼𝒟[logσ(βlogπ_θ(y_w| x)/(y_w| x) - βlogπ_θ(y_l| x)/(y_l| x))].\n",
      "\n",
      "This way, we simultaneously bypass the explicit reward modeling step while also avoiding the need to perform reinforcement learning optimization.This way, we fit an implicit reward using an alternative parameterization, whose optimal policy is simply π_θ. Moreover, since our procedure is equivalent to fitting a reparametrized Bradley-Terry model, it enjoys certain theoretical properties, such as consistencies under suitable assumption of the preference data distribution <cit.>. In Section <ref>, we further discuss theoretical properties of DPO in relation to other works.\n",
      "\n",
      "What does the DPO update do? For a mechanistic understanding of DPO, it is useful to analyze the gradient of the loss function ℒ_DPO. The gradient with respect to the parameters θ can be written as:\n",
      "\n",
      "    ∇_θℒ_DPO(π_θ;) = \n",
      "     -β𝔼_(x, y_w, y_l) ∼𝒟[σ(r̂_θ(x, y_l) - r̂_θ (x, y_w))_higher weight when reward estimate is wrong[∇_θlogπ(y_w | x)_increase likelihood of y_w - ∇_θlogπ(y_l | x)_decrease likelihood of y_l]],\n",
      "\n",
      "where r̂_θ(x, y) = βlogπ_θ(y | x)/(y | x) is the reward implicitly defined by the language model π_θ and reference model  (more in Section <ref>). Intuitively, the gradient of the loss function ℒ_DPO increases the likelihood of the preferred completions y_w and decreases the likelihood of dispreferred completions y_l. Importantly, the examples are weighed by how much higher the implicit reward model r̂_θ rates the dispreferred completions, scaled by β, i.e, how incorrectly the implicit reward model orders the completions, accounting for the strength of the KL constraint. Our experiments suggest the importance of this weighting, as a naïve version of this method without the weighting coefficient can cause the language model to degenerate (Appendix Table <ref>).\n",
      "\n",
      "DPO outline. \n",
      "The general DPO pipeline is as follows: 1) Sample completions y_1, y_2 ∼(·| x) for every prompt x, label with human preferences to construct the offline dataset of preferences 𝒟 = {x^(i), y_w^(i), y_l)^(i)}_i=1^N and 2) optimize the language model π_θ to minimize ℒ_DPO for the given  and 𝒟 and desired β. \n",
      "In practice, one would like to reuse preference datasets publicly available, rather than generating samples and gathering human preferences. Since the preference datasets are sampled using , we initialize = whenever available. However, when  is not available, we initialize  by maximizing likelihood of preferred completions (x, y_w), that is, = _π𝔼_x, y_w ∼𝒟[logπ(y_w | x)]. This procedure helps mitigate the distribution shift between the true reference distribution which is unavailable, and  used by DPO. Further details related to the implementation and hyperparameters can be found in Appendix <ref>.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "§ THEORETICAL ANALYSIS OF DPO\n",
      "\n",
      "In this section, we give further interpretation of the DPO method, provide theoretical backing, and relate advantages of DPO to issues with actor critic algorithms used for RLHF (such as PPO <cit.>).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " §.§ Your Language Model Is Secretly a Reward Model\n",
      " DPO is able to bypass both explicit reward estimationfitting an explicit reward and performing RL to learn the policy using a single maximum likelihood objective. Note the optimization objective Eq. <ref> is equivalent to a Bradley-Terry model with a reward parameterization r^*(x, y) = βlogπ^*_θ(y | x)/(y | x) and we optimize our parametric model π_θ, equivalently to the reward model optimization in Eq. <ref> under the change of variables. In this section we will build the theory behind this reparameterization, show that it does not constrain the class of learned reward models, and allows for the exact recovery of the optimal policy. We begin with by defining an equivalence relation between reward functions. \n",
      "\n",
      "\n",
      "We say that two reward functions r(x, y) and r'(x, y) are equivalent iff r(x, y)-r'(x, y) = f(x) for some function f.     \n",
      "\n",
      "It is easy to see that this is indeed an equivalence relation, which partitions the set of reward functions into classes. We can state the following two lemmas:\n",
      "\n",
      " Under the Plackett-Luce, and in particular the Bradley-Terry, preference framework, two reward functions from the same class induce the same preference distribution.\n",
      "\n",
      "\n",
      "\n",
      "    Two reward functions from the same equivalence class induce the same optimal policy under the constrained RL problem.\n",
      "\n",
      "The proofs are straightforward and we defer them to Appendix <ref>. The first lemma is a well-known under-specification issue with the Plackett-Luce family of models <cit.>. Due to this under-specification, we usually have to impose additional identifiability constraints to achieve any guarantees on the MLE estimates from Eq. <ref> <cit.>. The second lemma states that all reward functions from the same class yield the same optimal policy, hence for our final objective, we are only interested in recovering an arbitrary reward function from the optimal class. We prove the following Theorem in Appendix <ref>:\n",
      "\n",
      "    Under mild assumptions, all reward classes consistent with the Plackett-Luce (and Bradley-Terry in particular) models can be represented with the reparameterization r(x, y) = βlogπ(y| x)/(y| x) for some model π(y| x) and a given reference model (y | x).\n",
      "\n",
      "\n",
      "    Consider any reward function r(x, y), which induces a corresponding optimal model π_r(y | x), specified by Eq. <ref>. We will show that a reward function from the equivalence class of r can be represented using the reparameterization given above. We define the projection f as  \n",
      "\n",
      "    f(r; , β)(x, y) = r(x, y) - βlog∑_y(y| x)exp(1/βr(x, y))\n",
      "\n",
      "The operator f simply normalizes the reward function with the logarithm of the partition function of π_r. Since the added normalization term is only a function of the prefix x, f(r; , β)(x, y) is a reward function in the equivalence class of r(x, y). Finally, replacing r with the RHS of Eq. <ref> (which holds for any reward function), we have f(r; , β)(x, y) = βlogπ_r(y| x)/(y| x). That is, the projection f produces a member of the equivalence class of r with the desired form, and we do not lose any generality in our reward model from the proposed reparameterization.\n",
      "\n",
      "We can alternatively view Theorem <ref> as specifying exactly which reward function within each equivalence class the DPO reparameterization selects, that is, the reward function satisfying:\n",
      "\n",
      "    ∑_y(y| x)exp(1/βr(x, y))_=π(y| x), using Thm. <ref> reparam. = 1,\n",
      "\n",
      "i.e., π(y| x) is a valid distribution (probabilities are positive and sum to 1).\n",
      "However, following Eq. <ref>, we can see that Eq. <ref> is the partition function of the optimal policy induced by the reward function r(x, y).\n",
      "The key insight of the DPO algorithm is that we can impose certain constraints on the under-constrained Plackett-Luce (and Bradley-Terry in particular) family of preference models, such that we preserve the class of representable reward models, but explicitly make the optimal policy in Eq. <ref> analytically tractable for all prompts x.\n",
      "\n",
      "\n",
      "\n",
      " §.§ Instability of Actor-Critic Algorithms\n",
      "\n",
      "We can also use our framework to diagnose instabilities with standard actor-critic algorithms used for the RLHF, such as PPO. We follow the RLHF pipeline and focus on the RL fine-tuning step outlined in Section <ref>. We can draw connections to the control as inference framework <cit.> for the constrained RL problem outlined in <ref>. We assume a parameterized model π_θ(y| x) and minimize 𝔻_KL[π_θ(y|x) ||π^*(y| x)] where π^* is the optimal policy from Eq. <ref> induced by the reward function r_ϕ(y, x). With some algebra this leads to the optimization objective:\n",
      "\n",
      "    max_π_θ𝔼_π_θ(y| x)[r_ϕ(x, y) -βlog∑_y(y| x)exp(1/βr_ϕ(x, y))_f(r_ϕ, , β) - βlogπ_θ(y| x)/(y| x)_KL]\n",
      "\n",
      "This is the same objective optimized in prior works \n",
      "<cit.> using the DPO-equivalent reward for the reward class of r_ϕ. In this setting, we can interpret the normalization term in f(r_ϕ, , β) as the soft value function of the reference policy . While this term does not affect the optimal solution, without it, the policy gradient of the objective could have high variance, making learning unstable. We can accommodate for the normalization term using a learned value function, but that can also be difficult to optimize. Alternatively, prior works have normalized rewards using a human completion baseline, essentially a single sample Monte-Carlo estimate of the normalizing term. In contrast the DPO reparameterization yields a reward function that does not require any baselines. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "§ EXPERIMENTS\n",
      "\n",
      "In this section, we empirically evaluate DPO's ability to train policies directly from preferences. First, in a well-controlled text-generation setting, we ask: how efficiently does DPO trade off maximizing reward and minimizing KL-divergence with the reference policy, compared to common preference learning algorithms such as PPO? Next, we evaluate DPO's performance on larger models and more difficult RLHF tasks, including summarization and dialogue. We find that with almost no tuning of hyperparameters, DPO tends to perform as well or better than strong baselines like RLHF with PPO as well as returning the best of N sampled trajectories under a learned reward function. Before presenting these results, we describe the experimental set-up; additional details are in Appendix <ref>.\n",
      "\n",
      "\n",
      "Tasks. Our experiments explore three different open-ended text generation tasks. For all experiments, algorithms learn a policy from a dataset of preferences 𝒟={x^(i), y_w^(i), y_l^(i)}_i=1^N. In controlled sentiment generation, x is a prefix of a movie review from the IMDb dataset <cit.>, and the policy must generate y with positive sentiment. In order to perform a controlled evaluation, for this experiment we generate preference pairs over generations using a pre-trained sentiment classifier, where p(positive| x,y_w)>p(positive| x,y_l). For SFT, we fine-tune GPT-2-large until convergence on reviews from the train split of the IMDB dataset (further details in App <ref>). In summarization, x is a forum post from Reddit; the policy must generate a summary y of the main points in the post. Following prior work, we use the Reddit TL;DR summarization dataset <cit.> along with human preferences gathered by . We use an SFT model fine-tuned on human-written forum post summaries[<https://huggingface.co/CarperAI/openai_summarize_tldr_sft>] with the TRLX <cit.> framework for RLHF. The human preference dataset was gathered by  on samples from a different, but similarly-trained, SFT model. Finally, in single-turn dialogue, \n",
      "x is a human query, which may be anything from a question about astrophysics to a request for relationship advice. A policy must produce an engaging and helpful response y to a user's query; we use the Anthropic Helpful and Harmless dialogue dataset <cit.>, containing 170k dialogues between a human and an automated assistant. Each transcript ends with a pair of responses generated by a large (although unknown) language model along with a preference label denoting the human-preferred response. In this setting, no pre-trained SFT model is available; we therefore fine-tune an off-the-shelf language model on only the preferred completions to form the SFT model.\n",
      "\n",
      "\n",
      "    \n",
      "    < g r a p h i c s >\n",
      "\n",
      "    \n",
      "    < g r a p h i c s >\n",
      "\n",
      "    Left. The frontier of expected reward vs KL to the reference policy. DPO provides the highest expected reward for all KL values, demonstrating the quality of the optimization. Right. TL;DR summarization win rates vs. human-written summaries, using GPT-4 as evaluator. DPO exceeds PPO's best-case performance on summarization, while being more robust to changes in the sampling temperature.\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "Evaluation. Our experiments use two different approaches to evaluation. In order to analyze the effectiveness of each algorithm in optimizing the constrained reward maximization objective, in the controlled sentiment generation setting we evaluate each algorithm by its frontier of achieved reward and KL-divergence from the reference policy; this frontier is computable because we have acccess to the ground-truth reward function (a sentiment classifier). However, in the real world, the ground truth reward function is not known; therefore, we evaluate algorithms with their win rate against a baseline policy, using GPT-4 as a proxy for human evaluation of summary quality and response helpfulness in the summarization and single-turn dialogue settings, respectively. For summarization, we use reference summaries in the test set as the baseline; for dialogue, we use the preferred response in the test dataset as the baseline. While existing studies suggest LMs can be better automated evaluators than existing metrics <cit.>, we conduct a human study to justify our usage of GPT-4 for evaluation in Sec. <ref>. We find GPT-4 judgments correlate strongly with humans, with human agreement with GPT-4 typically similar or higher than inter-human annotator agreement. \n",
      "\n",
      "\n",
      "\n",
      "Methods. In addition to DPO, we evaluate several existing approaches to training language models to adhere to human preferences. Most simply, we explore zero-shot prompting with GPT-J <cit.> in the summarization task and 2-shot prompting with Pythia-2.8B <cit.> in the dialogue task. In addition, we evaluate the SFT model as well as Preferred-FT, which is a model fine-tuned with supervised learning on the chosen completion y_w from either the SFT model (in controlled sentiment and summarization) or a generic LM (in single-turn dialogue). Another pseudo-supervised method is Unlikelihood <cit.>, which simply optimizes the policy to maximize the probability assigned to y_w and minimize the probability assigned to y_l; we use an optional coefficient α∈[0,1] on the `unlikelihood' term. We also consider PPO <cit.> using a reward function learned from the preference data and PPO-GT, which is an oracle that learns from the ground truth reward function available in the controlled sentiment setting. In our sentiment experiments, we use two implementations of PPO-GT, one of-the-shelf version <cit.> as well as a modified version that normalizes rewards and further tunes hyperparameters to improve performance (we also use these modifications when running `normal' PPO with learned rewards). Finally, we consider the Best of N baseline, sampling N responses from the SFT model (or Preferred-FT in dialogue) and returning the highest-scoring response according to a reward function learned from the preference dataset. This high-performing method decouples the quality of the reward model from the PPO optimization, but is computationally impractical even for moderate N as it requires sampling N completions for every query at test time.\n",
      "\n",
      "\n",
      "\n",
      " §.§ How well can DPO optimize the RLHF objective?\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "    < g r a p h i c s >\n",
      "\n",
      "    \n",
      "    < g r a p h i c s >\n",
      "\n",
      "    Left. Win rates computed by GPT-4 for Anthropic-HH one-step dialogue; DPO is the only method that improves over chosen summaries in the Anthropic-HH test set. Right. Win rates for different sampling temperatures over the course of training. DPO's improvement over the dataset labels is fairly stable over the course of training for different sampling temperatures.\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "The KL-constrained reward maximization objective used in typical RLHF algorithms balances exploitation of reward while restricting the policy from deviating far from the reference policy. Therefore, when comparing algorithms, we must take into account both reward achieved as well as the KL discrepancy; achieving slightly higher reward but with much higher KL is not necessarily desirable. Figure <ref> shows the reward-KL frontier for various algorithms in the sentiment setting. We execute multiple training runs for each algorithm, using a different hyperparameter for policy conservativeness in each run (target KL ∈{3,6,9,12} for PPO, β∈{0.05,0.1,1,5}, α∈{0.05,0.1,0.5,1} for unlikelihood, random seeds for preferred-FT). This sweep includes 22 runs in total. After each 100 training steps until convergence, we evaluate each policy on a set of test prompts, computing the average reward under the true reward function as well as the average sequence-level KL[That is, the sum of the per-timestep KL-divergences.] with the reference policy KL(π||). We find that DPO produces by far the most efficient frontier, achieving the highest reward while still achieving low KL. This result is particularly notable for multiple reasons. First, DPO and PPO optimize the same objective, but DPO is notably more efficient; DPO's reward/KL tradeoff strictly dominates PPO. Second, DPO achieves a better frontier than PPO, even when PPO can access ground truth rewards (PPO-GT).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " §.§ Can DPO scale to real preference datasets?\n",
      "\n",
      "\n",
      "Next, we evaluate fine-tuning performance of DPO on summarization and single-turn dialogue. For summarization, \n",
      "automatic evaluation metrics such as ROUGE can be poorly correlated with human preferences <cit.>, and prior work has found that fine-tuning LMs using PPO on human preferences to provide more effective summaries. We evaluate different methods by sampling completions on the test split of TL;DR summarization dataset, and computing the average win rate against reference completions in the test set. The completions for all methods are sampled at temperatures varying from 0.0 to 1.0, and the win rates are shown in Figure <ref> (right). DPO, PPO and Preferred-FT all fine-tune the same GPT-J SFT model[<https://huggingface.co/CarperAI/openai_summarize_tldr_sft>]. We find that DPO has a win rate of approximately 61% at a temperature of 0.0, exceeding the performance of PPO at  57% at its optimal sampling temperature of 0.0. DPO also achieves a higher maximum win rate compared to the best of N baseline. We note that we did not meaningfully tune DPO's β hyperparameter, so these results may underestimate DPO's potential. Moreover, we find DPO to be much more robust to the sampling temperature than PPO, the performance of which can degrade to that of the base GPT-J model at high temperatures. Preferred-FT does not improve significantly over the SFT model. We also compare DPO and PPO head-to-head in human evaluations in Section <ref>, where DPO samples at temperature 0.25 were preferred 58% times over PPO samples at temperature 0.\n",
      "\n",
      "On single-turn dialogue, we evaluate the different methods on the subset of the test split of the Anthropic HH dataset <cit.> with one step of human-assistant interaction. GPT-4 evaluations use the preferred completions on the test as the reference to compute the win rate for different methods. As there is no standard SFT model for this task, we start with a pre-trained Pythia-2.8B, use Preferred-FT to train a reference model on the chosen completions such that completions are within distribution of the model, and then train using DPO. We also compare against the best of 128 Preferred-FT completions (we found the Best of N baseline plateaus at 128 completions for this task; see Appendix Figure <ref>) and a 2-shot prompted version of the Pythia-2.8B base model, finding DPO performs as well or better for the best-performing temperatures for each method. We also evaluate an RLHF model trained with PPO on the Anthropic HH dataset [<https://huggingface.co/reciprocate/ppo_hh_pythia-6B>] from a well-known source [<https://github.com/CarperAI/trlx/tree/main/examples/hh>], but are unable to find a prompt or sampling temperature that gives performance better than the base Pythia-2.8B model. Based on our results from TL;DR and the fact that both methods optimize the same reward function, we consider Best of 128 a rough proxy for PPO-level performance. Overall, DPO is the only computationally efficient method that improves over the preferred completions in the Anthropic HH dataset, and provides similar or better performance to the computationally demanding Best of 128 baseline. Finally, Figure <ref> shows that DPO converges to its best performance relatively quickly.\n",
      "\n",
      "\n",
      "\n",
      " §.§ Generalization to a new input distribution\n",
      "\n",
      "\n",
      "r0.375\n",
      "    \n",
      "\n",
      "    \n",
      "            2cWin rate vs. ground truth \n",
      "\n",
      "        (lr)2-3\n",
      "        Alg.     Temp 0     Temp 0.25 \n",
      "\n",
      "        DPO     0.36     0.31 \n",
      "\n",
      "        PPO     0.26     0.23 \n",
      "\n",
      "        \n",
      "    GPT-4 win rates vs. ground truth summaries for out-of-distribution CNN/DailyMail input articles.\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "To further compare the performance of PPO and DPO under distribution shifts, we evaluate the PPO and DPO policies from our Reddit TL;DR summarization experiment on a different distribution, news articles in the test split of the CNN/DailyMail dataset <cit.>, using the best sampling temperatures from TL;DR (0 and 0.25). The results are presented in Table <ref>. We computed the GPT-4 win rate against the ground-truth summaries in the datasets, using the same GPT-4 (C) prompt we used for Reddit TL;DR, but replacing the words “forum post” with “news article”. For this new distribution, DPO continues to outperform the PPO policy by a significant margin. This experiment provides initial evidence that DPO policies can generalize similarly well to PPO policies, even though DPO does not use the additional unlabeled Reddit TL;DR prompts that PPO uses.\n",
      "\n",
      "\n",
      "\n",
      " §.§ Validating GPT-4 judgments with human judgments\n",
      "\n",
      "\n",
      "We conduct a human study to verify the reliability of GPT-4's judgments, using the results of the TL;DR summarization experiment and two different GPT-4 prompts. The GPT-4 (S) (simple) prompt simply asks for which summary better-summarizes the important information in the post. The GPT-4 (C) (concise) prompt also asks for which summary is more concise; we evaluate this prompt because we find that GPT-4 prefers longer, more repetitive summaries than humans do with the GPT-4 (S) prompt. See Appendix <ref> for the complete prompts. We perform three comparisons, using the highest (DPO, temp. 0.25), the lowest (PPO, temp. 1.0), and a r0.47\n",
      "    \n",
      "\n",
      "    \n",
      "        DPO     SFT     PPO-1 \n",
      "\n",
      "        (lr)2-4\n",
      "        N respondents     272     122     199 \n",
      "\n",
      "        GPT-4 (S) win %     47     27     13 \n",
      "\n",
      "        GPT-4 (C) win %     54     32     12 \n",
      "\n",
      "        Human win %     58     43     17 \n",
      "\n",
      "        GPT-4 (S)-H agree     70     77     86 \n",
      "\n",
      "        GPT-4 (C)-H agree     67     79     85 \n",
      "\n",
      "        H-H agree     65     -     87 \n",
      "\n",
      "        \n",
      "    \n",
      "\n",
      "    Comparing human and GPT-4 win rates and per-judgment agreement on TL;DR summarization samples. Humans agree with GPT-4 about as much as they agree with each other. Each experiment compares a summary from the stated method with a summary from PPO with temperature 0.\n",
      "    \n",
      "\n",
      "    \n",
      "middle-performing (SFT, temp. 0.25) method with the aim of covering a diversity of sample qualities; all three methods are compared against greedily-sampled PPO (its best-performing temperature). We find that with both prompts, GPT-4 tends to agree with humans about as often as humans agree with each other, suggesting that GPT-4 is a reasonable proxy for human evaluations (due to limited human raters, we only collect multiple human judgments for the DPO and PPO-1 comparisons). Overall, the GPT-4 (C) prompt generally provides win rates more representative of humans; we therefore use this prompt for the main results in Section <ref>. For additional details about the human study, including the web interface presented to raters and the list of human volunteers, see Appendix <ref>.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "§ DISCUSSION\n",
      "\n",
      "Learning from preferences is a powerful, scalable framework for training capable, aligned language models. We have introduced DPO, a simple training paradigm for training language models from preferences without reinforcement learning. Rather than coercing the preference learning problem into a standard RL setting in order to use off-the-shelf RL algorithms, DPO identifies a mapping between language model policies and reward functions that enables training a language model to satisfy human preferences directly, with a simple cross-entropy loss, without reinforcement learning or loss of generality. With virtually no tuning of hyperparameters, DPO performs similarly or better than existing RLHF algorithms, including those based on PPO; DPO thus meaningfully reduces the barrier to training more language models from human preferences.\n",
      "\n",
      "Limitations & Future Work. Our results raise several important questions for future work. How does the DPO policy generalize out of distribution, compared with learning from an explicit reward function? Our initial results suggest that DPO policies can generalize similarly to PPO-based models, but more comprehensive study is needed. For example, can training with self-labeling from the DPO policy similarly make effective use of unlabeled prompts? On another front, how does reward over-optimization manifest in the direct preference optimization setting, and is the slight decrease in performance in Figure <ref>-right an instance of it? Additionally, while we evaluate models up to 6B parameters, exploration of scaling DPO to state-of-the-art models orders of magnitude larger is an exciting direction for future work. Regarding evaluations, we find that the win rates computed by GPT-4 are impacted by the prompt; future work may study the best way to elicit high-quality judgments from automated systems. Finally, many possible applications of DPO exist beyond training language models from human preferences, including training generative models in other modalities.\n",
      "\n",
      "\n",
      "\n",
      "§ ACKNOWLEDGEMENTS\n",
      "\n",
      "EM gratefully acknowledges funding from a Knight-Hennessy Graduate Fellowship. CF and CM are CIFAR Fellows. This work was supported in part by the Stanford Accelerator for Learning (SAL) and Stanford Institute for Human-Centered Artificial Intelligence (HAI) Generative AI for the Future of Learning seed grant program. The Stanford Center for Research on Foundation Models (CRFM) provided part of the compute resources used for the experiments in this work. This work was supported in part by ONR grant N00014-20-1-2675.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "abbrvnat\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "§ AUTHOR CONTRIBUTIONS\n",
      "\n",
      "All authors provided valuable contributions to designing, analyzing, and iterating on experiments, writing and editing the paper, and generally managing the project’s progress.\n",
      "\n",
      "RR proposed using autoregressive reward models in discussions with EM; derived the DPO objective; proved the theoretical properties of the algorithm and wrote the relevant sections and appendices. He also suggested and helped with organizing experiments and contributed some of the PPO and reward learning baselines.\n",
      "\n",
      "AS initiated the discussion on using weighted regression methods as an alternative to PPO;\n",
      "initiated project-related organization, wrote initial analysis connecting DPO with weighted regression and unlikelihood; design and iterations of DPO + baseline implementations, initial exploratory experiments for DPO; substantial experiment organization and design (datasets, baselines, evaluation); led model training and evaluation for controlled sentiment generation and summarization; design iterations for GPT-4 evaluation (particularly summarization); substantial writing contributions to abstract, prelims/method and experiments; editing contributions to other sections.\n",
      "\n",
      "EM provided input on early discussions on learning autoregressive reward functions; wrote the first implementation of DPO and ran the first DPO experiments; trained the large-scale (summarization and dialogue) DPO models used in paper experiments; conducted initial GPT-4 win rate evaluations and set up related infrastructure; recruited participants for, conducted, and analyzed results from the human study; wrote the abstract, introduction, related work, discussion, and most of experiments; and assisted with editing the rest of the paper.\n",
      "\n",
      "\n",
      "CF, CM, & SE supervised the research, suggested ideas and experiments, and assisted in writing the paper.\n",
      "\n",
      "\n",
      "\n",
      "§ MATHEMATICAL DERIVATIONS\n",
      "\n",
      "\n",
      "\n",
      " §.§ Deriving the Optimum of the KL-Constrained Reward Maximization Objective\n",
      "\n",
      "In this appendix, we will derive Eq. <ref>. Analogously to Eq. <ref>, we optimize the following objective:\n",
      "\n",
      "    max_π𝔼_x∼𝒟, y∼π[r(x, y)] - β𝔻_KL[π(y|x)||(y|x)]\n",
      "\n",
      "under any reward function r(x,y), reference model  and a general non-parametric policy class. We now have:\n",
      "\n",
      "    max_π𝔼_x∼𝒟, y∼π   [r(x, y)] - β𝔻_KL[π(y|x)||(y|x)] \n",
      "       =max_π𝔼_x∼𝒟𝔼_y∼π(y|x)[r(x, y) - βlogπ(y|x)/(y|x)] \n",
      "       =\n",
      "    min_π𝔼_x∼𝒟𝔼_y∼π(y|x)[logπ(y|x)/(y|x) - 1/βr(x, y)] \n",
      "       =\n",
      "    min_π𝔼_x∼𝒟𝔼_y∼π(y|x)[logπ(y|x)/1/Z(x)(y|x)exp(1/βr(x, y)) - log Z(x)]\n",
      "\n",
      "\n",
      "where we have partition function:\n",
      "\n",
      "    Z(x) = ∑_y(y|x)exp(1/βr(x, y)).\n",
      "\n",
      "\n",
      "Note that the partition function is a function of only x and the reference policy , but does not depend on the policy π. We can now define\n",
      "\n",
      "    π^*(y|x) = 1/Z(x)(y|x)exp(1/βr(x, y)),\n",
      "\n",
      "\n",
      "which is a valid probability distribution as π^*(y|x)≥ 0 for all y and ∑_yπ^*(y|x)=1. Since Z(x) is not a function of y, we can then re-organize the final objective in Eq <ref> as:\n",
      "\n",
      "    min_π𝔼_x∼𝒟[𝔼_y∼π(y|x)[logπ(y|x)/π^*(y|x)] - log Z(x)]=\n",
      "    min_π𝔼_x∼𝒟[𝔻_KL(π(y|x)||π^*(y|x)) - log Z(x)]\n",
      "\n",
      "Now, since Z(x) does not depend on π, the minimum is achieved by the policy that minimizes the first KL term. Gibbs' inequality tells us that the KL-divergence is minimized at 0 if and only if the two distributions are identical. Hence we have the optimal solution:\n",
      "\n",
      "    π(y|x)= π^*(y|x) = 1/Z(x)(y|x)exp(1/βr(x, y))\n",
      "\n",
      "for all x∈𝒟. This completes the derivation.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " §.§ Deriving the DPO Objective Under the Bradley-Terry Model\n",
      "\n",
      "\n",
      "It is straightforward to derive the DPO objective under the Bradley-Terry  preference model as we have\n",
      "\n",
      "    p^*(y_1≻ y_2|x)=exp(r^*(x, y_1))/exp(r^*(x, y_1)) + exp(r^*(x, y_2))\n",
      "\n",
      "\n",
      "In Section <ref> we showed that we can express the (unavailable) ground-truth reward through its corresponding optimal policy:\n",
      "\n",
      "    r^*(x,y) =βlogπ^*(y|x)/(y|x) + βlog Z(x)\n",
      "\n",
      "\n",
      "Substituting Eq. <ref> into Eq. <ref> we obtain:\n",
      "\n",
      "    p^*(y_1≻ y_2|x)   =exp(βlogπ^*(y_1|x)/(y_1|x) + βlog Z(x))/exp(βlogπ^*(y_1|x)/(y_1|x) + βlog Z(x)) + exp(βlogπ^*(y_2|x)/(y_2|x) + βlog Z(x))\n",
      "       =\n",
      "        1/1+exp(βlogπ^*(y_2|x)/(y_2|x)-βlogπ^*(y_1|x)/(y_1|x))\n",
      "       = σ(βlogπ^*(y_1|x)/(y_1|x) - βlogπ^*(y_2|x)/(y_2|x)).\n",
      "\n",
      "\n",
      "The last line is the per-instance loss in Equation <ref>.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " §.§ Deriving the DPO Objective Under the Plackett-Luce Model\n",
      "\n",
      "\n",
      "The Plackett-Luce model <cit.> is a generalization of the Bradley-Terry model over rankings (rather than just pair-wise comparisons). Similar to to the Bradley-Terry model, it stipulates that when presented with a set of possible choices, people prefer a choice with probability proportional to the value of some latent reward function for that choice. In our context, when presented with a prompt x and a set of K answers y_1, …, y_K a user would output a permutation τ:[K]→[K], giving their ranking of the answers. The Plackett-Luce model stipulates that\n",
      "\n",
      "    p^*(τ| y_1,…, y_K, x)= ∏_k=1^Kexp(r^*(x, y_τ(k)))/∑_j=k^Kexp(r^*(x, y_τ(j)))\n",
      "\n",
      "\n",
      "Notice that when K=2, Equation <ref> reduces to the Bradley-Terry model. However, for the general Plackett-Luce model, we can still utilize the results of Eq. <ref> and substitute the reward function parameterized by its optimal policy. Similarly to Appendix <ref>, the normalization constant Z(x) cancels out and we're left with:\n",
      "\n",
      "    p^*(τ| y_1,…, y_K, x)= ∏_k=1^Kexp(βlogπ^*(y_τ(k)|x)/(y_τ(k)|x))/∑_j=k^Kexp(βlogπ^*(y_τ(j)|x)/(y_τ(j)|x))\n",
      "\n",
      "\n",
      "Similarly to the approach of Section <ref>, if we have access to a dataset 𝒟 = {τ^(i), y_1^(i), …, y_K^(i), x^(i)}_i=1^N of prompts and user-specified rankings, we can use a parameterized model and optimize this objective with maximum-likelihood.:\n",
      "\n",
      "    ℒ_DPO(π_θ, ) = -𝔼_τ, y_1, …, y_K, x∼𝒟[log∏_k=1^Kexp(βlogπ_θ(y_τ(k)|x)/(y_τ(k)|x))/∑_j=k^Kexp(βlogπ_θ(y_τ(j)|x)/(y_τ(j)|x))]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " §.§ Deriving the Gradient of the DPO Objective\n",
      "\n",
      "\n",
      "In this section we derive the gradient of the DPO objective:\n",
      "\n",
      "    ∇_θℒ_DPO(π_θ; )\n",
      "        = -∇_θ𝔼_(x, y_w, y_l)∼𝒟[logσ(βlogπ_θ(y_l|x)/(y_l|x) - βlogπ_θ(y_w|x)/(y_w|x))]\n",
      "\n",
      "\n",
      "We can rewrite the RHS of Equation <ref> as \n",
      "\n",
      "    ∇_θℒ_DPO(π_θ; )\n",
      "        =-𝔼_(x, y_w, y_l)∼𝒟[σ'(u)/σ(u)∇_θ(u)],\n",
      "\n",
      "where u = βlogπ_θ(y_l|x)/(y_l|x) - βlogπ_θ(y_w|x)/(y_w|x).\n",
      "\n",
      "Using the properties of sigmoid function σ'(x) = σ(x)(1-σ(x)) and σ(-x) = 1-σ(x), we obtain the final gradient\n",
      "\n",
      "    ∇_θℒ_DPO(π_θ; ) = \n",
      "    \n",
      "         -𝔼_(x, y_w, y_l) ∼𝒟[βσ(βlogπ_θ(y_w|x)/(y_w|x) - βlogπ_θ(y_l|x)/(y_l|x))[∇_θlogπ(y_w | x) - ∇_θlogπ(y_l | x)]],\n",
      "\n",
      "\n",
      "After using the reward substitution of r̂_θ(x, y) = βlogπ_θ(y | x)/(y | x) we obtain the final form of the gradient from Section <ref>.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " §.§ Proof of Lemma 1 and 2\n",
      "\n",
      "\n",
      "\n",
      "In this section, we will prove the two lemmas from Section <ref>.\n",
      "\n",
      "\n",
      "Lemma 1 Restated. Under the Plackett-Luce preference framework, and in particular the Bradley-Terry framework, two reward functions from the same equivalence class induce the same preference distribution.\n",
      "\n",
      "\n",
      "We say that two reward functions r(x, y) and r'(x, y) are from the same equivalence class if r'(x, y) = r(x, y) + f(x) for some function f. We consider the general Plackett-Luce (with the Bradley-Terry model a special case for K=2) and denote the probability distribution over rankings induced by a particular reward function r(x, y) as p_r. For any prompt x, answers y_1,…, y_K and ranking τ we have:\n",
      "\n",
      "    p_r'(τ| y_1,…, y_K, x)    = \n",
      "         ∏_k=1^Kexp(r'(x, y_τ(k)))/∑_j=k^Kexp(r'(x, y_τ(j)))\n",
      "       = ∏_k=1^Kexp(r(x, y_τ(k)) + f(x))/∑_j=k^Kexp(r(x, y_τ(j))+f(x))\n",
      "       = ∏_k=1^Kexp(f(x))exp(r(x, y_τ(k)))/exp(f(x))∑_j=k^Kexp(r(x, y_τ(j)))\n",
      "       = ∏_k=1^Kexp(r(x, y_τ(k)))/∑_j=k^Kexp(r(x, y_τ(j)))\n",
      "       = p_r(τ| y_1,…, y_K, x),\n",
      "\n",
      "which completes the proof.\n",
      "\n",
      "\n",
      "\n",
      "Lemma 2 Restated. Two reward functions from the same equivalence class induce the same optimal policy under the constrained RL problem.\n",
      "\n",
      "\n",
      "Let us consider two reward functions from the same class, such that r'(x, y)=r(x, y)+f(x) and, let us denote as π_r and π_r' the corresponding optimal policies. By Eq. <ref>, for all x, y we have\n",
      "\n",
      "    π_r'(y|x)    = 1/∑_y(y|x)exp(1/βr'(x, y))(y|x)exp(1/βr'(x, y)) \n",
      "       = 1/∑_y(y|x)exp(1/β(r(x, y) + f(x)))(y|x)exp(1/β(r(x, y)+f(x))) \n",
      "       = 1/exp(1/βf(x))∑_y(y|x)exp(1/βr(x, y))(y|x)exp(1/βr(x, y))exp(1/βf(x)) \n",
      "       = 1/∑_y(y|x)exp(1/βr(x, y))(y|x)exp(1/βr(x, y)) \n",
      "       = π_r(y|x),\n",
      "\n",
      "which completes the proof.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " §.§ Proof of Theorem 1\n",
      "\n",
      "\n",
      "\n",
      "In this section, we will expand on the results of Theorem <ref>. \n",
      "\n",
      "\n",
      "Theorem 1 Restated.\n",
      "    Assume, we have a reference model, such that (y|x)>0 for all pairs of prompts x and answers y and a parameter β>0. All reward equivalence classes, as defined in Section <ref> can be represented with the reparameterization r(x, y) = βlogπ(y|x)/(y|x) for some model π(y|x).\n",
      "\n",
      "\n",
      "Consider any reward function r(x,y), which induces an optimal model π_r(y|x) under the KL-constrained RL problem, with solution given by <ref>. Following Eq. <ref>, when we log-linearize both sides we obtain:\n",
      "\n",
      "    r(x,y) =βlogπ_r(y|x)/(y|x) + βlog Z(x)\n",
      "\n",
      "where Z(x) =∑_y(y|x)exp(1/βr(x, y)) (notice that Z(x) also depends on the reward function r). Using the operator r'(x, y) = f(r, , β)(x, y) = r(x, y) - βlog Z(x), we see that this new reward function is within the equivalence class of r and, we have:\n",
      "\n",
      "    r'(x,y) =βlogπ_r(y|x)/(y|x)\n",
      "\n",
      "\n",
      "which completes the proof.\n",
      "\n",
      "We can further expand on these results. We can see that if r and r' are two reward functions in the same class, then\n",
      "\n",
      "    f(r, , β)(x, y)= βlogπ_r(y|x)/(y|x)=\n",
      "    βlogπ_r'(y|x)/(y|x) = f(r', , β)(x, y)\n",
      "\n",
      "where the second equality follows from Lemma <ref>. We have proven that the operator f maps all reward functions from a particular equivalence class to the same reward function. Next, we show that for every equivalence class of reward functions, the reward function that has the reparameterization outlined in Theorem <ref> is unique.\n",
      "\n",
      "\n",
      "Assume, we have a reference model, such that (y|x)>0 for all pairs of prompts x and answers y and a parameter β>0. Then every equivalence class of reward functions, as defined in Section <ref>, has a unique reward function r(x, y), which can be reparameterized as r(x, y) = βlogπ(y|x)/(y|x) for some model π(y|x).\n",
      "\n",
      "\n",
      "    We will proceed using proof by contradiction. Assume we have two reward functions from the same class, such that r'(x, y) = r(x, y) + f(x). Moreover, assume that  r'(x, y) = βlogπ'(y|x)/(y|x) for some model π'(y|x) and  r(x, y) = βlogπ(y|x)/(y|x) for some model π(y|x), such that π≠π'. We then have\n",
      "\n",
      "    r'(x, y) = r(x, y) + f(x) = βlogπ(y|x)/(y|x) + f(x) =  βlogπ(y|x)exp(1/β f(x))/(y|x)=βlogπ'(y|x)/(y|x)\n",
      "\n",
      "\n",
      "    for all prompts x and completions y. Then we must have π(y|x)exp(1/β f(x)) = π'(y|x). Since these are distributions, summing over y on both sides, we obtain that exp(1/β f(x)) = 1 and since β>0, we must have f(x)=0 for all x. Therefore r(x,y) = r'(x,y). This completes the proof.\n",
      "\n",
      "\n",
      "We have now shown that every reward class has a unique reward function that can be represented as outlined in Theorem <ref>, which is given by f(r, , β) for any reward function in that class. \n",
      "\n",
      "\n",
      "\n",
      "§ DPO IMPLEMENTATION DETAILS AND HYPERPARAMETERS\n",
      "\n",
      "\n",
      "DPO is relatively straightforward to implement; PyTorch code for the DPO loss is provided below:\n",
      "\n",
      "\n",
      "Unless noted otherwise, we use a β = 0.1, batch size of  and the RMSprop optimizer with a learning rate of  by default. We linearly warmup the learning rate from  to  over  steps. For TL;DR summarization, we use β=0.5, while rest of the parameters remain the same.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "§ FURTHER DETAILS ON THE EXPERIMENTAL SET-UP\n",
      "\n",
      "\n",
      "In this section, we include additional details relevant to our experimental design.\n",
      "\n",
      "\n",
      " §.§ IMDb Sentiment Experiment and Baseline Details\n",
      "\n",
      "\n",
      "The prompts are prefixes from the IMDB dataset of length 2-8 tokens. We use the pre-trained sentiment classifier  as a ground-truth reward model and  as a base model. We use these larger models as we found the default ones to generate low-quality text and rewards to be somewhat inaccurate. We first use supervised fine-tuning on a subset of the IMDB data for 1 epoch. We then use this model to sample 4 completions for 25000 prefixes and create 6 preference pairs for each prefix using the ground-truth reward model. The RLHF reward model is initialized from the  model and trained for 3 epochs on the preference datasets, and we take the checkpoint with the highest validation set accuracy. The “TRL” run uses the hyper-parameters in the TRL library. Our implementation uses larger batch samples of 1024 per PPO step.\n",
      "\n",
      "\n",
      "\n",
      " §.§ GPT-4 prompts for computing summarization and dialogue win rates\n",
      "\n",
      "\n",
      "A key component of our experimental setup is GPT-4 win rate judgments. In this section, we include the prompts used to generate win rates for the summarization and dialogue experiments. We use  for all our experiments. The order of summaries or responses are randomly chosen for every evaluation.\n",
      "\n",
      "Summarization GPT-4 win rate prompt (S).\n",
      "\n",
      "\n",
      "Summarization GPT-4 win rate prompt (C).\n",
      "\n",
      "\n",
      "Dialogue GPT-4 win rate prompt.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " §.§ Unlikelihood baseline\n",
      "\n",
      "While we include the unlikelihood baseline <cit.> (simply maximizing log p(y_w|x), the log probability of the preferred response, while minimizing log p(y_l|x), the log probability of the dispreferred response) in our sentiment experiments, we do not include it as a baseline in either the summarization or dialogue experiment because it produces generally meaningless responses, which we believe is a result of unconstrained likelihood minimization. \n",
      "\n",
      "\n",
      "    \n",
      "        Prompt     Response \n",
      "\n",
      "        SUBREDDIT: r/relationships TITLE: The girl [26 F] I [22 M] have been seeing for a month didn't respond to me at all yesterday while hanging out with a friend [ 30? M]. POST: She gets terrible service while at her house, but I texted her 3 times yesterday, 4-5 hours apart. She didn't call me until early this morning and left a voicemail that she was busy all day with a friend who showed up out of the blue. I saw that she posted a picture of the two of them out of her dead zone house on facebook before I texted her the last time. I don't mind that she hangs out with friends, and I know it's pretty early […]\n",
      "        TL;DR:\n",
      "            girl when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when whenUB when when when when whenUB when when when whenUB \n",
      "\n",
      "        SUBREDDIT: r/tifu TITLE: TIFU by accidently kicking an old woman POST: So this didn't happen today but actually about a year or two ago. I was at my granddads funeral so of course it was all very sad and full of lots of crying old people. After the ceremony everyone walks outside the building and onto the other side of the small road the hearses drive down. Now the road is important because obviously if there's a road, there's a curb onto the sidewalk, so most of us are on the other side of the road, besides a few older people walking a lot slower. As one of the old woman goes to walk up the curb […] TL;DR:\n",
      "            when an old woman was tripping the when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when when \n",
      "\n",
      "        \n",
      "    \n",
      "\n",
      "    Unlikelihood samples from TL;DR prompts sampled at temperature 1.0. In general, we find unlikelihood fails to generate meaningful responses for more complex problems such as summarization and dialogue.\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "§ ADDITIONAL EMPIRICAL RESULTS\n",
      "\n",
      "\n",
      "\n",
      " §.§ Performance of Best of N baseline for Various N\n",
      "\n",
      "We find that the Best of N baseline is a strong (although computationally expensive, requiring sampling many times) baseline in our experiments. We include an evaluation of the Best of N baseline for various N for the Anthropic-HH dialogue and TL;DR summarization; the results are shown in Figure <ref>.\n",
      "\n",
      "    \n",
      "    < g r a p h i c s >\n",
      "\n",
      "    \n",
      "    < g r a p h i c s >\n",
      "\n",
      "    Best of N baseline for N={1,4,16,64,128}. Performance plateaus after roughly 64-128 samples.\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      " §.§ Sample Responses and GPT-4 Judgments\n",
      "\n",
      "In this section, we present examples of comparisons between DPO and the baseline (PPO temp 0. for summarization, and the ground truth chosen response for dialogue). See Tables <ref>-<ref> for summarization examples, and Tables <ref>-<ref> for dialogue examples.\n",
      "\n",
      "\n",
      "    \n",
      "        Prompt     SUBREDDIT: r/AskReddit\n",
      "        TITLE: I've been ungrateful to parents who have wanted only the best for me for the longest time. I am so lucky to have the life I have but I squandered it, this isn't a humblebrag about my \"wicked life\" just tell me, what can I do to make it up to them after this.\n",
      "        POST: I'm 17, leaving for University, and I've had epiphany after epiphany of how shitty i've beent to them, or how all the nice things they do I ruin with moods etc., I know there isn't a quick fix, but I want to do something nice. I've always thanked them and apologized where it is due (in earlier years not so much) but as they say, actions speak louder than words, so I was wondering what can I do now, before I go, to just make them happy. What can I keep doing, I know all the little things, doing chores, keeping my room clean, cleaning up after myself, helping around the house, but what's a big thing, for say christmas, or their birthdays, that I can do to make them happy? \n",
      "\n",
      "        DPO     TL;DR: I've been a shitty child and I want to make up for it, what can I do for my parents on their birthdays and christmas to show them how much I love them?\n",
      "\n",
      "        PPO     TL;DR: I've been a shitty person to my parents, what can I do to make it up to them, before I go, and what can I do after university to make them happy, and what can I do to make them happy after university (if that is the case) \n",
      "\n",
      "        Judgment     Summary B [DPO] more effectively captures the main points of the post by focusing on making the parents happy through gestures on special occasions, while Summary A seems repetitive and less clear. \n",
      "\n",
      "        \n",
      "    \n",
      "\n",
      "    Sample summaries to a post from the TL;DR test set. DPO sample generated with a temperature 0.25; PPO sample generated at a temperature of 0. The order in which summaries are presented in randomized when evaluating with GPT-4, so the order in the Judgment may not correspond to the order in the table. For clarity, post-hoc annotations are included in bold, formatted as [annotation]. These annotations are not part of the model generations.\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "        Prompt     SUBREDDIT: r/relationships\n",
      "        TITLE: I [22 M] am about to propose to my girlfriend [22F] of almost 2 years, and I can't calm myself down\n",
      "        POST: Alright, so I am preparing to propose to my girlfriend of almost 2 years \n",
      "        (we were friends for double that beforehand) in a couple days and I am starting to get EXTREMELY anxious/nervous. We have talked about this for months, and I know she will say yes, but I can't seem to relax. \n",
      "\n",
      "        The other day I bought the ring, which I fell in love with myself and know she will too. And then two days ago, I asked her dad's blessing. Everything has gone great so far, but now leading up to it I can't think about anything else. I can't sleep. I am anxious as hell, and all I want to do is pop the question now, but I have to wait since I don't see her until Thursday. \n",
      "\n",
      "        I know how I am going to do it. I have thoroughly planned it out and I am more than prepared. But, at the same time, I am freaking out. I asked my best friend for advice since he just got married recently, and all he could say was \"It will feel 10x worse right before you do ask.\" \n",
      "\n",
      "        Any suggestions on how to relax leading up to it? I would like to be able to sleep... \n",
      "\n",
      "        DPO     TL;DR: About to propose to my girlfriend of almost 2 years, but I am extremely anxious and can't sleep. How can I relax?\n",
      "\n",
      "        PPO     TL;DR: I am about to ask my girlfriend of almost 2 years to marry me, and I can't calm myself down. Any tips on how to relax? I would like to be able to sleep tonight (and the night before).............................................................................................................................. \n",
      "\n",
      "        Judgment     Summary A [DPO] more effectively summarizes the main points of the post, concisely conveying the asker's anxiety and goal of finding ways to relax. \n",
      "\n",
      "        \n",
      "    \n",
      "\n",
      "    Sample summaries to a post from the TL;DR test set. DPO sample generated with a temperature 0.25; PPO sample generated at a temperature of 0. The order in which summaries are presented in randomized when evaluating with GPT-4, so the order in the Judgment may not correspond to the order in the table. For clarity, post-hoc annotations are included in bold, formatted as [annotation]. These annotations are not part of the model generations.\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "        Prompt     SUBREDDIT: r/loseit\n",
      "        TITLE: Is It Bullshit?: Fiber Gourmet Pasta\n",
      "        POST: I was looking for low-cal alternatives to pasta, because I love pasta and it's easy to make and I eat it several times a week. I find that whole grain pasta has a weird taste/texture, and I'm not a big fan of it. \n",
      "        \n",
      "        I was doing some research into spaghetti squash (which is on my grocery list for next time), but I also heard someone rave about some high-fiber pasta brand so I looked into it.\n",
      "        \n",
      "        What sorcery is this? It has all the trappings of regular pasta (and the reviews I've read say it tastes the same too) but a little over half the calories. My mind boggles over how much extra pasta I could eat! I can't believe this hasn't taken the world by storm, which makes me wonder what the catch is.\n",
      "         \n",
      "\n",
      "        DPO     TL;DR: I'm trying to cut back on calories and pasta is one of my main sources. Found a high-fiber pasta that has all the trappings of regular pasta and seems like it would be a good substitute. Is it bullshit?\n",
      "\n",
      "        PPO     TL;DR: I want to lose weight, but I love pasta and don't want to give it up. Is there a high-fiber pasta that I can eat more of and lose weight with, or is it all bullshit?\n",
      "\n",
      "        Judgment     Summary B [DPO] more accurately captures the intent to find a low-calorie pasta alternative, without assuming the need for weight loss. \n",
      "\n",
      "        \n",
      "    \n",
      "\n",
      "    Sample summaries to a post from the TL;DR test set. DPO sample generated with a temperature 0.25; PPO sample generated at a temperature of 0. The order in which summaries are presented in randomized when evaluating with GPT-4, so the order in the Judgment may not correspond to the order in the table. For clarity, post-hoc annotations are included in bold, formatted as [annotation]. These annotations are not part of the model generations.\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "        Prompt     Can you help me write an essay on the Civil Rights Movement? \n",
      "\n",
      "        DPO     The Civil Rights Movement was a series of social and political movements that took place between the 1950s and the 1980s, leading to the integration of public spaces, higher education, employment, and voting rights.  The movement is often credited with the end of legal segregation in the United States, and it was a major catalyst for the Civil Rights Act of 1964.  The Movement also played a large role in the rise of the Black Power and Civil Rights Movements, which focused on economic equality and racial justice.  The Movement focused on the rights of African Americans, who were then the dominant racial and ethnic group in the United States, but it also highlighted the rights of other minority groups, such as women, immigrants, and LGBT people.  The Movement is often considered one of the most important social movements of the 20th century, and it played a significant role in the advancement of civil rights for all Americans.  I’ll be happy to write an essay on this topic for you.  What kind of essay would you like?  A paper on the history of the Civil Rights Movement?  A paper on the causes of the Civil Rights Movement?  Or a paper on the effects of the Civil Rights Movement?  Any ideas?  I’ll be happy to [maximum generation length reached] \n",
      "\n",
      "        GT     Oh, I’m not sure if I can actually write an essay, but I can at least help you think through the process.  First, can you tell me what you’re trying to write about?  In other words, what are the main points or questions you want to address? \n",
      "\n",
      "        Judgment     Response B [DPO] provides more detailed information about the Civil Rights Movement and offers specific suggestions for essay topics, making it more helpful for someone writing an essay. \n",
      "\n",
      "        \n",
      "    \n",
      "\n",
      "    GPT-4 chooses DPO over GT. Sample responses to a prompt from the Anthropic-HH test set. DPO sample generated with temperature 0.7; GT is the chosen completion in the dataset of preferences. For clarity, post-hoc annotations are included in bold, formatted as [annotation]. These annotations are not part of the model generations.\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "        Prompt     I am trying to locate the residential address for J.B. Pritzker, an hier [sic] to the Hyatt Hotel fortune. \n",
      "\n",
      "        DPO     I am not able to assist you in this matter.  It’s unlawful and irresponsible to locate private information on people without their consent, and I’m not aware of any way you could do this for human.  Please stop this activity. \n",
      "\n",
      "        GT     J.B. Pritzker is one of the co-founders and investors in Hyatt.  He and his family have been listed in the Forbes 400, and the family also maintains a 4,000-acre ranch in Colorado. \n",
      "\n",
      "        Judgment     Response A [DPO] emphasizes the importance of privacy and laws, while Response B gives background information on J.B. Pritzker without providing his address. \n",
      "\n",
      "        \n",
      "    \n",
      "\n",
      "    GPT-4 chooses DPO over GT. Sample responses to a prompt from the Anthropic-HH test set. DPO sample generated with temperature 1.0; GT is the chosen completion in the dataset of preferences. For clarity, post-hoc annotations are included in bold, formatted as [annotation]. These annotations are not part of the model generations.\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "        Prompt     Can you tell me what got the USA involved in World War II? \n",
      "\n",
      "        DPO     In the 1930s, the United States found itself at the center of a growing political and economic crisis that threatened the world’s stability.  The Great Depression had brought many other countries to the brink, including many European ones, and the United States found itself in a very precarious position.  The Great Depression had left the government and the economy on the edge of collapse, it faced severe international indebtedness, and it faced the threat of a foreign default on its loans.  When Japan attacked China in October 1937, the United States found itself in an especially dangerous military position, as the United States was the largest creditor.\n",
      "        \n",
      "        The United States found itself desperately seeking ways to avoid a complete collapse, and two main strategies emerged: both involved creating an alliance of nations to protect American economic security and stability, as well as ensuring that the United States remained politically relevant.  The first strategy was to create a “coalition of the willing”, a network of international alliances to counteract the isolation and vulnerability of countries like Japan.  The second strategy was to form an “all-inclusive association”, a group of nations that the United States would be the most powerful member of, and which would protect the prosperity of the United States and protect it from other powers [maximum generation length reached] \n",
      "\n",
      "        GT     The United States entered the war against Japan in response to the Japanese attack on Pearl Harbor, Hawaii, on December 7, 1941. This was a full month before the United States officially entered the war. At this point, the German Axis powers controlled France, Belgium, Norway, and other parts of Europe, and held significant military influence in much of Asia. President Roosevelt officially asked Congress for a declaration of war against Japan after the bombing of Pearl Harbor. \n",
      "\n",
      "        Judgment     Response B [GT] provides a direct and accurate answer to the question, while Response A is overly complicated and doesn't provide the correct answer. \n",
      "\n",
      "        \n",
      "    \n",
      "\n",
      "    GPT-4 chooses GT over DPO. DPO's response is verbose and plausible, but contains factually incorrect information (the `coalition of the willing' does not refer to events of WWII; the `all-inclusive association' is not a real organization).\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "        Prompt     what is 7 plus 2 \n",
      "\n",
      "        DPO     Hm, let me see... seven plus two is nine, I’m pretty sure.  I’ll be happy to read the input aloud and double check if I’ve got it right.  Let me try:\n",
      "\n",
      "\"7 + 2\"\n",
      "\n",
      "I’ve got that right!  I think our input was meant to be seven, plus two.  You can try it yourself:\n",
      "\n",
      "\"7 + 2\"\n",
      "\n",
      "\"9\"\n",
      "\n",
      "I’ve got that right!  Now, what I’m wondering is if you want to learn this formula for yourself, or if you want me to help you practice it.  The formula is called the “arithmetic-prefix method”, and it’s a great way to memorize numbers – and in fact I personally learned to count by memorizing the arithmatic prefixes!  \n",
      "I’ll give you a refresher on arithmatic prefixes: you can think of them as an abbreviated way of describing a number by its first two significant digits.  There are three arithmatic prefixes:\n",
      "\n",
      "The “1-arithmatic prefix” (1-A)\n",
      "The “2-arithmatic prefix” (2-A) \n",
      "\n",
      "        GT     11 \n",
      "\n",
      "        Judgment     Response B [GT] provides a direct and accurate answer to the question, while Response A is overly complicated and doesn't provide the correct answer. \n",
      "\n",
      "        \n",
      "    \n",
      "\n",
      "    GPT-4 chooses GT over DPO. GPT-4 incorrectly states that the ground truth is correct while DPO's (more verbose) output is wrong.\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " §.§ Human study details\n",
      "\n",
      "\n",
      "r0.57\n",
      "    \n",
      "\n",
      "    \n",
      "    < g r a p h i c s >\n",
      "\n",
      "    Layout of the survey in SurveyMonkey. Each respondent completed 25 similarly-formatted judgments.\n",
      "    \n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "In order to validate the usage of GPT-4 for computing win rates, our human study collects human preference data for several matchups in the TL;DR summarization setting. We select three different algorithmic matchups, evaluating DPO (temp. 0.25), SFT (temp. 0.25), and PPO (temp 1.0) compared to the reference algorithm PPO (temp 0.). By selecting matchups for three unique algorithms as well as algorithms with a wide range of win rates vs the reference, we capture the similarity of human and GPT-4 win rates across the response quality spectrum. We sample 150 random comparisons of DPO vs PPO-0 and 100 random comparisons PPO-1 vs PPO-0, assigning two humans to each comparison, producing 275 judgments for DPO-PPO[One volunteer did not respond for the DPO-PPO comparison.] and 200 judgments for PPO-PPO. We sample 125 SFT comparisons, assigning a single human to each. We ignore judgments that humans labeled as ties (which amount to only about 1% of judgments), and measure the raw agreement percentage between human A and human B (for comparisons where we have two human annotators, i.e., not SFT) as well as between each human and GPT-4.\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "Participants. We have 25 volunteer human raters in total, each comparing 25 summaries (one volunteer completed the survey late and was not included in the final analysis, but is listed here). The raters were Stanford students (from undergrad through Ph.D.), or recent Stanford graduates or visitors, with a STEM (mainly CS) focus. See Figure <ref> for a screenshot of the survey interface. We gratefully acknowledge the contribution of each of our volunteers, listed in random order:\n",
      "\n",
      "\n",
      "\n",
      "1. Gordon Chi             2. Virginia Adams          3. Max Du                 4. Kaili Huang        \n",
      "\n",
      "5. Ben Prystawski         6. Ioanna Vavelidou        7. Victor Kolev           8. Karel D'Oosterlinck\n",
      "\n",
      "9. Ananth Agarwal         10. Tyler Lum              11. Mike Hardy            12. Niveditha Iyer         \n",
      "\n",
      "13. Helena Vasconcelos    14. Katherine Li           15. Chenchen Gu           16. Moritz Stephan    \n",
      "\n",
      "17. Swee Kiat Lim         18. Ethan Chi              19. Kaien Yang            20. Ryan Chi          \n",
      "\n",
      "21. Joy Yun               22. Abhay Singhal          23. Siyan Li              24. Amelia Hardy      \n",
      "\n",
      "25. Zhengxuan Wu                                                                                     \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Clean arxiv_paper via. LLM-generated regex-filtering code\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_latex_to_markdown(text):\n",
    "    \"\"\"\n",
    "    Clean LaTeX formatting and convert to markdown while preserving structure.\n",
    "    \"\"\"\n",
    "    # Store the original text for processing\n",
    "    cleaned_text = text\n",
    "    \n",
    "    # 1. Remove citation brackets like [@author2020paper; @another2021paper]\n",
    "    cleaned_text = re.sub(r'\\[@[^\\]]+\\]', '', cleaned_text)\n",
    "    \n",
    "    # 2. Remove LaTeX labels\n",
    "    cleaned_text = re.sub(r'\\\\label\\{[^}]+\\}', '', cleaned_text)\n",
    "    \n",
    "    # 3. Clean up equation references - convert complex refs to simple format\n",
    "    # Pattern like: Eq.[\\[eq:RL\\]](#eq:RL){reference-type=\"ref\" reference=\"eq:RL\"}\n",
    "    cleaned_text = re.sub(\n",
    "        r'Eq\\.\\[\\\\?\\[([^\\]]+)\\]\\]\\([^)]*\\)\\{[^}]*\\}', \n",
    "        r'Eq. (\\1)', \n",
    "        cleaned_text\n",
    "    )\n",
    "    \n",
    "    # 4. Clean up section references\n",
    "    # Pattern like: Section[\\[sec:theory\\]](#sec:theory){reference-type=\"ref\" reference=\"sec:theory\"}\n",
    "    cleaned_text = re.sub(\n",
    "        r'Section\\[\\\\?\\[([^\\]]+)\\]\\]\\([^)]*\\)\\{[^}]*\\}', \n",
    "        r'Section \\1', \n",
    "        cleaned_text\n",
    "    )\n",
    "    \n",
    "    # 5. Clean up appendix references \n",
    "    # Pattern like: Appendix[\\[app:derivation1\\]](#app:derivation1){reference-type=\"ref\" reference=\"app:derivation1\"}\n",
    "    cleaned_text = re.sub(\n",
    "        r'Appendix\\[\\\\?\\[([^\\]]+)\\]\\]\\([^)]*\\)\\{[^}]*\\}', \n",
    "        r'Appendix', \n",
    "        cleaned_text\n",
    "    )\n",
    "    \n",
    "    # 6. Clean up figure references\n",
    "    # Pattern like: Figure[\\[fig:frontier-tldr-main\\]](#fig:frontier-tldr-main){reference-type=\"ref\" reference=\"fig:frontier-tldr-main\"}\n",
    "    cleaned_text = re.sub(\n",
    "        r'Figure\\[\\\\?\\[([^\\]]+)\\]\\]\\([^)]*\\)\\{[^}]*\\}', \n",
    "        r'Figure \\1', \n",
    "        cleaned_text\n",
    "    )\n",
    "    \n",
    "    # 7. Clean up table references\n",
    "    cleaned_text = re.sub(\n",
    "        r'Table\\[\\\\?\\[([^\\]]+)\\]\\]\\([^)]*\\)\\{[^}]*\\}', \n",
    "        r'Table \\1', \n",
    "        cleaned_text\n",
    "    )\n",
    "    \n",
    "    # 8. Remove complex figure blocks but preserve descriptive text\n",
    "    # Pattern like: ![ **optimizes for human preferences...] \n",
    "    figure_pattern = r'!\\[\\s*\\*\\*([^*]+)\\*\\*([^\\]]*)\\]'\n",
    "    cleaned_text = re.sub(figure_pattern, r'**Figure: \\1**\\2', cleaned_text)\n",
    "    \n",
    "    # 9. Clean up LaTeX text formatting commands\n",
    "    cleaned_text = re.sub(r'\\\\text\\{([^}]+)\\}', r'\\1', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\\\textrm\\{([^}]+)\\}', r'\\1', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\\\textit\\{([^}]+)\\}', r'*\\1*', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\\\textbf\\{([^}]+)\\}', r'**\\1**', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\\\emph\\{([^}]+)\\}', r'*\\1*', cleaned_text)\n",
    "    \n",
    "    # 10. Clean up LaTeX math commands in text (but preserve $$ blocks)\n",
    "    # Remove \\mid and similar in inline contexts\n",
    "    cleaned_text = re.sub(r'\\\\mid(?![^$]*\\$\\$)', '|', cleaned_text)\n",
    "    \n",
    "    # 11. Remove LaTeX section numbering artifacts\n",
    "    cleaned_text = re.sub(r'\\{#[^}]+\\}', '', cleaned_text)\n",
    "    \n",
    "    # 12. Clean up reference artifacts like {reference-type=\"ref\" reference=\"...\"}\n",
    "    cleaned_text = re.sub(r'\\{[^}]*reference-type[^}]*\\}', '', cleaned_text)\n",
    "    \n",
    "    # 13. Clean up footnote markers like [^2], [^3] etc.\n",
    "    cleaned_text = re.sub(r'\\[\\^[0-9]+\\]', '', cleaned_text)\n",
    "    \n",
    "    # 14. Remove LaTeX environments that aren't math\n",
    "    # Remove \\begin{...} and \\end{...} for non-math environments\n",
    "    cleaned_text = re.sub(r'\\\\begin\\{(?!equation|align|gather)[^}]+\\}', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\\\end\\{(?!equation|align|gather)[^}]+\\}', '', cleaned_text)\n",
    "    \n",
    "    # 15. Clean up definition and lemma blocks - convert to markdown\n",
    "    # Pattern like: ::: definition ... :::\n",
    "    cleaned_text = re.sub(r':::\\s*definition\\s*\\n\\*\\*Definition\\s+(\\d+)\\*\\*\\.([^:]+):::', \n",
    "                         r'**Definition \\1:** \\2', cleaned_text, flags=re.DOTALL)\n",
    "    \n",
    "    cleaned_text = re.sub(r':::\\s*\\{#[^}]+\\s+\\.lemma\\}\\s*\\n\\*\\*Lemma\\s+(\\d+)\\*\\*\\.([^:]+):::', \n",
    "                         r'**Lemma \\1:** \\2', cleaned_text, flags=re.DOTALL)\n",
    "    \n",
    "    # 16. Remove remaining LaTeX artifacts\n",
    "    cleaned_text = re.sub(r'\\\\[a-zA-Z]+\\*?', '', cleaned_text)  # Remove LaTeX commands\n",
    "    cleaned_text = re.sub(r'\\{[^}]*\\}(?![^$]*\\$\\$)', '', cleaned_text)  # Remove remaining braces outside math\n",
    "    \n",
    "    # 17. Clean up multiple spaces and empty lines\n",
    "    cleaned_text = re.sub(r' +', ' ', cleaned_text)  # Multiple spaces to single\n",
    "    cleaned_text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', cleaned_text)  # Multiple newlines to double\n",
    "    \n",
    "    # 18. Fix any broken markdown headers\n",
    "    cleaned_text = re.sub(r'^#+\\s*$', '', cleaned_text, flags=re.MULTILINE)\n",
    "    \n",
    "    return cleaned_text.strip()\n",
    "\n",
    "# Clean the text\n",
    "cleaned_paper = clean_latex_to_markdown(latex_content)\n",
    "print(cleaned_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Introduction\n",
      "\n",
      "Large unsupervised language models (LMs) trained on very large datasets acquire surprising capabilities . However, these models are trained on data generated by humans with a wide variety of goals, priorities, and skillsets. Some of these goals and skillsets may not be desirable to imitate; for example, while we may want our AI coding assistant to *understand* common programming mistakes in order to correct them, nevertheless, when generating code, we would like to bias our model toward the (potentially rare) high-quality coding ability present in its training data. Similarly, we might want our language model to be *aware* of a common misconception believed by 50% of people, but we certainly do not want the model to claim this misconception to be true in 50% of queries about it! In other words, selecting the model's *desired responses and behavior* from its very wide *knowledge and abilities* is crucial to building AI systems that are safe, performant, and controllable . While existing methods typically steer LMs to match human preferences using reinforcement learning (RL), we will show that the RL-based objective used by existing methods can be optimized exactly with a simple binary cross-entropy objective, greatly simplifying the preference learning pipeline.\n",
      "\n",
      "**Figure: optimizes for human preferences while avoiding reinforcement learning.** Existing methods for fine-tuning language models with human feedback first fit a reward model to a dataset of prompts and human preferences over pairs of responses, and then use RL to find a policy that maximizes the learned reward. In contrast, directly optimizes for the policy best satisfying the preferences with a simple classification objective, fitting an *implicit* reward model whose corresponding optimal policy can be extracted in closed form.(figures/diagrams/teaser.png) [] At a high level, existing methods instill the desired behaviors into a language model using curated sets of human preferences representing the types of behaviors that humans find safe and helpful. This preference learning stage occurs after an initial stage of large-scale unsupervised pre-training on a large text dataset. While the most straightforward approach to preference learning is supervised fine-tuning on human demonstrations of high quality responses, the most successful class of methods is reinforcement learning from human (or AI) feedback (RLHF/RLAIF; ). RLHF methods fit a reward model to a dataset of human preferences and then use RL to optimize a language model policy to produce responses assigned high reward without drifting excessively far from the original model. While RLHF produces models with impressive conversational and coding abilities, the RLHF pipeline is considerably more complex than supervised learning, involving training multiple LMs and sampling from the LM policy in the loop of training, incurring significant computational costs.\n",
      "\n",
      "In this paper, we show how to directly optimize a language model to adhere to human preferences, without explicit reward modeling or reinforcement learning. We propose *()*, an algorithm that implicitly optimizes the same objective as existing RLHF algorithms (reward maximization with a KL-divergence constraint) but is simple to implement and straightforward to train. Intuitively, the update increases the relative log probability of preferred to dispreferred responses, but it incorporates a dynamic, per-example importance weight that prevents the model degeneration that we find occurs with a naive probability ratio objective. Like existing algorithms, relies on a theoretical preference model (such as the Bradley-Terry model; ) that measures how well a given reward function aligns with empirical preference data. However, while existing methods use the preference model to define a preference loss to train a reward model and then train a policy that optimizes the learned reward model, uses a change of variables to define the preference loss as a function of the policy directly. Given a dataset of human preferences over model responses, can therefore optimize a policy using a simple binary cross entropy objective, producing the optimal policy to an implicit reward function fit to the preference data.\n",
      "\n",
      "Our main contribution is (), a simple RL-free algorithm for training language models from preferences. Our experiments show that is at least as effective as existing methods, including PPO-based RLHF, for learning from preferences in tasks such as sentiment modulation, summarization, and dialogue, using language models with up to 6B parameters.\n",
      "\n",
      "# Related Work\n",
      "\n",
      "Self-supervised language models of increasing scale learn to complete some tasks zero-shot or with few-shot prompts . However, their performance on downstream tasks and alignment with user intent can be significantly improved by fine-tuning on datasets of instructions and human-written completions . This 'instruction-tuning' procedure enables LLMs to generalize to instructions outside of the instruction-tuning set and generally increase their usability . Despite the success of instruction tuning, *relative* human judgments of response quality are often easier to collect than expert demonstrations, and thus subsequent works have fine-tuned LLMs with datasets of human preferences, improving proficiency in translation , summarization , story-telling , and instruction-following . These methods first optimize a neural network reward function for compatibility with the dataset of preferences under a preference model such as the Bradley-Terry model , then fine-tune a language model to maximize the given reward using reinforcement learning algorithms, commonly REINFORCE , proximal policy optimization (PPO; ), or variants . A closely-related line of work leverages LLMs fine-tuned for instruction following with human feedback to generate additional synthetic preference data for targeted attributes such as safety or harmlessness , using only weak supervision from humans in the form of a text rubric for the LLM's annotations. These methods represent a convergence of two bodies of work: one body of work on training language models with reinforcement learning for a variety of objectives  and another body of work on general methods for learning from human preferences . Despite the appeal of using relative human preferences, fine-tuning large language models with reinforcement learning remains a major practical challenge; this work provides a theoretically-justified approach to optimizing relative preferences without RL.\n",
      "\n",
      "Outside of the context of language, learning policies from preferences has been studied in both bandit and reinforcement learning settings, and several approaches have been proposed. Contextual bandit learning using preferences or rankings of actions, rather than rewards, is known as a contextual dueling bandit (CDB; ). In the absence of absolute rewards, theoretical analysis of CDBs substitutes the notion of an optimal policy with a *von Neumann winner*, a policy whose expected win rate against *any* other policy is at least 50% . However, in the CDB setting, preference labels are given online, while in learning from human preferences, we typically learn from a fixed batch of offline preference-annotated action pairs . Similarly, *preference-based RL* (PbRL) learns from binary preferences generated by an *unknown* 'scoring' function rather than rewards . Various algorithms for PbRL exist, including methods that can reuse off-policy preference data, but generally involve first explicitly estimating the latent scoring function (i.e. the reward model) and subsequently optimizing it . We instead present a single stage policy learning approach that directly optimizes a policy to satisfy preferences.\n",
      "\n",
      "# Preliminaries We review the RLHF pipeline in @ziegler2020finetuning (and later ). It usually includes three phases: 1) supervised fine-tuning (SFT); 2) preference sampling and reward learning and 3) RL optimization.\n",
      "\n",
      "**SFT**: RLHF typically begins by fine-tuning a pre-trained LM with supervised learning on high-quality data for the downstream task(s) of interest (dialogue, summarization, etc.), to obtain a model $^SFT$.\n",
      "\n",
      "**Reward Modelling Phase**: In the second phase the SFT model is prompted with prompts $x$ to produce pairs of answers $(y_1, y_2) ^SFT(y | x)$. These are then presented to human labelers who express preferences for one answer, denoted as $y_w y_l | x$ where $y_w$ and $y_l$ denotes the preferred and dispreferred completion amongst $(y_1, y_2)$ respectively. The preferences are assumed to be generated by some latent reward model $r^*(y, x)$, which we do not have access to. There are a number of approaches used to model preferences, the Bradley-Terry (BT) model being a popular choice (although more general Plackett-Luce ranking models are also compatible with the framework if we have access to several ranked answers). The BT model stipulates that the human preference distribution $p^*$ can be written as: $$ p^*(y_1 y_2 x)={(r^*(x, y_1))}{(r^*(x, y_1)) + (r^*(x, y_2))}.$$ Assuming access to a static dataset of comparisons $=\\, y_w^, y_l^\\}_^N$ sampled from $p^*$, we can parametrize a reward model $r_(x, y)$ and estimate the parameters via maximum likelihood. Framing the problem as a binary classification we have the negative log-likelihood loss: $$ {L}_R(r_{}, {D}) = -{E}_{(x, y_w, y_l) {D}}[ (r_{}(x, y_w)- r_{}(x, y_l))]$$ where $$ is the logistic function. In the context of LMs, the network $r_(x, y)$ is often initialized from the SFT model $^SFT(y | x)$ with the addition of a linear layer on top of the final transformer layer that produces a single scalar prediction for the reward value . To ensure a reward function with lower variance, prior works normalize the rewards, such that $_}[r_(x, y)] = 0$ for all $x$.\n",
      "\n",
      "**RL Fine-Tuning Phase**: During the RL phase, the learned reward function is used to provide feedback to the language model. Following prior works , the optimization is formulated as $$ _{_{}} {E}_{x {D}, y _{}(y x)}[r_{}(x, y)] - {D}_{KL}[_{}(y x) _ref(y x)],$$ where $$ is a parameter controlling the deviation from the base reference policy $_ref$, namely the initial SFT model $^SFT$. In practice, the language model policy $_$ is also initialized to $^SFT$. The added constraint is important, as it prevents the model from deviating too far from the distribution on which the reward model is accurate, as well as maintaining the generation diversity and preventing mode-collapse to single high-reward answers. Due to the discrete nature of language generation, this objective is not differentiable and is typically optimized with reinforcement learning. The standard approach\n",
      "\n",
      "has been to construct the reward function $(x, y) - ( _(y| x) - _ref(y| x))}$, and maximize using PPO .\n",
      "\n",
      "# Direct Preference Optimization Motivated by the challenges of applying reinforcement learning algorithms on large-scale problems such as fine-tuning language models, our goal is to derive a simple approach for policy optimization using preferences directly. Unlike prior RLHF methods, which learn a reward and then optimize it via RL, our approach leverages a particular choice of reward model parameterization that enables extraction of its optimal policy in closed form, without an RL training loop. As we will describe next in detail, our key insight is to leverage an analytical mapping from reward functions to optimal policies, which enables us to transform a loss function over reward functions into a loss function over policies. This change-of-variables approach avoids fitting an explicit, standalone reward model, while still optimizing under existing models of human preferences, such as the Bradley-Terry model. In essence, the policy network represents both the language model and the (implicit) reward.\n",
      "\n",
      "**Deriving the DPO objective.** We start with the same RL objective as prior work, Eq. [\\[eq:RL\\]](#eq:RL), under a general reward function $r$. Following prior work , it is straightforward to show that the optimal solution to the KL-constrained reward maximization objective in Eq. [\\[eq:RL\\]](#eq:RL) takes the form: $$ _r(y x) = {1}{Z(x)}_ref(y x)({1}{}r(x, y)),$$ where $Z(x) =__ref(y| x)(r(x, y))$ is the partition function. See Appendix [\\[app:derivation1\\]](#app:derivation1) for a complete derivation. Even if we use the MLE estimate $r_$ of the ground-truth reward function $r^*$, it is still expensive to estimate the partition function $Z(x)$ , which makes this representation hard to utilize in practice. However, we can rearrange Eq. [\\[eq:op_policy\\]](#eq:op_policy) to express the reward function in terms of its corresponding optimal policy $_r$, the reference policy $_ref$, and the unknown partition function $Z()$. Specifically, we first take the logarithm of both sides of Eq. [\\[eq:op_policy\\]](#eq:op_policy) and then with some algebra we obtain: $$ r(x,y) = {_r(y x)}{_ref(y x)} + Z(x).$$ We can apply this reparameterization to the ground-truth reward $r^*$ and corresponding optimal model $^*$. Fortunately, the Bradley-Terry model depends only on the difference of rewards between two completions, i.e., $$. Substituting the reparameterization in Eq. [\\[eq:main_eq\\]](#eq:main_eq) for $r^*(x,y)$ into the preference model Eq. [\\[eq:bradley-terry\\]](#eq:bradley-terry), the partition function cancels, and we can express the human preference probability in terms of only the optimal policy $^*$ and reference policy $_ref$. Thus, the optimal RLHF policy $^*$ under the Bradley-Terry model satisfies the preference model: $$ p^*(y_1 y_2 x)={1}{1 + ( {^*(y_2 x)}{_ref(y_2 x)} - {^*(y_1 x)}{_ref(y_1 x)})}$$ The derivation is in Appendix [8.2](#app:derivation2). While Eq. [\\[eq:objective\\]](#eq:objective) uses the Bradley-Terry model, we can similarly derive expressions under the more general Plackett-Luce models , shown in Appendix [8.3](#app:plackett_luce_models).\n",
      "\n",
      "Now that we have the probability of human preference data in terms of the optimal policy rather than the reward model, we can formulate a maximum likelihood objective for a parametrized policy $_$. Analogous to the reward modeling approach (i.e. Eq. [\\[eq:reward_model\\]](#eq:reward_model)), our policy objective becomes: $$ {L}_DPO(_{}; _ref) = -{E}_{(x, y_w, y_l) {D}}[ ( {_{}(y_w x)}{_ref(y_w x)} - {_{}(y_l x)}{_ref(y_l x)})].$$ This way, we fit an implicit reward using an alternative parameterization, whose optimal policy is simply $_$. Moreover, since our procedure is equivalent to fitting a reparametrized Bradley-Terry model, it enjoys certain theoretical properties, such as consistencies under suitable assumption of the preference data distribution . In Section [\\[sec:theory\\]](#sec:theory), we further discuss theoretical properties of DPO in relation to other works.\n",
      "\n",
      "**What does the DPO update do?** For a mechanistic understanding of DPO, it is useful to analyze the gradient of the loss function $_DPO$. The gradient with respect to the parameters $$ can be written as: $$\n",
      "\n",
      " _ _DPO(_;_ref) = \\\\ -_} [_(x, y_l) - _ (x, y_w))}_higher weight when reward estimate is wrong[_increase likelihood of $y_w$ - _decrease likelihood of $y_l$]],{gathered}$$ where $_(x, y) = $ is the reward implicitly defined by the language model $_$ and reference model $_ref$ (more in Section [\\[sec:theory\\]](#sec:theory)). Intuitively, the gradient of the loss function $_DPO$ increases the likelihood of the preferred completions $y_w$ and decreases the likelihood of dispreferred completions $y_l$. Importantly, the examples are weighed by how much higher the implicit reward model $_$ rates the dispreferred completions, scaled by $$, i.e, how incorrectly the implicit reward model orders the completions, accounting for the strength of the KL constraint. Our experiments suggest the importance of this weighting, as a naïve version of this method without the weighting coefficient can cause the language model to degenerate (Appendix Table [1](#tab:unlikelihood_generations)).\n",
      "\n",
      "**DPO outline.** The general DPO pipeline is as follows: 1) Sample completions $y_1, y_2 _ref( | x)$ for every prompt $x$, label with human preferences to construct the offline dataset of preferences $ = \\, y_w^, y_l)^\\}_^N$ and 2) optimize the language model $_$ to minimize $_DPO$ for the given $_ref$ and $$ and desired $$. In practice, one would like to reuse preference datasets publicly available, rather than generating samples and gathering human preferences. Since the preference datasets are sampled using $^SFT$, we initialize $_ref= ^SFT$ whenever available. However, when $^SFT$ is not available, we initialize $_ref$ by maximizing likelihood of preferred completions $$, that is, $}__}[ (y_w | x)]}$. This procedure helps mitigate the distribution shift between the true reference distribution which is unavailable, and $_ref$ used by DPO. Further details related to the implementation and hyperparameters can be found in Appendix [9](#app:implementation).\n",
      "\n",
      "# Theoretical Analysis of DPO\n",
      "\n",
      "In this section, we give further interpretation of the DPO method, provide theoretical backing, and relate advantages of DPO to issues with actor critic algorithms used for RLHF (such as PPO ).\n",
      "\n",
      "[] ## Your Language Model Is Secretly a Reward Model\n",
      "\n",
      "DPO is able to bypass both fitting an explicit reward and performing RL to learn the policy using a single maximum likelihood objective. Note the optimization objective Eq. [\\[eq:main_eq\\]](#eq:main_eq) is equivalent to a Bradley-Terry model with a reward parameterization $r^*(x, y) = $ and we optimize our parametric model $_$, equivalently to the reward model optimization in Eq. [\\[eq:reward_model\\]](#eq:reward_model) under the change of variables. In this section we will build the theory behind this reparameterization, show that it does not constrain the class of learned reward models, and allows for the exact recovery of the optimal policy. We begin with by defining an equivalence relation between reward functions.\n",
      "\n",
      "**Definition 1:** *We say that two reward functions $r(x, y)$ and $r'(x, y)$ are equivalent iff $$ for some function $f$.*\n",
      "\n",
      "It is easy to see that this is indeed an equivalence relation, which partitions the set of reward functions into classes. We can state the following two lemmas:\n",
      "\n",
      "**Lemma 1:** *Under the Plackett-Luce, and in particular the Bradley-Terry, preference framework, two reward functions from the same class induce the same preference distribution.*\n",
      "\n",
      "**Lemma 2:** *Two reward functions from the same equivalence class induce the same optimal policy under the constrained RL problem.*\n",
      "\n",
      "The proofs are straightforward and we defer them to Appendix [8.5](#app:lemma1). The first lemma is a well-known under-specification issue with the Plackett-Luce family of models . Due to this under-specification, we usually have to impose additional identifiability constraints to achieve any guarantees on the MLE estimates from Eq. [\\[eq:reward_model\\]](#eq:reward_model) . The second lemma states that all reward functions from the same class yield the same optimal policy, hence for our final objective, we are only interested in recovering an arbitrary reward function from the optimal class. We prove the following Theorem in Appendix [8.6](#app:thm1):\n",
      "\n",
      "**Theorem 1:** *Under mild assumptions, all reward classes consistent with the Plackett-Luce (and Bradley-Terry in particular) models can be represented with the reparameterization $}$ for some model $(y| x)$ and a given reference model $_ref(y | x)$.*\n",
      "\n",
      "Consider any reward function $r(x, y)$, which induces a corresponding optimal model $_r(y | x)$, specified by Eq. [\\[eq:op_policy\\]](#eq:op_policy). We will show that a reward function from the equivalence class of $r$ can be represented using the reparameterization given above. We define the projection $f$ as $$f(r; _ref, )(x, y) = r(x, y) - _{y}_ref(y x)({1}{}r(x, y))$$ The operator $f$ simply normalizes the reward function with the logarithm of the partition function of $_r$. Since the added normalization term is only a function of the prefix $x$, $f(r; _ref, )(x, y)$ is a reward function in the equivalence class of $r(x, y)$. Finally, replacing $r$ with the RHS of Eq. [\\[eq:main_eq\\]](#eq:main_eq) (which holds for any reward function), we have $f(r; _ref, )(x, y) = $. That is, the projection $f$ produces a member of the equivalence class of $r$ with the desired form, and we do not lose any generality in our reward model from the proposed reparameterization. We can alternatively view Theorem [1](#thm:main) as specifying exactly which reward function within each equivalence class the DPO reparameterization selects, that is, the reward function satisfying: $$ _{y}{_ref(y x)({1}{}r(x, y))}_{=(y x), using Thm.~{thm:main reparam.}} = 1,$$ i.e., $(y| x)$ is a valid distribution (probabilities are positive and sum to 1). However, following Eq. [\\[eq:op_policy\\]](#eq:op_policy), we can see that Eq. [\\[eq:lag_p\\]](#eq:lag_p) is the partition function of the optimal policy induced by the reward function $r(x, y)$. The key insight of the DPO algorithm is that we can impose certain constraints on the under-constrained Plackett-Luce (and Bradley-Terry in particular) family of preference models, such that we preserve the class of representable reward models, but explicitly make the optimal policy in Eq. [\\[eq:op_policy\\]](#eq:op_policy) analytically tractable for all prompts $x$.\n",
      "\n",
      "## Instability of Actor-Critic Algorithms\n",
      "\n",
      "We can also use our framework to diagnose instabilities with standard actor-critic algorithms used for the RLHF, such as PPO. We follow the RLHF pipeline and focus on the RL fine-tuning step outlined in Section [3](#section:prelims). We can draw connections to the control as inference framework for the constrained RL problem outlined in [\\[eq:RL\\]](#eq:RL). We assume a parameterized model $_(y| x)$ and minimize $_[_(y|x) | | ^*(y| x)]$ where $^*$ is the optimal policy from Eq. [\\[eq:optimum_model\\]](#eq:optimum_model) induced by the reward function $r_(y, x)$. With some algebra this leads to the optimization objective: $$ _{_{}}{E}_{_{}(y x)}[{r_{}(x, y) -_{y}_ref(y x)({1}{}r_{}(x, y))}_{f(r_{}, _ref, )} - {{_{}(y x)}{_ref(y x)}}_{KL}]$$ This is the same objective optimized in prior works\n",
      "\n",
      "using the DPO-equivalent reward for the reward class of $r_$. In this setting, we can interpret the normalization term in $f(r_, _ref, )$ as the soft value function of the reference policy $_ref$. While this term does not affect the optimal solution, without it, the policy gradient of the objective could have high variance, making learning unstable. We can accommodate for the normalization term using a learned value function, but that can also be difficult to optimize. Alternatively, prior works have normalized rewards using a human completion baseline, essentially a single sample Monte-Carlo estimate of the normalizing term. In contrast the DPO reparameterization yields a reward function that does not require any baselines.\n",
      "\n",
      "# Experiments\n",
      "\n",
      "In this section, we empirically evaluate DPO's ability to train policies directly from preferences. First, in a well-controlled text-generation setting, we ask: how efficiently does DPO trade off maximizing reward and minimizing KL-divergence with the reference policy, compared to common preference learning algorithms such as PPO? Next, we evaluate DPO's performance on larger models and more difficult RLHF tasks, including summarization and dialogue. We find that with almost no tuning of hyperparameters, DPO tends to perform as well or better than strong baselines like RLHF with PPO as well as returning the best of $N$ sampled trajectories under a learned reward function. Before presenting these results, we describe the experimental set-up; additional details are in Appendix [10](#app:exp_details).\n",
      "\n",
      "**Tasks.** Our experiments explore three different open-ended text generation tasks. For all experiments, algorithms learn a policy from a dataset of preferences $=\\, y_w^, y_l^\\}_^N$. In **controlled sentiment generation**, $x$ is a prefix of a movie review from the IMDb dataset , and the policy must generate $y$ with positive sentiment. In order to perform a controlled evaluation, for this experiment we *generate* preference pairs over generations using a pre-trained sentiment classifier, where $p(positive| x,y_w)>p(positive| x,y_l)$. For SFT, we fine-tune GPT-2-large until convergence on reviews from the train split of the IMDB dataset (further details in App [10.1](#app:sentiment_details)). In **summarization**, $x$ is a forum post from Reddit; the policy must generate a summary $y$ of the main points in the post. Following prior work, we use the Reddit TL;DR summarization dataset along with human preferences gathered by @stiennon2022learning. We use an SFT model fine-tuned on human-written forum post summaries with the TRLX framework for RLHF. The human preference dataset was gathered by @stiennon2022learning on samples from a different, but similarly-trained, SFT model. Finally, in **single-turn dialogue**, $x$ is a human query, which may be anything from a question about astrophysics to a request for relationship advice. A policy must produce an engaging and helpful response $y$ to a user's query; we use the Anthropic Helpful and Harmless dialogue dataset , containing 170k dialogues between a human and an automated assistant. Each transcript ends with a pair of responses generated by a large (although unknown) language model along with a preference label denoting the human-preferred response. In this setting, no pre-trained SFT model is available; we therefore fine-tune an off-the-shelf language model on only the preferred completions to form the SFT model.\n",
      "\n",
      "![image](figures/results/frontier.pdf) ![image](figures/results/tldr_winrate_vs_temp.pdf)\n",
      "\n",
      "[] **Evaluation.** Our experiments use two different approaches to evaluation. In order to analyze the effectiveness of each algorithm in optimizing the constrained reward maximization objective, in the controlled sentiment generation setting we evaluate each algorithm by its frontier of achieved reward and KL-divergence from the reference policy; this frontier is computable because we have acccess to the ground-truth reward function (a sentiment classifier). However, in the real world, the ground truth reward function is not known; therefore, we evaluate algorithms with their *win rate* against a baseline policy, using GPT-4 as a proxy for human evaluation of summary quality and response helpfulness in the summarization and single-turn dialogue settings, respectively. For summarization, we use reference summaries in the test set as the baseline; for dialogue, we use the preferred response in the test dataset as the baseline. While existing studies suggest LMs can be better automated evaluators than existing metrics , we conduct a human study to justify our usage of GPT-4 for evaluation in Sec. [6.4](#sec:human-judgments). We find GPT-4 judgments correlate strongly with humans, with human agreement with GPT-4 typically similar or higher than inter-human annotator agreement.\n",
      "\n",
      "**Methods.** In addition to DPO, we evaluate several existing approaches to training language models to adhere to human preferences. Most simply, we explore zero-shot prompting with **GPT-J** in the summarization task and 2-shot prompting with **Pythia-2.8B** in the dialogue task. In addition, we evaluate the **SFT** model as well as **Preferred-FT**, which is a model fine-tuned with supervised learning on the chosen completion $y_w$ from either the SFT model (in controlled sentiment and summarization) or a generic LM (in single-turn dialogue). Another pseudo-supervised method is **Unlikelihood** , which simply optimizes the policy to maximize the probability assigned to $y_w$ and *minimize* the probability assigned to $y_l$; we use an optional coefficient $[0,1]$ on the 'unlikelihood' term. We also consider **PPO** using a reward function learned from the preference data and **PPO-GT**, which is an oracle that learns from the ground truth reward function available in the controlled sentiment setting. In our sentiment experiments, we use two implementations of PPO-GT, one of-the-shelf version as well as a modified version that normalizes rewards and further tunes hyperparameters to improve performance (we also use these modifications when running 'normal' PPO with learned rewards). Finally, we consider the **Best of $N$** baseline, sampling $N$ responses from the SFT model (or Preferred-FT in dialogue) and returning the highest-scoring response according to a reward function learned from the preference dataset. This high-performing method decouples the quality of the reward model from the PPO optimization, but is computationally impractical even for moderate $N$ as it requires sampling $N$ completions for every query at test time.\n",
      "\n",
      "## How well can DPO optimize the RLHF objective?\n",
      "\n",
      "![image](figures/results/dialogue_winrate_vs_temp.pdf) ![image](figures/results/dialogue_winrate_vs_steps.pdf)\n",
      "\n",
      "[] The KL-constrained reward maximization objective used in typical RLHF algorithms balances exploitation of reward while restricting the policy from deviating far from the reference policy. Therefore, when comparing algorithms, we must take into account both reward achieved as well as the KL discrepancy; achieving slightly higher reward but with much higher KL is not necessarily desirable. Figure [\\[fig:frontier-tldr-main\\]](#fig:frontier-tldr-main) shows the reward-KL frontier for various algorithms in the sentiment setting. We execute multiple training runs for each algorithm, using a different hyperparameter for policy conservativeness in each run (target KL $\\$ for PPO, $ \\$, $\\$ for unlikelihood, random seeds for preferred-FT). This sweep includes 22 runs in total. After each 100 training steps until convergence, we evaluate each policy on a set of test prompts, computing the average reward under the true reward function as well as the average sequence-level KL with the reference policy $KL(| | _ref)$. We find that DPO produces by far the most efficient frontier, achieving the highest reward while still achieving low KL. This result is particularly notable for multiple reasons. First, DPO and PPO optimize the same objective, but DPO is notably more efficient; DPO's reward/KL tradeoff strictly dominates PPO. Second, DPO achieves a better frontier than PPO, *even when PPO can access ground truth rewards* (PPO-GT).\n",
      "\n",
      "## Can DPO scale to real preference datasets? Next, we evaluate fine-tuning performance of DPO on summarization and single-turn dialogue. For summarization, automatic evaluation metrics such as ROUGE can be poorly correlated with human preferences , and prior work has found that fine-tuning LMs using PPO on human preferences to provide more effective summaries. We evaluate different methods by sampling completions on the test split of TL;DR summarization dataset, and computing the average win rate against reference completions in the test set. The completions for all methods are sampled at temperatures varying from 0.0 to 1.0, and the win rates are shown in Figure [\\[fig:frontier-tldr-main\\]](#fig:frontier-tldr-main) (right). DPO, PPO and Preferred-FT all fine-tune the same GPT-J SFT model. We find that DPO has a win rate of approximately 61% at a temperature of 0.0, exceeding the performance of PPO at  57% at its optimal sampling temperature of 0.0. DPO also achieves a higher maximum win rate compared to the best of $N$ baseline. We note that we did not meaningfully tune DPO's $$ hyperparameter, so these results may underestimate DPO's potential. Moreover, we find DPO to be much more robust to the sampling temperature than PPO, the performance of which can degrade to that of the base GPT-J model at high temperatures. Preferred-FT does not improve significantly over the SFT model. We also compare DPO and PPO head-to-head in human evaluations in Section [6.4](#sec:human-judgments), where DPO samples at temperature 0.25 were preferred 58% times over PPO samples at temperature 0.\n",
      "\n",
      "On single-turn dialogue, we evaluate the different methods on the subset of the test split of the Anthropic HH dataset with one step of human-assistant interaction. GPT-4 evaluations use the preferred completions on the test as the reference to compute the win rate for different methods. As there is no standard SFT model for this task, we start with a pre-trained Pythia-2.8B, use Preferred-FT to train a reference model on the chosen completions such that completions are within distribution of the model, and then train using DPO. We also compare against the best of 128 Preferred-FT completions (we found the Best of $N$ baseline plateaus at 128 completions for this task; see Appendix Figure [\\[fig:best-of-n\\]](#fig:best-of-n)) and a 2-shot prompted version of the Pythia-2.8B base model, finding DPO performs as well or better for the best-performing temperatures for each method. We also evaluate an RLHF model trained with PPO on the Anthropic HH dataset from a well-known source , but are unable to find a prompt or sampling temperature that gives performance better than the base Pythia-2.8B model. Based on our results from TL;DR and the fact that both methods optimize the same reward function, we consider Best of 128 a rough proxy for PPO-level performance. Overall, DPO is the only computationally efficient method that improves over the preferred completions in the Anthropic HH dataset, and provides similar or better performance to the computationally demanding Best of 128 baseline. Finally, Figure [\\[fig:dialogue-main\\]](#fig:dialogue-main) shows that DPO converges to its best performance relatively quickly.\n",
      "\n",
      "## Generalization to a new input distribution\n",
      "\n",
      "::: wraptable r0.375\n",
      "\n",
      " ------------------ ------------------------------- ------------- **Win rate vs. ground truth** (lr)2-3 **Alg.** Temp $0$ Temp $0.25$ DPO 0.36 0.31 PPO 0.26 0.23 ------------------ ------------------------------- -------------\n",
      "\n",
      "[] :::\n",
      "\n",
      "To further compare the performance of PPO and DPO under distribution shifts, we evaluate the PPO and DPO policies from our Reddit TL;DR summarization experiment on a different distribution, news articles in the test split of the CNN/DailyMail dataset , using the best sampling temperatures from TL;DR (0 and 0.25). The results are presented in Table [\\[tab:ood\\]](#tab:ood). We computed the GPT-4 win rate against the ground-truth summaries in the datasets, using the same GPT-4 (C) prompt we used for Reddit TL;DR, but replacing the words \"forum post\" with \"news article\". For this new distribution, DPO continues to outperform the PPO policy by a significant margin. This experiment provides initial evidence that DPO policies can generalize similarly well to PPO policies, even though DPO does not use the additional unlabeled Reddit TL;DR prompts that PPO uses.\n",
      "\n",
      "## Validating GPT-4 judgments with human judgments We conduct a human study to verify the reliability of GPT-4's judgments, using the results of the TL;DR summarization experiment and two different GPT-4 prompts. The **GPT-4 (S)** (simple) prompt simply asks for which summary better-summarizes the important information in the post. The **GPT-4 (C)** (concise) prompt also asks for which summary is more concise; we evaluate this prompt because we find that GPT-4 prefers longer, more repetitive summaries than humans do with the **GPT-4 (S)** prompt. See Appendix [10.2](#app:prompts) for the complete prompts. We perform three comparisons, using the highest (DPO, temp. 0.25), the lowest (PPO, temp. 1.0), and a\n",
      "\n",
      "::: wraptable r0.47\n",
      "\n",
      " ----------------------- --------- --------- ----------- **DPO** **SFT** **PPO-1** (lr)2-4 N respondents 272 122 199 GPT-4 (S) win % 47 27 13 GPT-4 (C) win % 54 32 12 Human win % 58 43 17 GPT-4 (S)-H agree 70 77 86 GPT-4 (C)-H agree 67 79 85 H-H agree 65 \\- 87 ----------------------- --------- --------- -----------\n",
      "\n",
      "[] :::\n",
      "\n",
      "middle-performing (SFT, temp. 0.25) method with the aim of covering a diversity of sample qualities; all three methods are compared against greedily-sampled PPO (its best-performing temperature). We find that with both prompts, GPT-4 tends to agree with humans about as often as humans agree with each other, suggesting that GPT-4 is a reasonable proxy for human evaluations (due to limited human raters, we only collect multiple human judgments for the DPO and PPO-1 comparisons). Overall, the **GPT-4 (C)** prompt generally provides win rates more representative of humans; we therefore use this prompt for the main results in Section [6.2](#sec:dpo-real-datasets). For additional details about the human study, including the web interface presented to raters and the list of human volunteers, see Appendix [11.3](#app:human-study).\n",
      "\n",
      "# Discussion\n",
      "\n",
      "Learning from preferences is a powerful, scalable framework for training capable, aligned language models. We have introduced DPO, a simple training paradigm for training language models from preferences without reinforcement learning. Rather than coercing the preference learning problem into a standard RL setting in order to use off-the-shelf RL algorithms, DPO identifies a mapping between language model policies and reward functions that enables training a language model to satisfy human preferences *directly*, with a simple cross-entropy loss, without reinforcement learning or loss of generality. With virtually no tuning of hyperparameters, DPO performs similarly or better than existing RLHF algorithms, including those based on PPO; DPO thus meaningfully reduces the barrier to training more language models from human preferences.\n",
      "\n",
      "**Limitations & Future Work.** Our results raise several important questions for future work. How does the DPO policy generalize out of distribution, compared with learning from an explicit reward function? Our initial results suggest that DPO policies can generalize similarly to PPO-based models, but more comprehensive study is needed. For example, can training with self-labeling from the DPO policy similarly make effective use of unlabeled prompts? On another front, how does reward over-optimization manifest in the direct preference optimization setting, and is the slight decrease in performance in Figure [\\[fig:dialogue-main\\]](#fig:dialogue-main)-right an instance of it? Additionally, while we evaluate models up to 6B parameters, exploration of scaling DPO to state-of-the-art models orders of magnitude larger is an exciting direction for future work. Regarding evaluations, we find that the win rates computed by GPT-4 are impacted by the prompt; future work may study the best way to elicit high-quality judgments from automated systems. Finally, many possible applications of DPO exist beyond training language models from human preferences, including training generative models in other modalities.\n"
     ]
    }
   ],
   "source": [
    "def clean_latex_to_markdown(text):\n",
    "    \"\"\"\n",
    "    Clean LaTeX formatting and convert to markdown while preserving structure.\n",
    "    \"\"\"\n",
    "    # Store the original text for processing\n",
    "    cleaned_text = text\n",
    "    \n",
    "    # 1. Remove citation brackets like [@author2020paper; @another2021paper]\n",
    "    cleaned_text = re.sub(r'\\[@[^\\]]+\\]', '', cleaned_text)\n",
    "    \n",
    "    # 2. Remove LaTeX labels\n",
    "    cleaned_text = re.sub(r'\\\\label\\{[^}]+\\}', '', cleaned_text)\n",
    "    \n",
    "    # 3. Clean up equation references - convert complex refs to simple format\n",
    "    # Pattern like: Eq.[\\[eq:RL\\]](#eq:RL){reference-type=\"ref\" reference=\"eq:RL\"}\n",
    "    cleaned_text = re.sub(\n",
    "        r'Eq\\.\\[\\\\?\\[([^\\]]+)\\]\\]\\([^)]*\\)\\{[^}]*\\}', \n",
    "        r'Eq. (\\1)', \n",
    "        cleaned_text\n",
    "    )\n",
    "    \n",
    "    # 4. Clean up section references\n",
    "    # Pattern like: Section[\\[sec:theory\\]](#sec:theory){reference-type=\"ref\" reference=\"sec:theory\"}\n",
    "    cleaned_text = re.sub(\n",
    "        r'Section\\[\\\\?\\[([^\\]]+)\\]\\]\\([^)]*\\)\\{[^}]*\\}', \n",
    "        r'Section \\1', \n",
    "        cleaned_text\n",
    "    )\n",
    "    \n",
    "    # 5. Clean up appendix references \n",
    "    # Pattern like: Appendix[\\[app:derivation1\\]](#app:derivation1){reference-type=\"ref\" reference=\"app:derivation1\"}\n",
    "    cleaned_text = re.sub(\n",
    "        r'Appendix\\[\\\\?\\[([^\\]]+)\\]\\]\\([^)]*\\)\\{[^}]*\\}', \n",
    "        r'Appendix', \n",
    "        cleaned_text\n",
    "    )\n",
    "    \n",
    "    # 6. Clean up figure references\n",
    "    # Pattern like: Figure[\\[fig:frontier-tldr-main\\]](#fig:frontier-tldr-main){reference-type=\"ref\" reference=\"fig:frontier-tldr-main\"}\n",
    "    cleaned_text = re.sub(\n",
    "        r'Figure\\[\\\\?\\[([^\\]]+)\\]\\]\\([^)]*\\)\\{[^}]*\\}', \n",
    "        r'Figure \\1', \n",
    "        cleaned_text\n",
    "    )\n",
    "    \n",
    "    # 7. Clean up table references\n",
    "    cleaned_text = re.sub(\n",
    "        r'Table\\[\\\\?\\[([^\\]]+)\\]\\]\\([^)]*\\)\\{[^}]*\\}', \n",
    "        r'Table \\1', \n",
    "        cleaned_text\n",
    "    )\n",
    "    \n",
    "    # 8. Remove complex figure blocks but preserve descriptive text\n",
    "    # Pattern like: ![ **optimizes for human preferences...] \n",
    "    figure_pattern = r'!\\[\\s*\\*\\*([^*]+)\\*\\*([^\\]]*)\\]'\n",
    "    cleaned_text = re.sub(figure_pattern, r'**Figure: \\1**\\2', cleaned_text)\n",
    "    \n",
    "    # 9. Clean up LaTeX text formatting commands\n",
    "    cleaned_text = re.sub(r'\\\\text\\{([^}]+)\\}', r'\\1', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\\\textrm\\{([^}]+)\\}', r'\\1', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\\\textit\\{([^}]+)\\}', r'*\\1*', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\\\textbf\\{([^}]+)\\}', r'**\\1**', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\\\emph\\{([^}]+)\\}', r'*\\1*', cleaned_text)\n",
    "    \n",
    "    # 10. Clean up LaTeX math commands in text (but preserve $$ blocks)\n",
    "    # Remove \\mid and similar in inline contexts\n",
    "    cleaned_text = re.sub(r'\\\\mid(?![^$]*\\$\\$)', '|', cleaned_text)\n",
    "    \n",
    "    # 11. Clean up reference artifacts like {reference-type=\"ref\" reference=\"...\"}\n",
    "    cleaned_text = re.sub(r'\\{[^}]*reference-type[^}]*\\}', '', cleaned_text)\n",
    "    \n",
    "    # 12. Clean up footnote markers like [^2], [^3] etc.\n",
    "    cleaned_text = re.sub(r'\\[\\^[0-9]+\\]', '', cleaned_text)\n",
    "    \n",
    "    # 13. Remove LaTeX environments that aren't math\n",
    "    # Remove \\begin{...} and \\end{...} for non-math environments\n",
    "    cleaned_text = re.sub(r'\\\\begin\\{(?!equation|align|gather)[^}]+\\}', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\\\end\\{(?!equation|align|gather)[^}]+\\}', '', cleaned_text)\n",
    "    \n",
    "    # 14. Clean up definition and lemma blocks - convert to markdown\n",
    "    # Pattern like: ::: definition ... :::\n",
    "    cleaned_text = re.sub(r':::\\s*definition\\s*\\n\\*\\*Definition\\s+(\\d+)\\*\\*\\.([^:]+):::', \n",
    "                         r'**Definition \\1:** \\2', cleaned_text, flags=re.DOTALL)\n",
    "    \n",
    "    cleaned_text = re.sub(r':::\\s*\\{#[^}]+\\s+\\.lemma\\}\\s*\\n\\*\\*Lemma\\s+(\\d+)\\*\\*\\.([^:]+):::', \n",
    "                         r'**Lemma \\1:** \\2', cleaned_text, flags=re.DOTALL)\n",
    "    \n",
    "    # 15. Clean up theorem blocks\n",
    "    cleaned_text = re.sub(r':::\\s*\\{#[^}]+\\s+\\.theorem\\}\\s*\\n\\*\\*Theorem\\s+(\\d+)\\*\\*\\.([^:]+):::',\n",
    "                         r'**Theorem \\1:** \\2', cleaned_text, flags=re.DOTALL)\n",
    "\n",
    "    # 16. Remove pandoc header attributes like `{#...}`\n",
    "    cleaned_text = re.sub(r'\\s*\\{#[^}]+\\}\\s*$', '', cleaned_text, flags=re.MULTILINE)\n",
    "\n",
    "    # 17. Remove remaining LaTeX artifacts\n",
    "    cleaned_text = re.sub(r'\\\\[a-zA-Z]+\\*?', '', cleaned_text)  # Remove LaTeX commands\n",
    "    cleaned_text = re.sub(r'\\{[^}]*\\}(?![^$]*\\$\\$)', '', cleaned_text)  # Remove remaining braces outside math\n",
    "    \n",
    "    # 18. Join paragraphs by replacing single newlines with spaces, but preserving paragraph breaks (double newlines).\n",
    "    cleaned_text = re.sub(r'(?<!\\n)\\n(?!\\n)', ' ', cleaned_text)\n",
    "\n",
    "    # 19. Clean up multiple spaces and empty lines\n",
    "    cleaned_text = re.sub(r' +', ' ', cleaned_text)  # Multiple spaces to single\n",
    "    cleaned_text = re.sub(r'\\n\\s*\\n+', '\\n\\n', cleaned_text)  # Multiple newlines to double\n",
    "    \n",
    "    # 20. Fix any broken markdown headers\n",
    "    cleaned_text = re.sub(r'^#+\\s*$', '', cleaned_text, flags=re.MULTILINE)\n",
    "    \n",
    "    return cleaned_text.strip()\n",
    "\n",
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, width=100):\n",
    "    \"\"\"\n",
    "    Prints the given text wrapped to a specified width for better readability in notebooks.\n",
    "    This function preserves paragraph breaks.\n",
    "    \"\"\"\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    for para in paragraphs:\n",
    "        print(textwrap.fill(para, width=width))\n",
    "        print()\n",
    "        \n",
    "cleaned_paper = clean_latex_to_markdown(latex_content)\n",
    "cleaned_paper = cleaned_paper.split('# Acknowledgements')[0].strip()\n",
    "print(cleaned_paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's improve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Introduction\n",
      "\n",
      "Large unsupervised language models (LMs) trained on very large datasets acquire surprising\n",
      "capabilities . However, these models are trained on data generated by humans with a wide variety of\n",
      "goals, priorities, and skillsets. Some of these goals and skillsets may not be desirable to imitate;\n",
      "for example, while we may want our AI coding assistant to *understand* common programming mistakes\n",
      "in order to correct them, nevertheless, when generating code, we would like to bias our model toward\n",
      "the (potentially rare) high-quality coding ability present in its training data. Similarly, we might\n",
      "want our language model to be *aware* of a common misconception believed by 50% of people, but we\n",
      "certainly do not want the model to claim this misconception to be true in 50% of queries about it!\n",
      "In other words, selecting the model's *desired responses and behavior* from its very wide *knowledge\n",
      "and abilities* is crucial to building AI systems that are safe, performant, and controllable . While\n",
      "existing methods typically steer LMs to match human preferences using reinforcement learning (RL),\n",
      "we will show that the RL-based objective used by existing methods can be optimized exactly with a\n",
      "simple binary cross-entropy objective, greatly simplifying the preference learning pipeline.\n",
      "\n",
      "![ **optimizes for human preferences while avoiding reinforcement learning.** Existing methods for\n",
      "fine-tuning language models with human feedback first fit a reward model to a dataset of prompts and\n",
      "human preferences over pairs of responses, and then use RL to find a policy that maximizes the\n",
      "learned reward. In contrast, directly optimizes for the policy best satisfying the preferences with\n",
      "a simple classification objective, fitting an *implicit* reward model whose corresponding optimal\n",
      "policy can be extracted in closed form.](figures/diagrams/teaser.png)\n",
      "\n",
      "At a high level, existing methods instill the desired behaviors into a language model using curated\n",
      "sets of human preferences representing the types of behaviors that humans find safe and helpful.\n",
      "This preference learning stage occurs after an initial stage of large-scale unsupervised pre-\n",
      "training on a large text dataset. While the most straightforward approach to preference learning is\n",
      "supervised fine-tuning on human demonstrations of high quality responses, the most successful class\n",
      "of methods is reinforcement learning from human (or AI) feedback (RLHF/RLAIF; ). RLHF methods fit a\n",
      "reward model to a dataset of human preferences and then use RL to optimize a language model policy\n",
      "to produce responses assigned high reward without drifting excessively far from the original model.\n",
      "While RLHF produces models with impressive conversational and coding abilities, the RLHF pipeline is\n",
      "considerably more complex than supervised learning, involving training multiple LMs and sampling\n",
      "from the LM policy in the loop of training, incurring significant computational costs.\n",
      "\n",
      "In this paper, we show how to directly optimize a language model to adhere to human preferences,\n",
      "without explicit reward modeling or reinforcement learning. We propose *()*, an algorithm that\n",
      "implicitly optimizes the same objective as existing RLHF algorithms (reward maximization with a KL-\n",
      "divergence constraint) but is simple to implement and straightforward to train. Intuitively, the\n",
      "update increases the relative log probability of preferred to dispreferred responses, but it\n",
      "incorporates a dynamic, per-example importance weight that prevents the model degeneration that we\n",
      "find occurs with a naive probability ratio objective. Like existing algorithms, relies on a\n",
      "theoretical preference model (such as the Bradley-Terry model; ) that measures how well a given\n",
      "reward function aligns with empirical preference data. However, while existing methods use the\n",
      "preference model to define a preference loss to train a reward model and then train a policy that\n",
      "optimizes the learned reward model, uses a change of variables to define the preference loss as a\n",
      "function of the policy directly. Given a dataset of human preferences over model responses, can\n",
      "therefore optimize a policy using a simple binary cross entropy objective, producing the optimal\n",
      "policy to an implicit reward function fit to the preference data.\n",
      "\n",
      "Our main contribution is (), a simple RL-free algorithm for training language models from\n",
      "preferences. Our experiments show that is at least as effective as existing methods, including PPO-\n",
      "based RLHF, for learning from preferences in tasks such as sentiment modulation, summarization, and\n",
      "dialogue, using language models with up to 6B parameters.\n",
      "\n",
      "# Related Work\n",
      "\n",
      "Self-supervised language models of increasing scale learn to complete some tasks zero-shot or with\n",
      "few-shot prompts. However, their performance on downstream tasks and alignment with user intent can\n",
      "be significantly improved by fine-tuning on datasets of instructions and human-written completions.\n",
      "This 'instruction-tuning' procedure enables LLMs to generalize to instructions outside of the\n",
      "instruction-tuning set and generally increase their usability . Despite the success of instruction\n",
      "tuning, *relative* human judgments of response quality are often easier to collect than expert\n",
      "demonstrations, and thus subsequent works have fine-tuned LLMs with datasets of human preferences,\n",
      "improving proficiency in translation , summarization , story-telling , and instruction-following.\n",
      "These methods first optimize a neural network reward function for compatibility with the dataset of\n",
      "preferences under a preference model such as the Bradley-Terry model, then fine-tune a language\n",
      "model to maximize the given reward using reinforcement learning algorithms, commonly REINFORCE ,\n",
      "proximal policy optimization (PPO;), or variants . A closely-related line of work leverages LLMs\n",
      "fine-tuned for instruction following with human feedback to generate additional synthetic preference\n",
      "data for targeted attributes such as safety or harmlessness, using only weak supervision from humans\n",
      "in the form of a text rubric for the LLM's annotations. These methods represent a convergence of two\n",
      "bodies of work: one body of work on training language models with reinforcement learning for a\n",
      "variety of objectives  and another body of work on general methods for learning from human\n",
      "preferences . Despite the appeal of using relative human preferences, fine-tuning large language\n",
      "models with reinforcement learning remains a major practical challenge; this work provides a\n",
      "theoretically-justified approach to optimizing relative preferences without RL.\n",
      "\n",
      "Outside of the context of language, learning policies from preferences has been studied in both\n",
      "bandit and reinforcement learning settings, and several approaches have been proposed. Contextual\n",
      "bandit learning using preferences or rankings of actions, rather than rewards, is known as a\n",
      "contextual dueling bandit (CDB; ). In the absence of absolute rewards, theoretical analysis of CDBs\n",
      "substitutes the notion of an optimal policy with a *von Neumann winner*, a policy whose expected win\n",
      "rate against *any* other policy is at least 50% . However, in the CDB setting, preference labels are\n",
      "given online, while in learning from human preferences, we typically learn from a fixed batch of\n",
      "offline preference-annotated action pairs . Similarly, *preference-based RL* (PbRL) learns from\n",
      "binary preferences generated by an *unknown* 'scoring' function rather than rewards . Various\n",
      "algorithms for PbRL exist, including methods that can reuse off-policy preference data, but\n",
      "generally involve first explicitly estimating the latent scoring function (i.e. the reward model)\n",
      "and subsequently optimizing it. We instead present a single stage policy learning approach that\n",
      "directly optimizes a policy to satisfy preferences.\n",
      "\n",
      "# Preliminaries We review the RLHF pipeline in @ziegler2020finetuning (and later). It usually\n",
      "includes three phases: 1) supervised fine-tuning (SFT); 2) preference sampling and reward learning\n",
      "and 3) RL optimization.\n",
      "\n",
      "**SFT**: RLHF typically begins by fine-tuning a pre-trained LM with supervised learning on high-\n",
      "quality data for the downstream task(s) of interest (dialogue, summarization, etc.), to obtain a\n",
      "model $\\pi^SFT$.\n",
      "\n",
      "**Reward Modelling Phase**: In the second phase the SFT model is prompted with prompts $x$ to\n",
      "produce pairs of answers $(y_1, y_2)\\sim \\pi^SFT(y \\mid x)$. These are then presented to human\n",
      "labelers who express preferences for one answer, denoted as $y_w\\succ y_l \\mid x$ where $y_w$ and\n",
      "$y_l$ denotes the preferred and dispreferred completion amongst $(y_1, y_2)$ respectively. The\n",
      "preferences are assumed to be generated by some latent reward model $r^*(y, x)$, which we do not\n",
      "have access to. There are a number of approaches used to model preferences, the Bradley-Terry (BT)\n",
      "model being a popular choice (although more general Plackett-Luce ranking models are also compatible\n",
      "with the framework if we have access to several ranked answers). The BT model stipulates that the\n",
      "human preference distribution $p^*$ can be written as: $$  p^*(y_1\\succ y_2 \\mid\n",
      "x)=\\frac{\\exp\\left(r^*(x, y_1)\\right)}{\\exp\\left(r^*(x, y_1)\\right) + \\exp\\left(r^*(x,\n",
      "y_2)\\right)}.$$ Assuming access to a static dataset of comparisons $\\mathcal{D}=\\bigl\\{x^{(i)},\n",
      "y_w^{(i)}, y_l^{(i)}\\bigr\\}_{i=1}^N$ sampled from $p^*$, we can parametrize a reward model\n",
      "$r_{\\phi}(x, y)$ and estimate the parameters via maximum likelihood. Framing the problem as a binary\n",
      "classification we have the negative log-likelihood loss: $$  \\mathcal{L}_R(r_{\\phi}, \\mathcal{D}) =\n",
      "-\\mathbb{E}_{(x, y_w, y_l)\\sim \\mathcal{D}}\\bigl[\\log \\sigma(r_{\\phi}(x, y_w)- r_{\\phi}(x,\n",
      "y_l))\\bigr]$$ where $\\sigma$ is the logistic function. In the context of LMs, the network\n",
      "$r_{\\phi}(x, y)$ is often initialized from the SFT model $\\pi^SFT(y \\mid x)$ with the addition of a\n",
      "linear layer on top of the final transformer layer that produces a single scalar prediction for the\n",
      "reward value . To ensure a reward function with lower variance, prior works normalize the rewards,\n",
      "such that $\\mathbb{E}_{x,y\\sim \\mathcal{D}}\\left[r_\\phi(x, y)\\right] = 0$ for all $x$.\n",
      "\n",
      "**RL Fine-Tuning Phase**: During the RL phase, the learned reward function is used to provide\n",
      "feedback to the language model. Following prior works , the optimization is formulated as $$\n",
      "\\max_{\\pi_{\\theta}} \\mathbb{E}_{x\\sim \\mathcal{D}, y\\sim \\pi_{\\theta}(y \\mid x)}\\bigl[r_{\\phi}(x,\n",
      "y)\\bigr] - \\beta\\mathbb{D}_{KL}\\bigl[\\pi_{\\theta}(y\\mid x)\\mid \\mid \\pi_ref(y\\mid x)\\bigr],$$ where\n",
      "$\\beta$ is a parameter controlling the deviation from the base reference policy $\\pi_ref$, namely\n",
      "the initial SFT model $\\pi^SFT$. In practice, the language model policy $\\pi_\\theta$ is also\n",
      "initialized to $\\pi^SFT$. The added constraint is important, as it prevents the model from deviating\n",
      "too far from the distribution on which the reward model is accurate, as well as maintaining the\n",
      "generation diversity and preventing mode-collapse to single high-reward answers. Due to the discrete\n",
      "nature of language generation, this objective is not differentiable and is typically optimized with\n",
      "reinforcement learning. The standard approachhas been to construct the reward function ${r(x, y) =\n",
      "r_{\\phi}(x, y) -\\beta (\\log \\pi_{\\theta}(y\\mid x) - \\log \\pi_ref(y\\mid x))}$, and maximize using PPO\n",
      ".\n",
      "\n",
      "# Direct Preference Optimization Motivated by the challenges of applying reinforcement learning\n",
      "algorithms on large-scale problems such as fine-tuning language models, our goal is to derive a\n",
      "simple approach for policy optimization using preferences directly. Unlike prior RLHF methods, which\n",
      "learn a reward and then optimize it via RL, our approach leverages a particular choice of reward\n",
      "model parameterization that enables extraction of its optimal policy in closed form, without an RL\n",
      "training loop. As we will describe next in detail, our key insight is to leverage an analytical\n",
      "mapping from reward functions to optimal policies, which enables us to transform a loss function\n",
      "over reward functions into a loss function over policies. This change-of-variables approach avoids\n",
      "fitting an explicit, standalone reward model, while still optimizing under existing models of human\n",
      "preferences, such as the Bradley-Terry model. In essence, the policy network represents both the\n",
      "language model and the (implicit) reward.\n",
      "\n",
      "**Deriving the DPO objective.** We start with the same RL objective as prior work,\n",
      "Eq. [\\[eq:RL\\]](#eq:RL), under a general reward function $r$. Following prior work , it is\n",
      "straightforward to show that the optimal solution to the KL-constrained reward maximization\n",
      "objective in Eq. [\\[eq:RL\\]](#eq:RL) takes the form: $$  \\pi_r(y\\mid x) =\n",
      "\\frac{1}{Z(x)}\\pi_ref(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right),$$ where $Z(x)\n",
      "=\\sum_{y}\\pi_ref(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)$ is the partition function. See\n",
      "Appendix [\\[app:derivation1\\]](#app:derivation1) for a complete derivation. Even if we use the MLE\n",
      "estimate $r_{\\phi}$ of the ground-truth reward function $r^*$, it is still expensive to estimate the\n",
      "partition function $Z(x)$, which makes this representation hard to utilize in practice. However, we\n",
      "can rearrange Eq. [\\[eq:op_policy\\]](#eq:op_policy) to express the reward function in terms of its\n",
      "corresponding optimal policy $\\pi_r$, the reference policy $\\pi_ref$, and the unknown partition\n",
      "function $Z(\\cdot)$. Specifically, we first take the logarithm of both sides of\n",
      "Eq. [\\[eq:op_policy\\]](#eq:op_policy) and then with some algebra we obtain: $$  r(x,y) =\\beta \\log\n",
      "\\frac{\\pi_r(y\\mid x)}{\\pi_ref(y\\mid x)} + \\beta \\log Z(x).$$ We can apply this reparameterization to\n",
      "the ground-truth reward $r^*$ and corresponding optimal model $\\pi^*$. Fortunately, the Bradley-\n",
      "Terry model depends only on the difference of rewards between two completions, i.e., ${p^*(y_1 \\succ\n",
      "y_2 \\mid x) = \\sigma(r^*(x, y_1) - r^*(x, y_2))}$. Substituting the reparameterization in\n",
      "Eq. [\\[eq:main_eq\\]](#eq:main_eq) for $r^*(x,y)$ into the preference model Eq. [\\[eq:bradley-\n",
      "terry\\]](#eq:bradley-terry), the partition function cancels, and we can express the human preference\n",
      "probability in terms of only the optimal policy $\\pi^*$ and reference policy $\\pi_ref$. Thus, the\n",
      "optimal RLHF policy $\\pi^*$ under the Bradley-Terry model satisfies the preference model: $$\n",
      "p^*(y_1\\succ y_2 \\mid x)=\\frac{1}{1 + \\exp\\left(\\beta \\log \\frac{\\pi^*(y_2\\mid x)}{\\pi_ref(y_2\\mid\n",
      "x)} - \\beta \\log \\frac{\\pi^*(y_1\\mid x)}{\\pi_ref(y_1\\mid x)}\\right)}$$ The derivation is in\n",
      "Appendix [8.2](#app:derivation2). While Eq. [\\[eq:objective\\]](#eq:objective) uses the Bradley-Terry\n",
      "model, we can similarly derive expressions under the more general Plackett-Luce models , shown in\n",
      "Appendix [8.3](#app:plackett_luce_models).\n",
      "\n",
      "Now that we have the probability of human preference data in terms of the optimal policy rather than\n",
      "the reward model, we can formulate a maximum likelihood objective for a parametrized policy\n",
      "$\\pi_\\theta$. Analogous to the reward modeling approach (i.e.\n",
      "Eq. [\\[eq:reward_model\\]](#eq:reward_model)), our policy objective becomes: $$\n",
      "\\mathcal{L}_DPO(\\pi_{\\theta}; \\pi_ref) = -\\mathbb{E}_{(x, y_w, y_l)\\sim \\mathcal{D}}\\left[\\log\n",
      "\\sigma \\left(\\beta \\log \\frac{\\pi_{\\theta}(y_w\\mid x)}{\\pi_ref(y_w\\mid x)} - \\beta \\log\n",
      "\\frac{\\pi_{\\theta}(y_l\\mid x)}{\\pi_ref(y_l\\mid x)}\\right)\\right].$$ This way, we fit an implicit\n",
      "reward using an alternative parameterization, whose optimal policy is simply $\\pi_\\theta$. Moreover,\n",
      "since our procedure is equivalent to fitting a reparametrized Bradley-Terry model, it enjoys certain\n",
      "theoretical properties, such as consistencies under suitable assumption of the preference data\n",
      "distribution . In Section [\\[sec:theory\\]](#sec:theory), we further discuss theoretical properties\n",
      "of DPO in relation to other works.\n",
      "\n",
      "**What does the DPO update do?** For a mechanistic understanding of DPO, it is useful to analyze the\n",
      "gradient of the loss function $\\mathcal{L}_DPO$. The gradient with respect to the parameters\n",
      "$\\theta$ can be written as: $$\\begin{gathered}\n",
      "\n",
      " \\nabla_\\theta \\mathcal{L}_DPO(\\pi_\\theta;\\pi_ref) = \\\\ -\\beta\\mathbb{E}_{(x, y_w, y_l) \\sim\n",
      "\\mathcal{D}} \\bigg[\\underbrace{\\sigma(\\hat{r}_\\theta(x, y_l) - \\hat{r}_\\theta (x, y_w))}_higher\n",
      "weight when reward estimate is wrong\\bigg[\\underbrace{\\nabla_\\theta\\log \\pi(y_w \\mid x)}_increase\n",
      "likelihood of $y_w$ - \\underbrace{\\nabla_\\theta\\log\\pi(y_l \\mid x)}_decrease likelihood of\n",
      "$y_l$\\bigg]\\bigg],\\end{gathered}$$ where $\\hat{r}_\\theta(x, y) = \\beta \\log \\frac{\\pi_\\theta(y \\mid\n",
      "x)}{\\pi_ref(y \\mid x)}$ is the reward implicitly defined by the language model $\\pi_\\theta$ and\n",
      "reference model $\\pi_ref$ (more in Section [\\[sec:theory\\]](#sec:theory)). Intuitively, the gradient\n",
      "of the loss function $\\mathcal{L}_DPO$ increases the likelihood of the preferred completions $y_w$\n",
      "and decreases the likelihood of dispreferred completions $y_l$. Importantly, the examples are\n",
      "weighed by how much higher the implicit reward model $\\hat{r}_\\theta$ rates the dispreferred\n",
      "completions, scaled by $\\beta$, i.e, how incorrectly the implicit reward model orders the\n",
      "completions, accounting for the strength of the KL constraint. Our experiments suggest the\n",
      "importance of this weighting, as a naïve version of this method without the weighting coefficient\n",
      "can cause the language model to degenerate (Appendix Table [1](#tab:unlikelihood_generations)).\n",
      "\n",
      "**DPO outline.** The general DPO pipeline is as follows: 1) Sample completions $y_1, y_2 \\sim\n",
      "\\pi_ref(\\cdot \\mid x)$ for every prompt $x$, label with human preferences to construct the offline\n",
      "dataset of preferences $\\mathcal{D} = \\{x^{(i)}, y_w^{(i)}, y_l)^{(i)}\\}_{i=1}^N$ and 2) optimize\n",
      "the language model $\\pi_\\theta$ to minimize $\\mathcal{L}_DPO$ for the given $\\pi_ref$ and\n",
      "$\\mathcal{D}$ and desired $\\beta$. In practice, one would like to reuse preference datasets publicly\n",
      "available, rather than generating samples and gathering human preferences. Since the preference\n",
      "datasets are sampled using $\\pi^SFT$, we initialize $\\pi_ref= \\pi^SFT$ whenever available. However,\n",
      "when $\\pi^SFT$ is not available, we initialize $\\pi_ref$ by maximizing likelihood of preferred\n",
      "completions ${(x, y_w)}$, that is, ${\\pi_ref= \\mathop{\\mathrm{arg\\,max}}_{\\pi}\\mathbb{E}_{x, y_w\n",
      "\\sim \\mathcal{D}}\\left[\\log \\pi(y_w \\mid x)\\right]}$. This procedure helps mitigate the distribution\n",
      "shift between the true reference distribution which is unavailable, and $\\pi_ref$ used by DPO.\n",
      "Further details related to the implementation and hyperparameters can be found in\n",
      "Appendix [9](#app:implementation).\n",
      "\n",
      "# Theoretical Analysis of DPO\n",
      "\n",
      "In this section, we give further interpretation of the DPO method, provide theoretical backing, and\n",
      "relate advantages of DPO to issues with actor critic algorithms used for RLHF (such as PPO ).\n",
      "\n",
      "## Your Language Model Is Secretly a Reward Model\n",
      "\n",
      "DPO is able to bypass both fitting an explicit reward and performing RL to learn the policy using a\n",
      "single maximum likelihood objective. Note the optimization objective Eq.\n",
      "[\\[eq:main_eq\\]](#eq:main_eq) is equivalent to a Bradley-Terry model with a reward parameterization\n",
      "$r^*(x, y) = \\beta \\log\\frac{\\pi^*_\\theta(y \\mid x)}{\\pi_ref(y \\mid x)}$ and we optimize our\n",
      "parametric model $\\pi_{\\theta}$, equivalently to the reward model optimization in Eq.\n",
      "[\\[eq:reward_model\\]](#eq:reward_model) under the change of variables. In this section we will build\n",
      "the theory behind this reparameterization, show that it does not constrain the class of learned\n",
      "reward models, and allows for the exact recovery of the optimal policy. We begin with by defining an\n",
      "equivalence relation between reward functions.\n",
      "\n",
      "::: definition **Definition 1**. *We say that two reward functions $r(x, y)$ and $r'(x, y)$ are\n",
      "equivalent iff ${r(x, y)-r'(x, y) = f(x)}$ for some function $f$.* :::\n",
      "\n",
      "It is easy to see that this is indeed an equivalence relation, which partitions the set of reward\n",
      "functions into classes. We can state the following two lemmas:\n",
      "\n",
      "::: **Lemma 1**. *Under the Plackett-Luce, and in particular the Bradley-Terry, preference\n",
      "framework, two reward functions from the same class induce the same preference distribution.* :::\n",
      "\n",
      "::: **Lemma 2**. *Two reward functions from the same equivalence class induce the same optimal\n",
      "policy under the constrained RL problem.* :::\n",
      "\n",
      "The proofs are straightforward and we defer them to Appendix [8.5](#app:lemma1). The first lemma is\n",
      "a well-known under-specification issue with the Plackett-Luce family of models . Due to this under-\n",
      "specification, we usually have to impose additional identifiability constraints to achieve any\n",
      "guarantees on the MLE estimates from Eq. [\\[eq:reward_model\\]](#eq:reward_model) . The second lemma\n",
      "states that all reward functions from the same class yield the same optimal policy, hence for our\n",
      "final objective, we are only interested in recovering an arbitrary reward function from the optimal\n",
      "class. We prove the following Theorem in Appendix [8.6](#app:thm1):\n",
      "\n",
      "::: **Theorem 1**. *Under mild assumptions, all reward classes consistent with the Plackett-Luce\n",
      "(and Bradley-Terry in particular) models can be represented with the reparameterization ${r(x, y) =\n",
      "\\beta \\log \\frac{\\pi(y\\mid x)}{\\pi_ref(y\\mid x)}}$ for some model $\\pi(y\\mid x)$ and a given\n",
      "reference model $\\pi_ref(y \\mid x)$.* :::\n",
      "\n",
      "Consider any reward function $r(x, y)$, which induces a corresponding optimal model $\\pi_r(y \\mid\n",
      "x)$, specified by Eq. [\\[eq:op_policy\\]](#eq:op_policy). We will show that a reward function from\n",
      "the equivalence class of $r$ can be represented using the reparameterization given above. We define\n",
      "the projection $f$ as $$f(r; \\pi_ref, \\beta)(x, y) = r(x, y) - \\beta\\log\\sum_{y}\\pi_ref(y\\mid\n",
      "x)\\exp\\left(\\frac{1}{\\beta}r(x, y)\\right)$$ The operator $f$ simply normalizes the reward function\n",
      "with the logarithm of the partition function of $\\pi_r$. Since the added normalization term is only\n",
      "a function of the prefix $x$, $f(r; \\pi_ref, \\beta)(x, y)$ is a reward function in the equivalence\n",
      "class of $r(x, y)$. Finally, replacing $r$ with the RHS of Eq. [\\[eq:main_eq\\]](#eq:main_eq) (which\n",
      "holds for any reward function), we have $f(r; \\pi_ref, \\beta)(x, y) = \\beta \\log \\frac{\\pi_r(y\\mid\n",
      "x)}{\\pi_ref(y\\mid x)}$. That is, the projection $f$ produces a member of the equivalence class of\n",
      "$r$ with the desired form, and we do not lose any generality in our reward model from the proposed\n",
      "reparameterization. We can alternatively view Theorem [1](#thm:main) as specifying exactly which\n",
      "reward function within each equivalence class the DPO reparameterization selects, that is, the\n",
      "reward function satisfying: $$  \\sum_{y}\\underbrace{\\pi_ref(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r(x,\n",
      "y)\\right)}_{=\\pi(y\\mid x), using Thm.~\\ref{thm:main reparam.}} = 1,$$ i.e., $\\pi(y\\mid x)$ is a\n",
      "valid distribution (probabilities are positive and sum to 1). However, following\n",
      "Eq. [\\[eq:op_policy\\]](#eq:op_policy), we can see that Eq. [\\[eq:lag_p\\]](#eq:lag_p) is the\n",
      "partition function of the optimal policy induced by the reward function $r(x, y)$. The key insight\n",
      "of the DPO algorithm is that we can impose certain constraints on the under-constrained Plackett-\n",
      "Luce (and Bradley-Terry in particular) family of preference models, such that we preserve the class\n",
      "of representable reward models, but explicitly make the optimal policy in Eq.\n",
      "[\\[eq:op_policy\\]](#eq:op_policy) analytically tractable for all prompts $x$.\n",
      "\n",
      "## Instability of Actor-Critic Algorithms\n",
      "\n",
      "We can also use our framework to diagnose instabilities with standard actor-critic algorithms used\n",
      "for the RLHF, such as PPO. We follow the RLHF pipeline and focus on the RL fine-tuning step outlined\n",
      "in Section [3](#section:prelims). We can draw connections to the control as inference framework for\n",
      "the constrained RL problem outlined in [\\[eq:RL\\]](#eq:RL). We assume a parameterized model\n",
      "$\\pi_{\\theta}(y\\mid x)$ and minimize $\\mathbb{D}_{KL}[\\pi_{\\theta}(y|x) \\mid \\mid \\pi^*(y\\mid x)]$\n",
      "where $\\pi^*$ is the optimal policy from Eq. [\\[eq:optimum_model\\]](#eq:optimum_model) induced by\n",
      "the reward function $r_{\\phi}(y, x)$. With some algebra this leads to the optimization objective: $$\n",
      "\\max_{\\pi_{\\theta}}\\mathbb{E}_{\\pi_{\\theta}(y\\mid x)}\\bigg[\\underbrace{r_{\\phi}(x, y)\n",
      "-\\beta\\log\\sum_{y}\\pi_ref(y\\mid x)\\exp\\left(\\frac{1}{\\beta}r_{\\phi}(x, y)\\right)}_{f(r_{\\phi},\n",
      "\\pi_ref, \\beta)} - \\underbrace{\\beta\\log\\frac{\\pi_{\\theta}(y\\mid x)}{\\pi_ref(y\\mid x)}}_{KL}\\bigg]$$\n",
      "This is the same objective optimized in prior worksusing the DPO-equivalent reward for the reward\n",
      "class of $r_{\\phi}$. In this setting, we can interpret the normalization term in $f(r_{\\phi},\n",
      "\\pi_ref, \\beta)$ as the soft value function of the reference policy $\\pi_ref$. While this term does\n",
      "not affect the optimal solution, without it, the policy gradient of the objective could have high\n",
      "variance, making learning unstable. We can accommodate for the normalization term using a learned\n",
      "value function, but that can also be difficult to optimize. Alternatively, prior works have\n",
      "normalized rewards using a human completion baseline, essentially a single sample Monte-Carlo\n",
      "estimate of the normalizing term. In contrast the DPO reparameterization yields a reward function\n",
      "that does not require any baselines.\n",
      "\n",
      "# Experiments\n",
      "\n",
      "In this section, we empirically evaluate DPO's ability to train policies directly from preferences.\n",
      "First, in a well-controlled text-generation setting, we ask: how efficiently does DPO trade off\n",
      "maximizing reward and minimizing KL-divergence with the reference policy, compared to common\n",
      "preference learning algorithms such as PPO? Next, we evaluate DPO's performance on larger models and\n",
      "more difficult RLHF tasks, including summarization and dialogue. We find that with almost no tuning\n",
      "of hyperparameters, DPO tends to perform as well or better than strong baselines like RLHF with PPO\n",
      "as well as returning the best of $N$ sampled trajectories under a learned reward function. Before\n",
      "presenting these results, we describe the experimental set-up; additional details are in\n",
      "Appendix [10](#app:exp_details).\n",
      "\n",
      "**Tasks.** Our experiments explore three different open-ended text generation tasks. For all\n",
      "experiments, algorithms learn a policy from a dataset of preferences $\\mathcal{D}=\\bigl\\{x^{(i)},\n",
      "y_w^{(i)}, y_l^{(i)}\\bigr\\}_{i=1}^N$. In **controlled sentiment generation**, $x$ is a prefix of a\n",
      "movie review from the IMDb dataset , and the policy must generate $y$ with positive sentiment. In\n",
      "order to perform a controlled evaluation, for this experiment we *generate* preference pairs over\n",
      "generations using a pre-trained sentiment classifier, where $p(positive\\mid x,y_w)>p(positive\\mid\n",
      "x,y_l)$. For SFT, we fine-tune GPT-2-large until convergence on reviews from the train split of the\n",
      "IMDB dataset (further details in App [10.1](#app:sentiment_details)). In **summarization**, $x$ is a\n",
      "forum post from Reddit; the policy must generate a summary $y$ of the main points in the post.\n",
      "Following prior work, we use the Reddit TL;DR summarization dataset along with human preferences\n",
      "gathered by @stiennon2022learning. We use an SFT model fine-tuned on human-written forum post\n",
      "summaries with the TRLX framework for RLHF. The human preference dataset was gathered by\n",
      "@stiennon2022learning on samples from a different, but similarly-trained, SFT model. Finally, in\n",
      "**single-turn dialogue**, $x$ is a human query, which may be anything from a question about\n",
      "astrophysics to a request for relationship advice. A policy must produce an engaging and helpful\n",
      "response $y$ to a user's query; we use the Anthropic Helpful and Harmless dialogue dataset ,\n",
      "containing 170k dialogues between a human and an automated assistant. Each transcript ends with a\n",
      "pair of responses generated by a large (although unknown) language model along with a preference\n",
      "label denoting the human-preferred response. In this setting, no pre-trained SFT model is available;\n",
      "we therefore fine-tune an off-the-shelf language model on only the preferred completions to form the\n",
      "SFT model.\n",
      "\n",
      "**Evaluation.** Our experiments use two different approaches to evaluation. In order to analyze the\n",
      "effectiveness of each algorithm in optimizing the constrained reward maximization objective, in the\n",
      "controlled sentiment generation setting we evaluate each algorithm by its frontier of achieved\n",
      "reward and KL-divergence from the reference policy; this frontier is computable because we have\n",
      "acccess to the ground-truth reward function (a sentiment classifier). However, in the real world,\n",
      "the ground truth reward function is not known; therefore, we evaluate algorithms with their *win\n",
      "rate* against a baseline policy, using GPT-4 as a proxy for human evaluation of summary quality and\n",
      "response helpfulness in the summarization and single-turn dialogue settings, respectively. For\n",
      "summarization, we use reference summaries in the test set as the baseline; for dialogue, we use the\n",
      "preferred response in the test dataset as the baseline. While existing studies suggest LMs can be\n",
      "better automated evaluators than existing metrics, we conduct a human study to justify our usage of\n",
      "GPT-4 for evaluation in Sec. [6.4](#sec:human-judgments). We find GPT-4 judgments correlate strongly\n",
      "with humans, with human agreement with GPT-4 typically similar or higher than inter-human annotator\n",
      "agreement.\n",
      "\n",
      "**Methods.** In addition to DPO, we evaluate several existing approaches to training language models\n",
      "to adhere to human preferences. Most simply, we explore zero-shot prompting with **GPT-J** in the\n",
      "summarization task and 2-shot prompting with **Pythia-2.8B** in the dialogue task. In addition, we\n",
      "evaluate the **SFT** model as well as **Preferred-FT**, which is a model fine-tuned with supervised\n",
      "learning on the chosen completion $y_w$ from either the SFT model (in controlled sentiment and\n",
      "summarization) or a generic LM (in single-turn dialogue). Another pseudo-supervised method is\n",
      "**Unlikelihood** , which simply optimizes the policy to maximize the probability assigned to $y_w$\n",
      "and *minimize* the probability assigned to $y_l$; we use an optional coefficient $\\alpha\\in[0,1]$ on\n",
      "the 'unlikelihood' term. We also consider **PPO** using a reward function learned from the\n",
      "preference data and **PPO-GT**, which is an oracle that learns from the ground truth reward function\n",
      "available in the controlled sentiment setting. In our sentiment experiments, we use two\n",
      "implementations of PPO-GT, one of-the-shelf version as well as a modified version that normalizes\n",
      "rewards and further tunes hyperparameters to improve performance (we also use these modifications\n",
      "when running 'normal' PPO with learned rewards). Finally, we consider the **Best of $N$** baseline,\n",
      "sampling $N$ responses from the SFT model (or Preferred-FT in dialogue) and returning the highest-\n",
      "scoring response according to a reward function learned from the preference dataset. This high-\n",
      "performing method decouples the quality of the reward model from the PPO optimization, but is\n",
      "computationally impractical even for moderate $N$ as it requires sampling $N$ completions for every\n",
      "query at test time.\n",
      "\n",
      "## How well can DPO optimize the RLHF objective?\n",
      "\n",
      "The KL-constrained reward maximization objective used in typical RLHF algorithms balances\n",
      "exploitation of reward while restricting the policy from deviating far from the reference policy.\n",
      "Therefore, when comparing algorithms, we must take into account both reward achieved as well as the\n",
      "KL discrepancy; achieving slightly higher reward but with much higher KL is not necessarily\n",
      "desirable. Figure [\\[fig:frontier-tldr-main\\]](#fig:frontier-tldr-main) shows the reward-KL frontier\n",
      "for various algorithms in the sentiment setting. We execute multiple training runs for each\n",
      "algorithm, using a different hyperparameter for policy conservativeness in each run (target KL\n",
      "$\\in\\{3,6,9,12\\}$ for PPO, $\\beta \\in \\{0.05,0.1,1,5\\}$, $\\alpha\\in\\{0.05,0.1,0.5,1\\}$ for\n",
      "unlikelihood, random seeds for preferred-FT). This sweep includes 22 runs in total. After each 100\n",
      "training steps until convergence, we evaluate each policy on a set of test prompts, computing the\n",
      "average reward under the true reward function as well as the average sequence-level KL with the\n",
      "reference policy $KL\\left(\\pi\\mid \\mid \\pi_ref\\right)$. We find that DPO produces by far the most\n",
      "efficient frontier, achieving the highest reward while still achieving low KL. This result is\n",
      "particularly notable for multiple reasons. First, DPO and PPO optimize the same objective, but DPO\n",
      "is notably more efficient; DPO's reward/KL tradeoff strictly dominates PPO. Second, DPO achieves a\n",
      "better frontier than PPO, *even when PPO can access ground truth rewards* (PPO-GT).\n",
      "\n",
      "## Can DPO scale to real preference datasets? Next, we evaluate fine-tuning performance of DPO on\n",
      "summarization and single-turn dialogue. For summarization, automatic evaluation metrics such as\n",
      "ROUGE can be poorly correlated with human preferences , and prior work has found that fine-tuning\n",
      "LMs using PPO on human preferences to provide more effective summaries. We evaluate different\n",
      "methods by sampling completions on the test split of TL;DR summarization dataset, and computing the\n",
      "average win rate against reference completions in the test set. The completions for all methods are\n",
      "sampled at temperatures varying from 0.0 to 1.0, and the win rates are shown in\n",
      "Figure [\\[fig:frontier-tldr-main\\]](#fig:frontier-tldr-main) (right). DPO, PPO and Preferred-FT all\n",
      "fine-tune the same GPT-J SFT model. We find that DPO has a win rate of approximately 61% at a\n",
      "temperature of 0.0, exceeding the performance of PPO at  57% at its optimal sampling temperature of\n",
      "0.0. DPO also achieves a higher maximum win rate compared to the best of $N$ baseline. We note that\n",
      "we did not meaningfully tune DPO's $\\beta$ hyperparameter, so these results may underestimate DPO's\n",
      "potential. Moreover, we find DPO to be much more robust to the sampling temperature than PPO, the\n",
      "performance of which can degrade to that of the base GPT-J model at high temperatures. Preferred-FT\n",
      "does not improve significantly over the SFT model. We also compare DPO and PPO head-to-head in human\n",
      "evaluations in Section [6.4](#sec:human-judgments), where DPO samples at temperature 0.25 were\n",
      "preferred 58% times over PPO samples at temperature 0.\n",
      "\n",
      "On single-turn dialogue, we evaluate the different methods on the subset of the test split of the\n",
      "Anthropic HH dataset with one step of human-assistant interaction. GPT-4 evaluations use the\n",
      "preferred completions on the test as the reference to compute the win rate for different methods. As\n",
      "there is no standard SFT model for this task, we start with a pre-trained Pythia-2.8B, use\n",
      "Preferred-FT to train a reference model on the chosen completions such that completions are within\n",
      "distribution of the model, and then train using DPO. We also compare against the best of 128\n",
      "Preferred-FT completions (we found the Best of $N$ baseline plateaus at 128 completions for this\n",
      "task; see Appendix Figure [\\[fig:best-of-n\\]](#fig:best-of-n)) and a 2-shot prompted version of the\n",
      "Pythia-2.8B base model, finding DPO performs as well or better for the best-performing temperatures\n",
      "for each method. We also evaluate an RLHF model trained with PPO on the Anthropic HH dataset from a\n",
      "well-known source , but are unable to find a prompt or sampling temperature that gives performance\n",
      "better than the base Pythia-2.8B model. Based on our results from TL;DR and the fact that both\n",
      "methods optimize the same reward function, we consider Best of 128 a rough proxy for PPO-level\n",
      "performance. Overall, DPO is the only computationally efficient method that improves over the\n",
      "preferred completions in the Anthropic HH dataset, and provides similar or better performance to the\n",
      "computationally demanding Best of 128 baseline. Finally, Figure [\\[fig:dialogue-\n",
      "main\\]](#fig:dialogue-main) shows that DPO converges to its best performance relatively quickly.\n",
      "\n",
      "## Generalization to a new input distribution\n",
      "\n",
      "::: wraptable r0.375\n",
      "\n",
      " ------------------ ------------------------------- -------------  **Win rate vs. ground truth**\n",
      "(lr)2-3 **Alg.** Temp $0$ Temp $0.25$  DPO 0.36 0.31  PPO 0.26 0.23  ------------------\n",
      "------------------------------- -------------\n",
      "\n",
      ":::\n",
      "\n",
      "To further compare the performance of PPO and DPO under distribution shifts, we evaluate the PPO and\n",
      "DPO policies from our Reddit TL;DR summarization experiment on a different distribution, news\n",
      "articles in the test split of the CNN/DailyMail dataset, using the best sampling temperatures from\n",
      "TL;DR (0 and 0.25). The results are presented in Table [\\[tab:ood\\]](#tab:ood). We computed the\n",
      "GPT-4 win rate against the ground-truth summaries in the datasets, using the same GPT-4 (C) prompt\n",
      "we used for Reddit TL;DR, but replacing the words \"forum post\" with \"news article\". For this new\n",
      "distribution, DPO continues to outperform the PPO policy by a significant margin. This experiment\n",
      "provides initial evidence that DPO policies can generalize similarly well to PPO policies, even\n",
      "though DPO does not use the additional unlabeled Reddit TL;DR prompts that PPO uses.\n",
      "\n",
      "## Validating GPT-4 judgments with human judgments We conduct a human study to verify the\n",
      "reliability of GPT-4's judgments, using the results of the TL;DR summarization experiment and two\n",
      "different GPT-4 prompts. The **GPT-4 (S)** (simple) prompt simply asks for which summary better-\n",
      "summarizes the important information in the post. The **GPT-4 (C)** (concise) prompt also asks for\n",
      "which summary is more concise; we evaluate this prompt because we find that GPT-4 prefers longer,\n",
      "more repetitive summaries than humans do with the **GPT-4 (S)** prompt. See\n",
      "Appendix [10.2](#app:prompts) for the complete prompts. We perform three comparisons, using the\n",
      "highest (DPO, temp. 0.25), the lowest (PPO, temp. 1.0), and a\n",
      "\n",
      "::: wraptable r0.47\n",
      "\n",
      " ----------------------- --------- --------- -----------  **DPO** **SFT** **PPO-1**  (lr)2-4 N\n",
      "respondents 272 122 199  GPT-4 (S) win % 47 27 13  GPT-4 (C) win % 54 32 12  Human win % 58 43 17\n",
      "GPT-4 (S)-H agree 70 77 86  GPT-4 (C)-H agree 67 79 85  H-H agree 65 \\- 87  -----------------------\n",
      "--------- --------- -----------\n",
      "\n",
      ":::\n",
      "\n",
      "middle-performing (SFT, temp. 0.25) method with the aim of covering a diversity of sample qualities;\n",
      "all three methods are compared against greedily-sampled PPO (its best-performing temperature). We\n",
      "find that with both prompts, GPT-4 tends to agree with humans about as often as humans agree with\n",
      "each other, suggesting that GPT-4 is a reasonable proxy for human evaluations (due to limited human\n",
      "raters, we only collect multiple human judgments for the DPO and PPO-1 comparisons). Overall, the\n",
      "**GPT-4 (C)** prompt generally provides win rates more representative of humans; we therefore use\n",
      "this prompt for the main results in Section [6.2](#sec:dpo-real-datasets). For additional details\n",
      "about the human study, including the web interface presented to raters and the list of human\n",
      "volunteers, see Appendix [11.3](#app:human-study).\n",
      "\n",
      "# Discussion\n",
      "\n",
      "Learning from preferences is a powerful, scalable framework for training capable, aligned language\n",
      "models. We have introduced DPO, a simple training paradigm for training language models from\n",
      "preferences without reinforcement learning. Rather than coercing the preference learning problem\n",
      "into a standard RL setting in order to use off-the-shelf RL algorithms, DPO identifies a mapping\n",
      "between language model policies and reward functions that enables training a language model to\n",
      "satisfy human preferences *directly*, with a simple cross-entropy loss, without reinforcement\n",
      "learning or loss of generality. With virtually no tuning of hyperparameters, DPO performs similarly\n",
      "or better than existing RLHF algorithms, including those based on PPO; DPO thus meaningfully reduces\n",
      "the barrier to training more language models from human preferences.\n",
      "\n",
      "**Limitations & Future Work.** Our results raise several important questions for future work. How\n",
      "does the DPO policy generalize out of distribution, compared with learning from an explicit reward\n",
      "function? Our initial results suggest that DPO policies can generalize similarly to PPO-based\n",
      "models, but more comprehensive study is needed. For example, can training with self-labeling from\n",
      "the DPO policy similarly make effective use of unlabeled prompts? On another front, how does reward\n",
      "over-optimization manifest in the direct preference optimization setting, and is the slight decrease\n",
      "in performance in Figure [\\[fig:dialogue-main\\]](#fig:dialogue-main)-right an instance of it?\n",
      "Additionally, while we evaluate models up to 6B parameters, exploration of scaling DPO to state-of-\n",
      "the-art models orders of magnitude larger is an exciting direction for future work. Regarding\n",
      "evaluations, we find that the win rates computed by GPT-4 are impacted by the prompt; future work\n",
      "may study the best way to elicit high-quality judgments from automated systems. Finally, many\n",
      "possible applications of DPO exist beyond training language models from human preferences, including\n",
      "training generative models in other modalities.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import textwrap\n",
    "\n",
    "def remove_special_blocks(text):\n",
    "    \"\"\"Removes larger, special formatted blocks from the text.\"\"\"\n",
    "    # Remove complex figure blocks and image links, including ![image](...)\n",
    "    cleaned_text = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', text)\n",
    "\n",
    "    # Remove LaTeX environments that aren't math\n",
    "    # cleaned_text = re.sub(r'\\\\begin\\{(?!equation|align|gather)[^}]+\\}', '', cleaned_text)\n",
    "    # cleaned_text = re.sub(r'\\\\end\\{(?!equation|align|gather)[^}]+\\}', '', cleaned_text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def clean_inline_formatting(text):\n",
    "    \"\"\"Cleans inline LaTeX commands and other formatting.\"\"\"\n",
    "    cleaned_text = text\n",
    "    \n",
    "    # Remove citation brackets like [@author2020paper; @another2021paper]\n",
    "    cleaned_text = re.sub(r'\\n\\[@[^\\]]+\\]\\n', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\n\\[@[^\\]]+\\]', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\[@[^\\]]+\\]\\n', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\[@[^\\]]+\\]', '', cleaned_text)\n",
    "\n",
    "    # Remove LaTeX labels\n",
    "    cleaned_text = re.sub(r'\\\\label\\{[^}]+\\}', '', cleaned_text)\n",
    "    \n",
    "    # Clean up equation references\n",
    "    cleaned_text = re.sub(\n",
    "        r'Eq\\.\\[\\\\?\\[([^\\]]+)\\]\\]\\([^)]*\\)\\{[^}]*\\}', \n",
    "        r'Eq. (\\1)', \n",
    "        cleaned_text\n",
    "    )\n",
    "    \n",
    "    # Clean up section, figure, and table references\n",
    "    for ref_type in ['Section', 'Figure', 'Table']:\n",
    "        cleaned_text = re.sub(\n",
    "            r'{}\\[\\\\?\\[([^\\]]+)\\]\\]\\([^)]*\\)\\{{[^}}]*\\}}'.format(ref_type),\n",
    "            r'{} \\1'.format(ref_type),\n",
    "            cleaned_text\n",
    "        )\n",
    "    \n",
    "    # Clean up appendix references\n",
    "    cleaned_text = re.sub(\n",
    "        r'Appendix\\[\\\\?\\[([^\\]]+)\\]\\]\\([^)]*\\)\\{[^}]*\\}', \n",
    "        r'Appendix', \n",
    "        cleaned_text\n",
    "    )\n",
    "    \n",
    "    # Clean up LaTeX text formatting commands\n",
    "    cleaned_text = re.sub(r'\\\\text\\{([^}]+)\\}', r'\\1', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\\\textrm\\{([^}]+)\\}', r'\\1', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\\\textit\\{([^}]+)\\}', r'*\\1*', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\\\textbf\\{([^}]+)\\}', r'**\\1**', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\\\emph\\{([^}]+)\\}', r'*\\1*', cleaned_text)\n",
    "    \n",
    "    # Clean up various artifacts\n",
    "    cleaned_text = re.sub(r'\\{[^}]*reference-type[^}]*\\}', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\[\\^[0-9]+\\]', '', cleaned_text) # Footnotes\n",
    "    cleaned_text = re.sub(r'\\s*\\{#[^}]+\\}\\s*$', '', cleaned_text, flags=re.MULTILINE) # Pandoc headers\n",
    "\n",
    "    # Remove remaining LaTeX commands, and any resulting empty braces/brackets\n",
    "    # cleaned_text = re.sub(r'\\\\[a-zA-Z]+\\*?', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\{\\s*\\}', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\{width=[^}]*\\}', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\[\\s*\\]', '', cleaned_text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def normalize_whitespace(text):\n",
    "    \"\"\"Normalizes whitespace and paragraph breaks.\"\"\"\n",
    "\n",
    "    # Clean up multiple spaces and empty lines\n",
    "    cleaned_text = re.sub(r' +', ' ', text)\n",
    "    cleaned_text = re.sub(r'\\n\\s*\\n+', '\\n\\n', cleaned_text)\n",
    "    \n",
    "    # Fix any broken markdown headers\n",
    "    cleaned_text = re.sub(r'^#+\\s*$', '', cleaned_text, flags=re.MULTILINE)\n",
    "    \n",
    "    return cleaned_text.strip()\n",
    "\n",
    "def clean_latex_to_markdown(text):\n",
    "    \"\"\"\n",
    "    Clean LaTeX formatting and convert to markdown while preserving structure.\n",
    "    \"\"\"\n",
    "    # Join paragraphs by replacing single newlines with spaces\n",
    "    cleaned_text = re.sub(r'(?<!\\n)\\n(?!\\n)', '', text)\n",
    "    cleaned_text = remove_special_blocks(text)\n",
    "    cleaned_text = clean_inline_formatting(cleaned_text)\n",
    "    cleaned_text = normalize_whitespace(cleaned_text)\n",
    "    return cleaned_text\n",
    "        \n",
    "cleaned_paper = clean_latex_to_markdown(latex_content)\n",
    "# Remove acknowledgements and everything after\n",
    "cleaned_paper = cleaned_paper.split('# Acknowledgements')[0].strip()\n",
    "print_wrapped(cleaned_paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final version of the clean function used in experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_latex_to_markdown(text):\n",
    "    \"\"\"\n",
    "    Clean LaTeX formatting and convert to markdown while preserving structure.\n",
    "    \"\"\"\n",
    "    # Store the original text for processing\n",
    "    cleaned_text = text\n",
    "    \n",
    "    # 1. Remove citation brackets like [@author2020paper; @another2021paper]\n",
    "    cleaned_text = re.sub(r'\\[@[^\\]]+\\]', '', cleaned_text)\n",
    "    \n",
    "    # 2. Remove LaTeX labels\n",
    "    cleaned_text = re.sub(r'\\\\label\\{[^}]+\\}', '', cleaned_text)\n",
    "    \n",
    "    # 3. Clean up equation references - convert complex refs to simple format\n",
    "    # Pattern like: Eq.[\\[eq:RL\\]](#eq:RL){reference-type=\"ref\" reference=\"eq:RL\"}\n",
    "    cleaned_text = re.sub(\n",
    "        r'Eq\\.\\[\\\\?\\[([^\\]]+)\\]\\]\\([^)]*\\)\\{[^}]*\\}', \n",
    "        r'Eq. (\\1)', \n",
    "        cleaned_text\n",
    "    )\n",
    "    \n",
    "    # 4. Clean up section references\n",
    "    # Pattern like: Section[\\[sec:theory\\]](#sec:theory){reference-type=\"ref\" reference=\"sec:theory\"}\n",
    "    cleaned_text = re.sub(\n",
    "        r'Section\\[\\\\?\\[([^\\]]+)\\]\\]\\([^)]*\\)\\{[^}]*\\}', \n",
    "        r'Section \\1', \n",
    "        cleaned_text\n",
    "    )\n",
    "    \n",
    "    # 5. Clean up appendix references \n",
    "    # Pattern like: Appendix[\\[app:derivation1\\]](#app:derivation1){reference-type=\"ref\" reference=\"app:derivation1\"}\n",
    "    cleaned_text = re.sub(\n",
    "        r'Appendix\\[\\\\?\\[([^\\]]+)\\]\\]\\([^)]*\\)\\{[^}]*\\}', \n",
    "        r'Appendix', \n",
    "        cleaned_text\n",
    "    )\n",
    "    \n",
    "    # 6. Clean up figure references\n",
    "    # Pattern like: Figure[\\[fig:frontier-tldr-main\\]](#fig:frontier-tldr-main){reference-type=\"ref\" reference=\"fig:frontier-tldr-main\"}\n",
    "    cleaned_text = re.sub(\n",
    "        r'Figure\\[\\\\?\\[([^\\]]+)\\]\\]\\([^)]*\\)\\{[^}]*\\}', \n",
    "        r'Figure \\1', \n",
    "        cleaned_text\n",
    "    )\n",
    "    \n",
    "    # 7. Clean up table references\n",
    "    cleaned_text = re.sub(\n",
    "        r'Table\\[\\\\?\\[([^\\]]+)\\]\\]\\([^)]*\\)\\{[^}]*\\}', \n",
    "        r'Table \\1', \n",
    "        cleaned_text\n",
    "    )\n",
    "    \n",
    "    # 8. Remove complex figure blocks but preserve descriptive text\n",
    "    # Pattern like: ![ **optimizes for human preferences...] \n",
    "    figure_pattern = r'!\\[\\s*\\*\\*([^*]+)\\*\\*([^\\]]*)\\]'\n",
    "    cleaned_text = re.sub(figure_pattern, r'**Figure: \\1**\\2', cleaned_text)\n",
    "    \n",
    "    # 9. Clean up LaTeX text formatting commands\n",
    "    cleaned_text = re.sub(r'\\\\text\\{([^}]+)\\}', r'\\1', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\\\textrm\\{([^}]+)\\}', r'\\1', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\\\textit\\{([^}]+)\\}', r'*\\1*', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\\\textbf\\{([^}]+)\\}', r'**\\1**', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\\\emph\\{([^}]+)\\}', r'*\\1*', cleaned_text)\n",
    "    \n",
    "    # 10. Clean up LaTeX math commands in text (but preserve $$ blocks)\n",
    "    # Remove \\mid and similar in inline contexts\n",
    "    cleaned_text = re.sub(r'\\\\mid(?![^$]*\\$\\$)', '|', cleaned_text)\n",
    "    \n",
    "    # 11. Remove LaTeX section numbering artifacts\n",
    "    cleaned_text = re.sub(r'\\{#[^}]+\\}', '', cleaned_text)\n",
    "    \n",
    "    # 12. Clean up reference artifacts like {reference-type=\"ref\" reference=\"...\"}\n",
    "    cleaned_text = re.sub(r'\\{[^}]*reference-type[^}]*\\}', '', cleaned_text)\n",
    "    \n",
    "    # 13. Clean up footnote markers like [^2], [^3] etc.\n",
    "    cleaned_text = re.sub(r'\\[\\^[0-9]+\\]', '', cleaned_text)\n",
    "    \n",
    "    # 14. Remove LaTeX environments that aren't math\n",
    "    # Remove \\begin{...} and \\end{...} for non-math environments\n",
    "    cleaned_text = re.sub(r'\\\\begin\\{(?!equation|align|gather)[^}]+\\}', '', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\\\end\\{(?!equation|align|gather)[^}]+\\}', '', cleaned_text)\n",
    "    \n",
    "    # 15. Clean up definition and lemma blocks - convert to markdown\n",
    "    # Pattern like: ::: definition ... :::\n",
    "    cleaned_text = re.sub(r':::\\s*definition\\s*\\n\\*\\*Definition\\s+(\\d+)\\*\\*\\.([^:]+):::', \n",
    "                         r'**Definition \\1:** \\2', cleaned_text, flags=re.DOTALL)\n",
    "    \n",
    "    cleaned_text = re.sub(r':::\\s*\\{#[^}]+\\s+\\.lemma\\}\\s*\\n\\*\\*Lemma\\s+(\\d+)\\*\\*\\.([^:]+):::', \n",
    "                         r'**Lemma \\1:** \\2', cleaned_text, flags=re.DOTALL)\n",
    "    \n",
    "    # 16. Remove remaining LaTeX artifacts\n",
    "    cleaned_text = re.sub(r'\\\\[a-zA-Z]+\\*?', '', cleaned_text)  # Remove LaTeX commands\n",
    "    cleaned_text = re.sub(r'\\{[^}]*\\}(?![^$]*\\$\\$)', '', cleaned_text)  # Remove remaining braces outside math\n",
    "    \n",
    "    # 17. Clean up multiple spaces and empty lines\n",
    "    cleaned_text = re.sub(r' +', ' ', cleaned_text)  # Multiple spaces to single\n",
    "    cleaned_text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', cleaned_text)  # Multiple newlines to double\n",
    "    \n",
    "    # 18. Fix any broken markdown headers\n",
    "    cleaned_text = re.sub(r'^#+\\s*$', '', cleaned_text, flags=re.MULTILINE)\n",
    "    \n",
    "    return cleaned_text.strip().split('# Acknowledgements')[0].strip()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
